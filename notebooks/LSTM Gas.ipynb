{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sudo-Oliver/Predictive-Analytics-Private/blob/main/notebooks/LSTM%20Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7R7RQPWgqys"
   },
   "source": [
    "**1. Data loading and Preprocessing**\n",
    "1. Library Setup and GPU Configuration:\n",
    "- Import required Python libraries for data analysis and deep learning\n",
    "- Configure PyTorch for Mac GPU (MPS) usage\n",
    "- Verify device availability and settings\n",
    "\n",
    "2. Data Loading and Initial Processing:\n",
    "- Load preprocessed sensor data from parquet file\n",
    "- Implement fallback to Google Drive download if file not present locally\n",
    "- Basic data validation and shape verification\n",
    "\n",
    "3.  Data Cleaning and Transformation:\n",
    "- Convert Unix timestamps to datetime format\n",
    "- Sort data by household ID and timestamp\n",
    "- Remove unnecessary sensor and room-related columns\n",
    "- Convert energy consumption units from Wh to kWh\n",
    "- Handle negative gas consumption values\n",
    "\n",
    "4. Feature Engineering:\n",
    "- Create cyclical time features (hour_sin, hour_cos)\n",
    "- Generate lag features for both electricity and gas consumption (1-3 hours)\n",
    "- Calculate rolling mean features (3h and 7h windows)\n",
    "- Handle missing values through forward and backward fill\n",
    "\n",
    "5. Correlation Analysis:\n",
    "- Calculate complete correlation matrix\n",
    "- Focus on correlations with target variables (electricity and gas consumption)\n",
    "- Sort and display correlations by strength\n",
    "- Visualize relationships between features and target variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPGwAXsugqyt",
    "outputId": "3c20df7b-85f1-4803-d5cc-6b5abf07b366"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from joblib import dump, load\n",
    "\n",
    "# Deep learning frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# File download\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch version and MPS availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())\n",
    "\n",
    "# Configure device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Metal GPU device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Verify current device\n",
    "print(\"Current device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    #Load preprocessed sensor data with fallback to Drive download\n",
    "    file_id = \"1KHQCVfwTxm5bjjITS8WMm9P3M12ETVsR\"\n",
    "    \n",
    "    download_path = Path('data/processed')\n",
    "    download_path.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = download_path / 'final_processed_data3.parquet'\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(\"Downloading from Google Drive...\")\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(url, str(file_path), quiet=False)\n",
    "    \n",
    "    if file_path.exists():\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(f\"Data loaded successfully: {df.shape} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not load or download data file\")\n",
    "\n",
    "def clean_data(df):\n",
    "    # Convert Unix timestamp to datetime\n",
    "    if 'timestamp_local' in df.columns:\n",
    "        df['timestamp_local'] = pd.to_datetime(df['timestamp_local'], unit='ms')\n",
    "        # Sort before setting index\n",
    "        df = df.sort_values(by=['homeid', 'timestamp_local'])\n",
    "        # Set index\n",
    "        df.set_index('timestamp_local', inplace=True)\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        'sensorid', 'median_temperature', '_room',\n",
    "        'sensorid_room', 'measured_entity',\n",
    "        'sensorid_electric', 'sensorid_gas'\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    \n",
    "    # First rename consumption columns from Wh to kWh\n",
    "    df = df.rename(columns={\n",
    "        'electric_total_consumption_Wh': 'electric_total_consumption_kWh',\n",
    "        'gas_total_consumption_Wh': 'gas_total_consumption_kWh'\n",
    "    })\n",
    "    \n",
    "    # Replace negative values with 0 in gas columns\n",
    "    gas_columns = [\n",
    "        'gas_mean_consumption', \n",
    "        'gas_min_consumption', \n",
    "        'gas_max_consumption',\n",
    "        'gas_median_consumption', \n",
    "        'gas_total_consumption_kWh'\n",
    "    ]\n",
    "    \n",
    "    # Use clip to replace negative values with 0 (more efficient than applymap)\n",
    "    df[gas_columns] = df[gas_columns].clip(lower=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_features(df, df_original):\n",
    "    \"\"\"Create time-based and lag features\"\"\"\n",
    "    # Verify datetime index\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame must have DatetimeIndex\")\n",
    "    \n",
    "    # Cyclical time features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    # Lag features using original values\n",
    "    for lag in range(1, 4):\n",
    "        df[f'electric_lag_{lag}'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].shift(lag)\n",
    "        df[f'gas_lag_{lag}'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].shift(lag)\n",
    "    \n",
    "    # Rolling means using original values\n",
    "    df['electric_rolling_mean_3h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "    df['electric_rolling_mean_7h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    df['gas_rolling_mean_3h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "    df['gas_rolling_mean_7h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main preprocessing pipeline\n",
    "print(\"Loading data...\")\n",
    "df = load_processed_data()\n",
    "\n",
    "print(\"\\nCleaning data...\")\n",
    "df_clean = clean_data(df.copy())\n",
    "df_original = df_clean.copy()  # Keep unmodified copy for feature creation\n",
    "\n",
    "print(\"\\nCreating features...\")\n",
    "df_clean = create_features(df_clean, df_original)\n",
    "\n",
    "print(\"Handling missing values...\")\n",
    "df_clean = df_clean.ffill().bfill()\n",
    "\n",
    "# Verify data\n",
    "print(\"\\nData shape after preprocessing:\", df_clean.shape)\n",
    "print(\"Index type:\", type(df_clean.index))\n",
    "print(\"Feature columns:\", df_clean.columns.tolist())\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "PSojNn9jgqyw",
    "outputId": "1a694ec4-2460-4986-e79e-4b6beda59b2a"
   },
   "outputs": [],
   "source": [
    "# Vollständige Korrelation mit allen spalten berechnen\n",
    "correlation_matrix_all = df_clean.corr()\n",
    "\n",
    "# Korrelation der Features mit den Zielvariablen (Strom und Gasverbtauch)\n",
    "correlation_target_all = correlation_matrix_all[['electric_total_consumption_kWh', 'gas_total_consumption_kWh']]\n",
    "\n",
    "# Sortieren nach Stärke der Korrelation\n",
    "correlation_target_all_sorted = correlation_target_all.abs().sort_values(by=['electric_total_consumption_kWh', 'gas_total_consumption_kWh'], ascending=False)\n",
    "\n",
    "# Korrelationsergebnisse anzeigen\n",
    "print(\"Full Feature Correlation:\")\n",
    "display(correlation_target_all_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering & Data Scaling ### \n",
    "- Directory Setup: Create directory for scalers\n",
    "- Data Backup: Store original unscaled data\n",
    "- Time Features: Add cyclical hour encoding\n",
    "- Lag Features: Create time-lagged features (1-3 hours)\n",
    "- Rolling Means: Calculate 3h and 7h rolling averages\n",
    "- Missing Values: Handle using forward and backward fill\n",
    "- Scaling Setup: Initialize StandardScalers\n",
    "- Feature Scaling: Scale features and target variables\n",
    "- Save Artifacts: Store scalers and parameters\n",
    "- Verification: Check scaling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FWybG4Qgqyw",
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Create scalers directory\n",
    "Path('scalers').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store original unscaled data\n",
    "df_original = df_clean.copy()\n",
    "\n",
    "# Extract hour and create cyclical features\n",
    "df_clean['hour'] = df_clean.index.hour\n",
    "df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)\n",
    "df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)\n",
    "\n",
    "# Create lag features using original values\n",
    "for lag in range(1, 4):\n",
    "    df_clean[f'electric_lag_{lag}'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].shift(lag)\n",
    "    df_clean[f'gas_lag_{lag}'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].shift(lag)\n",
    "\n",
    "# Create rolling means using original values\n",
    "df_clean['electric_rolling_mean_3h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df_clean['electric_rolling_mean_7h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "df_clean['gas_rolling_mean_3h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df_clean['gas_rolling_mean_7h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = df_clean.ffill().bfill()\n",
    "\n",
    "# Get original min/max values from unscaled data\n",
    "scaling_params = {\n",
    "    \"electric_mean\": df_original[\"electric_total_consumption_kWh\"].mean(),\n",
    "    \"electric_std\": df_original[\"electric_total_consumption_kWh\"].std(),\n",
    "    \"gas_mean\": df_original[\"gas_total_consumption_kWh\"].mean(),\n",
    "    \"gas_std\": df_original[\"gas_total_consumption_kWh\"].std(),\n",
    "}\n",
    "\n",
    "print(\"\\nOriginal values before scaling:\")\n",
    "for key, value in scaling_params.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_features = StandardScaler()\n",
    "scaler_electric = StandardScaler()\n",
    "scaler_gas = StandardScaler()\n",
    "\n",
    "# Scale features (excluding target variables)\n",
    "feature_columns = [col for col in df_clean.columns if col not in [\n",
    "    'electric_total_consumption_kWh', \n",
    "    'gas_total_consumption_kWh', \n",
    "    'homeid', \n",
    "    'hour'\n",
    "]]\n",
    "\n",
    "print(feature_columns)\n",
    "print(len(feature_columns))\n",
    "\n",
    "df_clean[feature_columns] = scaler_features.fit_transform(df_clean[feature_columns])\n",
    "\n",
    "# Scale target variables using original data\n",
    "df_clean['electric_total_consumption_kWh'] = scaler_electric.fit_transform(\n",
    "    df_original[['electric_total_consumption_kWh']]\n",
    ")\n",
    "df_clean['gas_total_consumption_kWh'] = scaler_gas.fit_transform(\n",
    "    df_original[['gas_total_consumption_kWh']]\n",
    ")\n",
    "\n",
    "# Save scalers\n",
    "dump(scaler_features, 'scalers/scaler_features.pkl')\n",
    "dump(df_clean['electric_total_consumption_kWh'], 'scalers/scaler_electric.pkl')\n",
    "dump(df_clean['gas_total_consumption_kWh'], 'scalers/scaler_gas.pkl')\n",
    "dump(scaling_params, 'scalers/scaling_params.pkl')  # Save mean and std for later inverse transformation\n",
    "\n",
    "# Verify scaling with mean and standard deviation\n",
    "print(\"\\nVerifying scalers:\")\n",
    "for name, scaler in [('Electric', scaler_electric), ('Gas', scaler_gas)]:\n",
    "    print(f\"\\n{name} scaler:\")\n",
    "    print(f\"Mean: {scaler.mean_[0]:.6f}\")\n",
    "    print(f\"Standard deviation: {scaler.scale_[0]:.6f}\")\n",
    "\n",
    "# Save preprocessed data\n",
    "# df_clean.to_parquet('lstm_preprocessed_data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8LZ_5OCgqyw"
   },
   "source": [
    "### 3. Time Series Data Preparation for LSTM Models ###\n",
    "- Parameter Definition: Set sequence length and identify feature columns\n",
    "- Memory Management: Create memory-mapped arrays for efficient data handling\n",
    "- Data Processing: Implement chunk-based processing for large datasets\n",
    "- Sequence Creation: Generate time series sequences for LSTM input\n",
    "- Data Split: Create train-test sets while maintaining temporal order\n",
    "- Coverage Analysis: Track household representation in datasets\n",
    "- Shape Verification: Confirm proper data structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9serancgqyw",
    "outputId": "5bdd6e36-12ae-498d-c95b-b489b718085b"
   },
   "outputs": [],
   "source": [
    "# Define parameters and columns\n",
    "time_steps = 90\n",
    "\n",
    "# Features and target definition (Strom und Gas)\n",
    "feature_columns = [col for col in df_clean.columns if col not in ['electric_total_consumption_kWh', 'gas_total_consumption_kWh', 'homeid', 'roomid']]\n",
    "target_column_electric = 'electric_total_consumption_kWh'\n",
    "target_column_gas = 'gas_total_consumption_kWh'\n",
    "\n",
    "def create_memmap_array(shape, filename, dtype='float32'):\n",
    "    path = Path('temp_arrays')\n",
    "    path.mkdir(exist_ok=True)\n",
    "    return np.memmap(path / filename, dtype=dtype, mode='w+', shape=shape)\n",
    "\n",
    "def process_data_efficiently(df_clean, target_column, feature_columns, time_steps, prefix):\n",
    "    total_sequences = len(df_clean) - time_steps\n",
    "    n_features = len(feature_columns)\n",
    "    \n",
    "    # Create memory-mapped arrays\n",
    "    X = create_memmap_array((total_sequences, time_steps, n_features), f'{prefix}_X.mmap')\n",
    "    y = create_memmap_array((total_sequences,), f'{prefix}_y.mmap')\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    chunk_size = 500\n",
    "    feature_data = df_clean[feature_columns].values\n",
    "    target_data = df_clean[target_column].values\n",
    "    \n",
    "    print(f\"Processing {prefix} data...\")\n",
    "    for chunk_start in range(0, total_sequences, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_sequences)\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            X[i] = feature_data[i:i + time_steps]\n",
    "            y[i] = target_data[i + time_steps]\n",
    "            \n",
    "        if chunk_start % (chunk_size * 10) == 0:\n",
    "            print(f\"Progress: {chunk_start/total_sequences*100:.1f}%\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Process gas data\n",
    "time_steps = 90\n",
    "feature_columns = [col for col in df_clean.columns if col not in \n",
    "                  ['electric_total_consumption_kWh', 'gas_total_consumption_kWh', 'homeid', 'roomid']]\n",
    "\n",
    "\n",
    "\n",
    "# Process gas data\n",
    "X_gas, y_gas = process_data_efficiently(\n",
    "    df_clean,\n",
    "    'gas_total_consumption_kWh',\n",
    "    feature_columns,\n",
    "    time_steps,\n",
    "    'gas'\n",
    ")\n",
    "\n",
    "# Split datasets\n",
    "X_train_gas, X_test_gas, y_train_gas, y_test_gas = train_test_split(\n",
    "    X_gas, y_gas, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Check how many households are included in the training and test set\n",
    "train_households_gas = df_clean.loc[df_clean.index[-len(y_train_gas):], 'homeid'].nunique()\n",
    "test_households_gas = df_clean.loc[df_clean.index[-len(y_test_gas):], 'homeid'].nunique()\n",
    "\n",
    "print(f\" Haushalte im Training (Gas): {train_households_gas}\")\n",
    "print(f\" Haushalte im Test (Gas): {test_households_gas}\")\n",
    "# Needs to be further investigated\n",
    "print(f\" Verlust an Haushalten: {254 - test_households_gas}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show shapes of the data\n",
    "train_test_summary = {\n",
    "    'X_train_gas': X_train_gas.shape,\n",
    "    'X_test_gas': X_test_gas.shape,\n",
    "}\n",
    "train_test_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E5KwcVmgqyx"
   },
   "source": [
    "### 4. LSTM Model Implementation and Training Pipeline ###\n",
    "- LSTM Architecture: Custom LSTM model with configurable layers and dropout\n",
    "- Device Management: MPS (Metal Performance Shaders) support for Mac\n",
    "- Data Handling: Efficient data loading and batch processing\n",
    "- Training Features: Early stopping, Model checkpointing, Progress monitoring and Loss tracking\n",
    "- Optimization: Adam optimizer with configurable learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Add dropout parameter to LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0  # Only apply dropout with multiple layers\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and configure MPS device for Mac\n",
    "has_mps = torch.backends.mps.is_available()\n",
    "if has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not found, using CPU\")\n",
    "\n",
    "# Improved data preparation with explicit float32\n",
    "def prepare_data_for_training(X, y, batch_size=8):\n",
    "    X_tensor = torch.FloatTensor(X).to(torch.float32)\n",
    "    y_tensor = torch.FloatTensor(y).reshape(-1, 1).to(torch.float32)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Early stopping handler\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Improved training function with early stopping\n",
    "def train_model(model, train_loader, model_name, num_epochs=10):\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    early_stopping = EarlyStopping(patience=2)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=f'Training {model_name}', position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        batch_pbar = tqdm(train_loader, \n",
    "                         desc=f'Epoch {epoch+1}/{num_epochs}',\n",
    "                         leave=False, \n",
    "                         position=1)\n",
    "        \n",
    "        for inputs, targets in batch_pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_pbar.set_postfix({'loss': f'{epoch_loss:.4f}'})\n",
    "\n",
    "        # Save checkpoint for each epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, os.path.join(checkpoint_dir, f'{model_name}_epoch{epoch+1}.pt'))\n",
    "        print(f\"\\nSaved checkpoint for epoch {epoch+1}\")\n",
    "\n",
    "        # Save best model separately\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, os.path.join(checkpoint_dir, f'{model_name}_best.pt'))\n",
    "            print(f\"New best model saved! Loss: {best_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        early_stopping(epoch_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n{model_name} training complete! Best loss: {best_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Helper function to load trained models\n",
    "def load_trained_model(model, model_name):\n",
    "    checkpoint_path = os.path.join('checkpoints', f'{model_name}_best.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Training pipeline\n",
    "print(\"Preparing data loaders...\")\n",
    "train_loader_gas = prepare_data_for_training(X_train_gas, y_train_gas)\n",
    "\n",
    "print(\"\\nTraining Gas Model...\")\n",
    "model_gas = LSTMModel(input_size=len(feature_columns), \n",
    "                      hidden_size=32, \n",
    "                      num_layers=2, \n",
    "                      output_size=1).to(device)\n",
    "model_gas = train_model(model_gas, train_loader_gas, \"gas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_78qhv1gqyx"
   },
   "source": [
    "### 5. Model Evaluation and Visualization Pipeline ###\n",
    "- Core Functions and Utilities\n",
    "- Model Loading and Evaluation\n",
    "- Visualization Functions\n",
    "- Main Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup with error handling\n",
    "def get_device():\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            print(\"MPS not available, falling back to CPU\")\n",
    "            return torch.device(\"cpu\")\n",
    "    except:\n",
    "        print(\"Error checking MPS, falling back to CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def calculate_metrics(predictions, actuals, model_name=None):\n",
    "    try:\n",
    "        # Basic metrics\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        # MAPE calculation with protection against small values\n",
    "        mask = np.abs(actuals) > np.percentile(np.abs(actuals), 5)  # Werte unter dem 5. Perzentil filtern\n",
    "        if np.any(mask):\n",
    "            mape = np.mean(np.abs((actuals[mask] - predictions[mask]) / \n",
    "                                 np.maximum(actuals[mask], 1e-10))) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "            print(f\" Warning: No valid values for MAPE calculation in {model_name}\")\n",
    "        \n",
    "        # SMAPE calculation (more robust against zero/small values)\n",
    "        denominator = (np.abs(actuals) + np.abs(predictions))\n",
    "        mask_smape = denominator >= 1e-10\n",
    "        if np.any(mask_smape):\n",
    "            smape = 100 * np.mean(2 * np.abs(predictions[mask_smape] - actuals[mask_smape]) / \n",
    "                                denominator[mask_smape])\n",
    "        else:\n",
    "            smape = np.nan\n",
    "        \n",
    "        # Baseline comparison\n",
    "        baseline_mse = mean_squared_error(actuals, np.full_like(actuals, np.mean(actuals)))\n",
    "        if mse > baseline_mse and model_name:\n",
    "            print(f\" Warning: {model_name} MSE ({mse:.2f}) is higher than baseline MSE ({baseline_mse:.2f})\")\n",
    "        \n",
    "        metrics = {\n",
    "            'RMSE': float(f\"{rmse:.4f}\"),\n",
    "            'MSE': float(f\"{mse:.4f}\"),\n",
    "            'MAE': float(f\"{mae:.4f}\"),\n",
    "            'R2': float(f\"{r2:.4f}\"),\n",
    "            'MAPE': float(f\"{mape:.4f}\") if not np.isnan(mape) else np.nan,\n",
    "            'SMAPE': float(f\"{smape:.4f}\") if not np.isnan(smape) else np.nan\n",
    "        }\n",
    "        \n",
    "        # Print additional insights\n",
    "        if model_name:\n",
    "            print(f\"\\nDetailed metrics for {model_name}:\")\n",
    "            print(f\"- MAPE: {metrics['MAPE']:.2f}% (traditional)\")\n",
    "            print(f\"- SMAPE: {metrics['SMAPE']:.2f}% (symmetric)\")\n",
    "            print(f\"- Values near zero: {(~mask).sum()} of {len(actuals)}\")\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error calculating metrics: {e}\")\n",
    "        return None\n",
    "# Inverse scaling with error handling\n",
    "def inverse_scale_predictions(predictions, actuals, scaler):\n",
    "    try:\n",
    "        pred_orig = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "        act_orig = scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "        return pred_orig, act_orig\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inverse scaling: {e}\")\n",
    "        return predictions.flatten(), actuals.flatten()\n",
    "\n",
    "# Enhanced model evaluation with additional visualizations\n",
    "def evaluate_model(model, model_name, test_loader, scaler):\n",
    "    print(f\"\\nEvaluating {model_name} model...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    # Prediction loop with error handling\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            for inputs, targets in tqdm(test_loader, desc=\"Making predictions\"):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "                actuals.append(targets.cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    # Process results\n",
    "    predictions = np.concatenate(predictions)\n",
    "    actuals = np.concatenate(actuals)\n",
    "\n",
    "    print(f\"Predictions:: {predictions}\")\n",
    "    print(f\"Ground Truth:: {actuals}\")\n",
    "    \n",
    "    # Inverse scaling\n",
    "    # predictions, actuals = inverse_scale_predictions(predictions, actuals, scaler)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(predictions, actuals)\n",
    "    \n",
    "    # Create formatted metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': list(metrics.keys()),\n",
    "        'Value': [f\"{v:.4f}\" for v in metrics.values()]\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMetrics for {model_name} Energy Consumption:\")\n",
    "    print(tabulate(metrics_df, headers='keys', tablefmt='pipe', showindex=False))\n",
    "    \n",
    "    return predictions, actuals, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model, model_name):\n",
    "    try:\n",
    "        checkpoint_path = os.path.join('checkpoints', f'{model_name}_best.pt')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\" Loaded best model for {model_name} from epoch {checkpoint['epoch']}\")\n",
    "        else:\n",
    "            print(f\" No checkpoint found for {model_name}, using untrained model\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading model {model_name}: {e}\")\n",
    "        return model\n",
    "\n",
    "def prepare_evaluation_data():\n",
    "    try:\n",
    "        # Load scalers\n",
    "        scalers = {'Gas': load('scalers/scaler_gas.pkl')}\n",
    "        \n",
    "        # Initialize models\n",
    "        models = {\n",
    "            'Gas': LSTMModel(input_size=len(feature_columns), \n",
    "                           hidden_size=32, \n",
    "                           num_layers=2, \n",
    "                           output_size=1).to(device)\n",
    "        }\n",
    "        \n",
    "        # Prepare test dataloaders\n",
    "        test_data = {'Gas': (X_test_gas, y_test_gas)}\n",
    "        \n",
    "        test_loaders = {\n",
    "            name: torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(\n",
    "                    torch.FloatTensor(X_test).to(torch.float32),\n",
    "                    torch.FloatTensor(y_test).reshape(-1, 1).to(torch.float32)\n",
    "                ),\n",
    "                batch_size=8,\n",
    "                shuffle=False\n",
    "            ) for name, (X_test, y_test) in test_data.items()\n",
    "        }\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {\n",
    "            name: {\n",
    "                'household_ids': df_clean.loc[df_clean.index[-len(y_test):], 'homeid'].values[:len(y_test)],\n",
    "                'timestamps': df_clean.index[-len(y_test):].values[:len(y_test)]\n",
    "            } for name, (_, y_test) in test_data.items()\n",
    "        }\n",
    "        \n",
    "        return models, scalers, test_loaders, metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error preparing evaluation data: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_model_evaluation():\n",
    "    evaluation_data = prepare_evaluation_data()\n",
    "    if evaluation_data is None:\n",
    "        return\n",
    "    \n",
    "    models, scalers, test_loaders, metadata = evaluation_data\n",
    "    results = {}\n",
    "    \n",
    "    # Create reports directory\n",
    "    reports_dir = Path('../reports/tables')\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            model = load_trained_model(model, name.lower())\n",
    "            predictions, actuals, raw_metrics = evaluate_model(\n",
    "                model=model,\n",
    "                model_name=name,\n",
    "                test_loader=test_loaders[name],\n",
    "                scaler=scalers[name]\n",
    "            )\n",
    "            \n",
    "            if predictions is not None:\n",
    "                metrics = calculate_metrics(predictions, actuals, name)\n",
    "                if metrics is None:\n",
    "                    continue\n",
    "                    \n",
    "                results[name] = {\n",
    "                    'predictions': predictions,\n",
    "                    'actuals': actuals,\n",
    "                    'metrics': metrics\n",
    "                }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" Error evaluating {name} model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_per_household(predictions, actuals, household_ids, timestamps, \n",
    "                                 model_name, window_size=24, max_houses=5, \n",
    "                                 show_plots=False, save_plots=True):\n",
    "\n",
    "    try:\n",
    "        # Debug shapes\n",
    "        print(\"Input array shapes:\")\n",
    "        print(f\"Predictions: {np.shape(predictions)}\")\n",
    "        print(f\"Actuals: {np.shape(actuals)}\")\n",
    "        print(f\"Household IDs: {np.shape(household_ids)}\")\n",
    "        print(f\"Timestamps: {np.shape(timestamps)}\")\n",
    "        \n",
    "        # Ensure 1D arrays\n",
    "        predictions = np.ravel(predictions)\n",
    "        actuals = np.ravel(actuals)\n",
    "        household_ids = np.ravel(household_ids)\n",
    "        timestamps = np.ravel(timestamps)\n",
    "        \n",
    "        # Convert inputs to pandas DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'predictions': predictions,\n",
    "            'actuals': actuals,\n",
    "            'household_id': household_ids,\n",
    "            'timestamp': pd.to_datetime(timestamps)\n",
    "        }).set_index('timestamp')\n",
    "        \n",
    "        # Verify DataFrame creation\n",
    "        print(\"\\nDataFrame info:\")\n",
    "        print(df.info())\n",
    "        \n",
    "        # Get unique households\n",
    "        unique_homes = df['household_id'].unique()\n",
    "        if len(unique_homes) > max_houses:\n",
    "            unique_homes = np.random.choice(unique_homes, max_houses, replace=False)\n",
    "        \n",
    "        # Plot directory setup\n",
    "        if save_plots:\n",
    "            plot_dir = Path('../reports/figures')\n",
    "            plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Plot each household\n",
    "        for home in unique_homes:\n",
    "            house_data = df[df['household_id'] == home].copy()\n",
    "            \n",
    "            if len(house_data) < window_size:\n",
    "                continue\n",
    "            \n",
    "            create_household_plot(\n",
    "                house_data, \n",
    "                home, \n",
    "                model_name, \n",
    "                window_size, \n",
    "                show_plots, \n",
    "                save_plots, \n",
    "                plot_dir\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Error during plotting: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_household_plot(data, home, model_name, window_size, show_plots, save_plots, plot_dir, \n",
    "                         resample_rule='1H'):  # Add resampling parameter\n",
    "    \n",
    "    # Resample data before plotting\n",
    "    data_resampled = data.resample(resample_rule).mean()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[3, 1])\n",
    "    \n",
    "    # Apply smoothing to resampled data\n",
    "    data_resampled['actual_smooth'] = data_resampled['actuals'].rolling(\n",
    "        window=max(2, window_size//6),  # Adjust window size for resampled data\n",
    "        min_periods=1\n",
    "    ).mean()\n",
    "    \n",
    "    data_resampled['pred_smooth'] = data_resampled['predictions'].rolling(\n",
    "        window=max(2, window_size//6),\n",
    "        min_periods=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Main plot with resampled data\n",
    "    ax1.plot(data_resampled.index, data_resampled['actual_smooth'], \n",
    "            label=\"Actual\", color='#2ecc71', linewidth=2)\n",
    "    ax1.plot(data_resampled.index, data_resampled['pred_smooth'], \n",
    "            label=\"Predicted\", color='#e74c3c', linewidth=2, linestyle=\"dashed\")\n",
    "    \n",
    "    # Error plot with resampled data\n",
    "    error = data_resampled['predictions'] - data_resampled['actuals']\n",
    "    ax2.plot(data_resampled.index, error, color='#3498db', alpha=0.6, label='Prediction Error')\n",
    "    ax2.axhline(y=0, color='#95a5a6', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add data reduction info to title\n",
    "    format_plots(ax1, ax2, home, model_name, \n",
    "                len(data_resampled), resample_rule)\n",
    "    \n",
    "    if save_plots:\n",
    "        save_path = plot_dir / f'{model_name.lower()}_household_{home}_predictions.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_predictions_aggregated(predictions, actuals, timestamps, model_name, \n",
    "                              window_size=24, show_plots=False, save_plots=True):\n",
    "    \n",
    "    # Ensure 1D arrays\n",
    "    predictions = np.ravel(predictions)\n",
    "    actuals = np.ravel(actuals)\n",
    "    timestamps = np.ravel(timestamps)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Convert to pandas Series\n",
    "    time_series = pd.to_datetime(timestamps)\n",
    "    actual_series = pd.Series(actuals, index=time_series)\n",
    "    pred_series = pd.Series(predictions, index=time_series)\n",
    "    \n",
    "    # Calculate rolling means\n",
    "    actual_smooth = actual_series.rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "    pred_smooth = pred_series.rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(actual_smooth, label='Actual (Smoothed)', color='green', alpha=0.6)\n",
    "    plt.plot(pred_smooth, label='Predicted (Smoothed)', color='red', alpha=0.6)\n",
    "    \n",
    "    plt.title(f'{model_name} Model: Aggregated Predictions vs Actuals')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Consumption (kWh)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Save plot if requested\n",
    "    if save_plots:\n",
    "        plot_dir = Path('../reports/figures')\n",
    "        plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(plot_dir / f'{model_name.lower()}_aggregated_predictions.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Show plot if requested\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def format_plots(ax1, ax2, home, model_name, data_points, resample_rule):\n",
    "    \"\"\"Format plot axes and labels with resampling info\"\"\"\n",
    "    ax1.set_title(f\"{model_name} Consumption - Household {home}\\n\"\n",
    "                 f\"Data points: {data_points} (Resampled to {resample_rule})\", \n",
    "                 fontsize=14, pad=20)\n",
    "    ax1.set_ylabel(\"Consumption (kWh)\", fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(fontsize=10)\n",
    "    \n",
    "    ax2.set_xlabel(\"Time\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Error (kWh)\", fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(fontsize=10)\n",
    "    \n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.tick_params(axis='both', labelsize=10)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results, metadata = run_model_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots\n",
    "for name in results:\n",
    "    plot_predictions_per_household(\n",
    "        predictions=results[name]['predictions'],\n",
    "        actuals=results[name]['actuals'],\n",
    "        household_ids=metadata[name]['household_ids'],\n",
    "        timestamps=metadata[name]['timestamps'],\n",
    "        model_name=name,\n",
    "        max_houses=5,\n",
    "        show_plots=True\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    time_series = pd.to_datetime(metadata[name]['timestamps'])\n",
    "    actual_series = pd.Series(results[name]['actuals'].flatten(), index=time_series)\n",
    "    pred_series = pd.Series(results[name]['predictions'].flatten(), index=time_series)\n",
    "    \n",
    "    # Calculate rolling means with 24-hour window\n",
    "    actual_smooth = actual_series.rolling(window=24, center=True, min_periods=1).mean()\n",
    "    pred_smooth = pred_series.rolling(window=24, center=True, min_periods=1).mean()\n",
    "    \n",
    "    plt.plot(actual_smooth, label='Actual (Smoothed)', color='green', alpha=0.6)\n",
    "    plt.plot(pred_smooth, label='Predicted (Smoothed)', color='red', alpha=0.6)\n",
    "    \n",
    "    plt.title(f'{name} Model: Aggregated Predictions vs Actuals')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Consumption (kWh)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plot_predictions_aggregated(\n",
    "        predictions=results[name]['predictions'],\n",
    "        actuals=results[name]['actuals'],\n",
    "        timestamps=metadata[name]['timestamps'],\n",
    "        model_name=name,\n",
    "        window_size=24,\n",
    "        show_plots=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SHAP Values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "\n",
    "\n",
    "# def plot_shap_results(shap_values, X_sample, feature_names, model_name):\n",
    "#     \"\"\"\n",
    "#     Plot SHAP results including summary and dependence plots\n",
    "#     try:\n",
    "#         # Create plots directory\n",
    "#         plot_dir = Path('../reports/figures/shap')\n",
    "#         plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         # Summary plot\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         shap.summary_plot(shap_values, X_sample, feature_names=feature_names,\n",
    "#                          show=False, plot_size=(10, 8))\n",
    "#         plt.title(f'SHAP Summary Plot - {model_name} Model')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(plot_dir / f'{model_name.lower()}_shap_summary.png', \n",
    "#                    bbox_inches='tight', dpi=300)\n",
    "#         plt.close()\n",
    "        \n",
    "#         # Feature importance plot\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         shap.summary_plot(shap_values, X_sample, feature_names=feature_names,\n",
    "#                          plot_type=\"bar\", show=False)\n",
    "#         plt.title(f'SHAP Feature Importance - {model_name} Model')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(plot_dir / f'{model_name.lower()}_shap_importance.png', \n",
    "#                    bbox_inches='tight', dpi=300)\n",
    "#         plt.close()\n",
    "        \n",
    "#         # Dependence plots for top features\n",
    "#         mean_shap_values = np.abs(shap_values).mean(0)\n",
    "#         top_features_idx = mean_shap_values.argsort()[-5:][::-1]\n",
    "        \n",
    "#         for idx in top_features_idx:\n",
    "#             plt.figure(figsize=(8, 6))\n",
    "#             shap.dependence_plot(idx, shap_values, X_sample, \n",
    "#                                feature_names=feature_names, show=False)\n",
    "#             plt.title(f'SHAP Dependence Plot - {feature_names[idx]}')\n",
    "#             plt.tight_layout()\n",
    "#             plt.savefig(plot_dir / f'{model_name.lower()}_shap_dependence_{feature_names[idx]}.png',\n",
    "#                        bbox_inches='tight', dpi=300)\n",
    "#             plt.close()\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating SHAP plots: {e}\")\n",
    "\n",
    "# def generate_shap_values(model, X_test, feature_columns, device, n_samples=500, background_size=100):\n",
    "#     print(\"Preparing SHAP analysis...\")\n",
    "    \n",
    "#     # Move model to CPU and set to eval mode\n",
    "#     model = model.cpu()\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Calculate minimum required samples (features + buffer)\n",
    "#     n_features = X_test.shape[1] * X_test.shape[2] if len(X_test.shape) == 3 else X_test.shape[1]\n",
    "#     min_samples = n_features + 50  # Add buffer\n",
    "    \n",
    "#     # Adjust sample sizes if needed\n",
    "#     n_samples = max(n_samples, min_samples)\n",
    "#     background_size = max(background_size, min_samples // 2)\n",
    "    \n",
    "#     print(f\"Using {n_samples} samples and {background_size} background samples\")\n",
    "    \n",
    "#     # Reshape 3D input (samples, sequence_length, features) to 2D\n",
    "#     if len(X_test.shape) == 3:\n",
    "#         print(f\"Reshaping input from {X_test.shape} to 2D...\")\n",
    "#         X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "#         feature_names = [f\"{feat}_t{t}\" \n",
    "#                         for feat in feature_columns \n",
    "#                         for t in range(X_test.shape[1])]\n",
    "#     else:\n",
    "#         X_test_reshaped = X_test\n",
    "#         feature_names = feature_columns\n",
    "    \n",
    "#     def model_predict(x):\n",
    "#         try:\n",
    "#             # Reshape back to 3D for LSTM\n",
    "#             if len(X_test.shape) == 3:\n",
    "#                 x = x.reshape(-1, X_test.shape[1], X_test.shape[2])\n",
    "            \n",
    "#             # Convert to tensor and get predictions\n",
    "#             x_tensor = torch.FloatTensor(x)\n",
    "#             with torch.no_grad():\n",
    "#                 return model(x_tensor).numpy()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in prediction: {e}\")\n",
    "#             return None\n",
    "\n",
    "#     try:\n",
    "#         # Select data subsets\n",
    "#         background_data = X_test_reshaped[:background_size]\n",
    "#         X_sample = X_test_reshaped[:n_samples]\n",
    "        \n",
    "#         # Estimate noise variance for LassoLarsIC\n",
    "#         y_pred = model_predict(background_data)\n",
    "#         noise_variance = np.var(y_pred)\n",
    "        \n",
    "#         print(\"Creating KernelExplainer...\")\n",
    "#         explainer = shap.KernelExplainer(\n",
    "#             model_predict, \n",
    "#             background_data,\n",
    "#             link=\"identity\",\n",
    "#             l1_reg=\"num_features(10)\",\n",
    "#             noise_level=noise_variance\n",
    "#         )\n",
    "        \n",
    "#         print(\"Computing SHAP values...\")\n",
    "#         shap_values = explainer.shap_values(\n",
    "#             X_sample, \n",
    "#             nsamples=200,  # Increased number of samples\n",
    "#             l1_reg=\"num_features(10)\"  # Regularization to handle high dimensionality\n",
    "#         )\n",
    "        \n",
    "#         return shap_values, feature_names, X_sample\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in SHAP analysis: {e}\")\n",
    "#         print(f\"Error details: {str(e)}\")\n",
    "#         return None, None, None\n",
    "\n",
    "# # Test with adjusted sample sizes\n",
    "# print(f\"X_test_gas shape: {X_test_gas.shape}\")\n",
    "# shap_values, features, X_sample = generate_shap_values(\n",
    "#     model=model_gas,\n",
    "#     X_test=X_test_gas,\n",
    "#     feature_columns=feature_columns,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# if shap_values is not None:\n",
    "#     plot_shap_results(shap_values, X_sample, features, \"Gas\")\n",
    "# else:\n",
    "#     print(\"SHAP analysis failed. Unable to create visualizations.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Predictive_Analytics_Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
