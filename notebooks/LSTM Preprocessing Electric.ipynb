{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sudo-Oliver/Predictive-Analytics-Private/blob/main/notebooks/LSTM%20Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7R7RQPWgqys"
   },
   "source": [
    "**1. Daten laden und vorbereiten**\n",
    "1. Laden der Daten in einen Dataframe\n",
    "2. Zeitspalte umwandeln (Unix-Timestamp -> Datetime)\n",
    "3. nach homeid gruppieren (jeder Haushalt hat seine eigene Zeitreihe)\n",
    "4. Sortieren nach Zeit innerhalb des Haushalts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPGwAXsugqyt",
    "outputId": "3c20df7b-85f1-4803-d5cc-6b5abf07b366"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import gdown\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import dump, load\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Metal plugin available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Metal GPU will be used\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Metal plugin available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Configure memory growth\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(\"Metal GPU will be used\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oD_P2DPgqyv"
   },
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     \"\"\"Clean and preprocess sensor data\"\"\"\n",
    "#     # Convert Unix timestamp to datetime\n",
    "#     df['timestamp_local'] = pd.to_datetime(df['timestamp_local'], unit='ms')\n",
    "\n",
    "#     # Set timestamp_local as index\n",
    "#     df.set_index('timestamp_local', inplace=True)\n",
    "\n",
    "#     # Sort by homeid and timestamp_local\n",
    "#     df = df.sort_values(by=['homeid', 'timestamp_local'])\n",
    "\n",
    "#     # Remove specified columns\n",
    "#     columns_to_drop = [\n",
    "#         'sensorid', 'median_temperature', '_room',\n",
    "#         'sensorid_room', 'measured_entity',\n",
    "#         'sensorid_electric', 'sensorid_gas'\n",
    "#     ]\n",
    "#     df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully: (1641653, 23) rows\n",
      "\n",
      "Cleaning data...\n",
      "\n",
      "Creating features...\n",
      "Handling missing values...\n",
      "\n",
      "Data shape after preprocessing: (1641653, 28)\n",
      "Index type: <class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
      "Feature columns: ['homeid', 'electric_min_consumption', 'electric_max_consumption', 'std_consumption', 'electric_median_consumption', 'electric_total_consumption_kWh', 'gas_mean_consumption', 'gas_min_consumption', 'gas_max_consumption', 'gas_median_consumption', 'gas_total_consumption_kWh', 'median_value', 'roomid', 'income_band_mid', 'education_map', 'hour', 'hour_sin', 'hour_cos', 'electric_lag_1', 'gas_lag_1', 'electric_lag_2', 'gas_lag_2', 'electric_lag_3', 'gas_lag_3', 'electric_rolling_mean_3h', 'electric_rolling_mean_7h', 'gas_rolling_mean_3h', 'gas_rolling_mean_7h']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "timestamp_local",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "homeid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "electric_min_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_max_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_median_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_total_consumption_kWh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_mean_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_min_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_max_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_median_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_total_consumption_kWh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "median_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roomid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "income_band_mid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "education_map",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "hour_sin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour_cos",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_rolling_mean_3h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_rolling_mean_7h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_rolling_mean_3h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_rolling_mean_7h",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "aae9d69f-7445-47b9-b321-b88530054609",
       "rows": [
        [
         "2016-09-20 09:00:00",
         "47",
         "0.069",
         "0.335",
         "0.0339046921378041",
         "0.194",
         "0.1798068371317398",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.224",
         "20.72",
         "652.0",
         "0.0",
         "8.0",
         "9",
         "0.7071067811865476",
         "-0.7071067811865475",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.17669019228437363",
         "0.17045690258964122",
         "0.21",
         "0.182"
        ],
        [
         "2016-09-20 10:00:00",
         "47",
         "0.068875",
         "0.45837500000000003",
         "0.03587545170777",
         "0.18762500000000001",
         "0.1766901922843736",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.21",
         "20.695",
         "652.0",
         "0.0",
         "8.0",
         "10",
         "0.49999999999999994",
         "-0.8660254037844387",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.17669019228437363",
         "0.17045690258964122",
         "0.21",
         "0.182"
        ],
        [
         "2016-09-20 11:00:00",
         "47",
         "0.06875",
         "0.58175",
         "0.0378462112777359",
         "0.18125",
         "0.17357354743700742",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.196",
         "20.669999999999998",
         "652.0",
         "0.0",
         "8.0",
         "11",
         "0.258819045102521",
         "-0.9659258262890682",
         "0.1766901922843736",
         "0.21",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.17669019228437363",
         "0.17045690258964122",
         "0.21",
         "0.182"
        ],
        [
         "2016-09-20 12:00:00",
         "47",
         "0.068625",
         "0.705125",
         "0.0398169708477018",
         "0.174875",
         "0.17045690258964122",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.182",
         "20.645",
         "652.0",
         "0.0",
         "8.0",
         "12",
         "1.2246467991473532e-16",
         "-1.0",
         "0.17357354743700742",
         "0.196",
         "0.1766901922843736",
         "0.21",
         "0.1798068371317398",
         "0.224",
         "0.1735735474370074",
         "0.17045690258964122",
         "0.19600000000000004",
         "0.182"
        ],
        [
         "2016-09-20 13:00:00",
         "47",
         "0.0685",
         "0.8285",
         "0.0417877304176677",
         "0.16849999999999998",
         "0.167340257742275",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.168",
         "20.619999999999997",
         "652.0",
         "0.0",
         "8.0",
         "13",
         "-0.2588190451025208",
         "-0.9659258262890683",
         "0.17045690258964122",
         "0.182",
         "0.17357354743700742",
         "0.196",
         "0.1766901922843736",
         "0.21",
         "0.17045690258964122",
         "0.17045690258964122",
         "0.18200000000000002",
         "0.182"
        ]
       ],
       "shape": {
        "columns": 28,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homeid</th>\n",
       "      <th>electric_min_consumption</th>\n",
       "      <th>electric_max_consumption</th>\n",
       "      <th>std_consumption</th>\n",
       "      <th>electric_median_consumption</th>\n",
       "      <th>electric_total_consumption_kWh</th>\n",
       "      <th>gas_mean_consumption</th>\n",
       "      <th>gas_min_consumption</th>\n",
       "      <th>gas_max_consumption</th>\n",
       "      <th>gas_median_consumption</th>\n",
       "      <th>...</th>\n",
       "      <th>electric_lag_1</th>\n",
       "      <th>gas_lag_1</th>\n",
       "      <th>electric_lag_2</th>\n",
       "      <th>gas_lag_2</th>\n",
       "      <th>electric_lag_3</th>\n",
       "      <th>gas_lag_3</th>\n",
       "      <th>electric_rolling_mean_3h</th>\n",
       "      <th>electric_rolling_mean_7h</th>\n",
       "      <th>gas_rolling_mean_3h</th>\n",
       "      <th>gas_rolling_mean_7h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp_local</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-09-20 09:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.033905</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 10:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068875</td>\n",
       "      <td>0.458375</td>\n",
       "      <td>0.035875</td>\n",
       "      <td>0.187625</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 11:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.581750</td>\n",
       "      <td>0.037846</td>\n",
       "      <td>0.181250</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 12:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068625</td>\n",
       "      <td>0.705125</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.174875</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 13:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.828500</td>\n",
       "      <td>0.041788</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.167340</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     homeid  electric_min_consumption  \\\n",
       "timestamp_local                                         \n",
       "2016-09-20 09:00:00      47                  0.069000   \n",
       "2016-09-20 10:00:00      47                  0.068875   \n",
       "2016-09-20 11:00:00      47                  0.068750   \n",
       "2016-09-20 12:00:00      47                  0.068625   \n",
       "2016-09-20 13:00:00      47                  0.068500   \n",
       "\n",
       "                     electric_max_consumption  std_consumption  \\\n",
       "timestamp_local                                                  \n",
       "2016-09-20 09:00:00                  0.335000         0.033905   \n",
       "2016-09-20 10:00:00                  0.458375         0.035875   \n",
       "2016-09-20 11:00:00                  0.581750         0.037846   \n",
       "2016-09-20 12:00:00                  0.705125         0.039817   \n",
       "2016-09-20 13:00:00                  0.828500         0.041788   \n",
       "\n",
       "                     electric_median_consumption  \\\n",
       "timestamp_local                                    \n",
       "2016-09-20 09:00:00                     0.194000   \n",
       "2016-09-20 10:00:00                     0.187625   \n",
       "2016-09-20 11:00:00                     0.181250   \n",
       "2016-09-20 12:00:00                     0.174875   \n",
       "2016-09-20 13:00:00                     0.168500   \n",
       "\n",
       "                     electric_total_consumption_kWh  gas_mean_consumption  \\\n",
       "timestamp_local                                                             \n",
       "2016-09-20 09:00:00                        0.179807                 0.112   \n",
       "2016-09-20 10:00:00                        0.176690                 0.112   \n",
       "2016-09-20 11:00:00                        0.173574                 0.112   \n",
       "2016-09-20 12:00:00                        0.170457                 0.112   \n",
       "2016-09-20 13:00:00                        0.167340                 0.112   \n",
       "\n",
       "                     gas_min_consumption  gas_max_consumption  \\\n",
       "timestamp_local                                                 \n",
       "2016-09-20 09:00:00                0.112                0.112   \n",
       "2016-09-20 10:00:00                0.112                0.112   \n",
       "2016-09-20 11:00:00                0.112                0.112   \n",
       "2016-09-20 12:00:00                0.112                0.112   \n",
       "2016-09-20 13:00:00                0.112                0.112   \n",
       "\n",
       "                     gas_median_consumption  ...  electric_lag_1  gas_lag_1  \\\n",
       "timestamp_local                              ...                              \n",
       "2016-09-20 09:00:00                   0.112  ...        0.179807      0.224   \n",
       "2016-09-20 10:00:00                   0.112  ...        0.179807      0.224   \n",
       "2016-09-20 11:00:00                   0.112  ...        0.176690      0.210   \n",
       "2016-09-20 12:00:00                   0.112  ...        0.173574      0.196   \n",
       "2016-09-20 13:00:00                   0.112  ...        0.170457      0.182   \n",
       "\n",
       "                     electric_lag_2  gas_lag_2  electric_lag_3  gas_lag_3  \\\n",
       "timestamp_local                                                             \n",
       "2016-09-20 09:00:00        0.179807      0.224        0.179807      0.224   \n",
       "2016-09-20 10:00:00        0.179807      0.224        0.179807      0.224   \n",
       "2016-09-20 11:00:00        0.179807      0.224        0.179807      0.224   \n",
       "2016-09-20 12:00:00        0.176690      0.210        0.179807      0.224   \n",
       "2016-09-20 13:00:00        0.173574      0.196        0.176690      0.210   \n",
       "\n",
       "                     electric_rolling_mean_3h  electric_rolling_mean_7h  \\\n",
       "timestamp_local                                                           \n",
       "2016-09-20 09:00:00                  0.176690                  0.170457   \n",
       "2016-09-20 10:00:00                  0.176690                  0.170457   \n",
       "2016-09-20 11:00:00                  0.176690                  0.170457   \n",
       "2016-09-20 12:00:00                  0.173574                  0.170457   \n",
       "2016-09-20 13:00:00                  0.170457                  0.170457   \n",
       "\n",
       "                     gas_rolling_mean_3h  gas_rolling_mean_7h  \n",
       "timestamp_local                                                \n",
       "2016-09-20 09:00:00                0.210                0.182  \n",
       "2016-09-20 10:00:00                0.210                0.182  \n",
       "2016-09-20 11:00:00                0.210                0.182  \n",
       "2016-09-20 12:00:00                0.196                0.182  \n",
       "2016-09-20 13:00:00                0.182                0.182  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "import gdown\n",
    "\n",
    "def load_processed_data():\n",
    "    \"\"\"Load preprocessed sensor data with fallback to Drive download\"\"\"\n",
    "    file_id = \"1KHQCVfwTxm5bjjITS8WMm9P3M12ETVsR\"\n",
    "    \n",
    "    download_path = Path('data/processed')\n",
    "    download_path.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = download_path / 'final_processed_data3.parquet'\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(\"Downloading from Google Drive...\")\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(url, str(file_path), quiet=False)\n",
    "    \n",
    "    if file_path.exists():\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(f\"Data loaded successfully: {df.shape} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not load or download data file\")\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and preprocess sensor data\"\"\"\n",
    "    # Convert Unix timestamp to datetime if needed\n",
    "    if 'timestamp_local' in df.columns:\n",
    "        df['timestamp_local'] = pd.to_datetime(df['timestamp_local'], unit='ms')\n",
    "        # Sort before setting index\n",
    "        df = df.sort_values(by=['homeid', 'timestamp_local'])\n",
    "        # Then set index\n",
    "        df.set_index('timestamp_local', inplace=True)\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        'sensorid', 'median_temperature', '_room',\n",
    "        'sensorid_room', 'measured_entity',\n",
    "        'sensorid_electric', 'sensorid_gas'\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    \n",
    "    # First rename consumption columns from Wh to kWh\n",
    "    df = df.rename(columns={\n",
    "        'electric_total_consumption_Wh': 'electric_total_consumption_kWh',\n",
    "        'gas_total_consumption_Wh': 'gas_total_consumption_kWh'\n",
    "    })\n",
    "    \n",
    "    # Replace negative values with 0 in gas columns\n",
    "    gas_columns = [\n",
    "        'gas_mean_consumption', \n",
    "        'gas_min_consumption', \n",
    "        'gas_max_consumption',\n",
    "        'gas_median_consumption', \n",
    "        'gas_total_consumption_kWh'\n",
    "    ]\n",
    "    \n",
    "    # Use clip to replace negative values with 0 (more efficient than applymap)\n",
    "    df[gas_columns] = df[gas_columns].clip(lower=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_features(df, df_original):\n",
    "    \"\"\"Create time-based and lag features\"\"\"\n",
    "    # Verify datetime index\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame must have DatetimeIndex\")\n",
    "    \n",
    "    # Cyclical time features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    # Lag features using original values\n",
    "    for lag in range(1, 4):\n",
    "        df[f'electric_lag_{lag}'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].shift(lag)\n",
    "        df[f'gas_lag_{lag}'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].shift(lag)\n",
    "    \n",
    "    # Rolling means using original values\n",
    "    df['electric_rolling_mean_3h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "    df['electric_rolling_mean_7h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    df['gas_rolling_mean_3h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "    df['gas_rolling_mean_7h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main preprocessing pipeline\n",
    "print(\"Loading data...\")\n",
    "df = load_processed_data()\n",
    "\n",
    "print(\"\\nCleaning data...\")\n",
    "df_clean = clean_data(df.copy())\n",
    "df_original = df_clean.copy()  # Keep unmodified copy for feature creation\n",
    "\n",
    "print(\"\\nCreating features...\")\n",
    "df_clean = create_features(df_clean, df_original)\n",
    "\n",
    "print(\"Handling missing values...\")\n",
    "df_clean = df_clean.ffill().bfill()\n",
    "\n",
    "# Verify data\n",
    "print(\"\\nData shape after preprocessing:\", df_clean.shape)\n",
    "print(\"Index type:\", type(df_clean.index))\n",
    "print(\"Feature columns:\", df_clean.columns.tolist())\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "bvXa2w8Ggqyv",
    "outputId": "d8654319-74fb-40c1-c690-abf5f0c0556f"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# from joblib import dump, load\n",
    "# import gdown\n",
    "\n",
    "# def load_processed_data():\n",
    "#     \"\"\"Load preprocessed sensor data with fallback to Drive download\"\"\"\n",
    "#     file_id = \"1KHQCVfwTxm5bjjITS8WMm9P3M12ETVsR\"\n",
    "    \n",
    "#     download_path = Path('data/processed')\n",
    "#     download_path.mkdir(parents=True, exist_ok=True)\n",
    "#     file_path = download_path / 'final_processed_data3.parquet'\n",
    "    \n",
    "#     if not file_path.exists():\n",
    "#         print(\"Downloading from Google Drive...\")\n",
    "#         url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "#         gdown.download(url, str(file_path), quiet=False)\n",
    "    \n",
    "#     if file_path.exists():\n",
    "#         df = pd.read_parquet(file_path)\n",
    "#         print(f\"Data loaded successfully: {df.shape} rows\")\n",
    "#         return df\n",
    "#     else:\n",
    "#         raise FileNotFoundError(\"Could not load or download data file\")\n",
    "\n",
    "# def create_features(df, df_original):\n",
    "#     \"\"\"Create time-based and lag features\"\"\"\n",
    "#     # Cyclical time features\n",
    "#     df['hour'] = df.index.hour\n",
    "#     df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "#     df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "#     # Lag features using original values\n",
    "#     for lag in range(1, 4):\n",
    "#         df[f'electric_lag_{lag}'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].shift(lag)\n",
    "#         df[f'gas_lag_{lag}'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].shift(lag)\n",
    "    \n",
    "#     # Rolling means using original values\n",
    "#     df['electric_rolling_mean_3h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "#     df['electric_rolling_mean_7h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "#     df['gas_rolling_mean_3h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "#     df['gas_rolling_mean_7h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Load data and create copies\n",
    "# print(\"Loading data...\")\n",
    "# df = load_processed_data()\n",
    "# df_clean = df.copy()\n",
    "# df_original = df.copy()  # Keep unmodified copy for feature creation\n",
    "\n",
    "# # Create features\n",
    "# print(\"\\nCreating features...\")\n",
    "# df_clean = create_features(df_clean, df_original)\n",
    "\n",
    "# # Handle missing values\n",
    "# print(\"Handling missing values...\")\n",
    "# df_clean = df_clean.ffill().bfill()\n",
    "\n",
    "# # Verify data\n",
    "# print(\"\\nData shape after preprocessing:\", df_clean.shape)\n",
    "# print(\"\\nFeature columns:\", df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "PSojNn9jgqyw",
    "outputId": "1a694ec4-2460-4986-e79e-4b6beda59b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Feature Correlation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "electric_total_consumption_kWh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_total_consumption_kWh",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "caeabfbb-ed19-4a3f-9d08-59fcbdf8b96e",
       "rows": [
        [
         "electric_total_consumption_kWh",
         "1.0",
         "0.032210991188142836"
        ],
        [
         "electric_rolling_mean_3h",
         "0.9100313160756388",
         "0.033895102836715846"
        ],
        [
         "electric_lag_1",
         "0.8152911899689684",
         "0.031145631508973527"
        ],
        [
         "electric_median_consumption",
         "0.8039026321537704",
         "0.025040525478488743"
        ],
        [
         "electric_rolling_mean_7h",
         "0.7951973862485043",
         "0.03580226184882135"
        ],
        [
         "std_consumption",
         "0.7750318839212954",
         "0.024052570289109152"
        ],
        [
         "electric_lag_2",
         "0.7059717833876442",
         "0.03055058605861383"
        ],
        [
         "electric_max_consumption",
         "0.6930172949513878",
         "0.04279976159803156"
        ],
        [
         "electric_lag_3",
         "0.6295458169890661",
         "0.03003627871681092"
        ],
        [
         "electric_min_consumption",
         "0.5166742860716043",
         "0.0396700797804552"
        ],
        [
         "income_band_mid",
         "0.15442070841511235",
         "0.03456014100539602"
        ],
        [
         "hour",
         "0.11771607001849929",
         "0.0016066641661523786"
        ],
        [
         "hour_sin",
         "0.11401568646886993",
         "0.0005382683243789035"
        ],
        [
         "median_value",
         "0.06626155582748322",
         "0.0030776618529730765"
        ],
        [
         "education_map",
         "0.05402817584363957",
         "0.012455997045961453"
        ],
        [
         "gas_total_consumption_kWh",
         "0.032210991188142836",
         "1.0"
        ],
        [
         "gas_lag_1",
         "0.03205067975127647",
         "0.9981468984709488"
        ],
        [
         "gas_rolling_mean_3h",
         "0.03195737862919121",
         "0.9990907342402742"
        ],
        [
         "gas_lag_2",
         "0.03152847189690646",
         "0.9970069222733274"
        ],
        [
         "gas_lag_3",
         "0.030912347607689",
         "0.9958242448619099"
        ],
        [
         "gas_rolling_mean_7h",
         "0.030865889960066666",
         "0.9973870585928848"
        ],
        [
         "gas_max_consumption",
         "0.02823352450427548",
         "0.9992745930927395"
        ],
        [
         "gas_mean_consumption",
         "0.025709881120095607",
         "0.9592977179253811"
        ],
        [
         "roomid",
         "0.025531138578427186",
         "0.033830527921338734"
        ],
        [
         "gas_median_consumption",
         "0.024411967434895307",
         "0.9405117332540934"
        ],
        [
         "hour_cos",
         "0.02439850631881646",
         "0.0019150250130588956"
        ],
        [
         "gas_min_consumption",
         "0.02397354698486005",
         "0.886737172191856"
        ],
        [
         "homeid",
         "0.023285963778932616",
         "0.03525501444483065"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 28
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>electric_total_consumption_kWh</th>\n",
       "      <th>gas_total_consumption_kWh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>electric_total_consumption_kWh</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_rolling_mean_3h</th>\n",
       "      <td>0.910031</td>\n",
       "      <td>0.033895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_lag_1</th>\n",
       "      <td>0.815291</td>\n",
       "      <td>0.031146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_median_consumption</th>\n",
       "      <td>0.803903</td>\n",
       "      <td>0.025041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_rolling_mean_7h</th>\n",
       "      <td>0.795197</td>\n",
       "      <td>0.035802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_consumption</th>\n",
       "      <td>0.775032</td>\n",
       "      <td>0.024053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_lag_2</th>\n",
       "      <td>0.705972</td>\n",
       "      <td>0.030551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_max_consumption</th>\n",
       "      <td>0.693017</td>\n",
       "      <td>0.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_lag_3</th>\n",
       "      <td>0.629546</td>\n",
       "      <td>0.030036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_min_consumption</th>\n",
       "      <td>0.516674</td>\n",
       "      <td>0.039670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income_band_mid</th>\n",
       "      <td>0.154421</td>\n",
       "      <td>0.034560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>0.117716</td>\n",
       "      <td>0.001607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_sin</th>\n",
       "      <td>0.114016</td>\n",
       "      <td>0.000538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_value</th>\n",
       "      <td>0.066262</td>\n",
       "      <td>0.003078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_map</th>\n",
       "      <td>0.054028</td>\n",
       "      <td>0.012456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_total_consumption_kWh</th>\n",
       "      <td>0.032211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_lag_1</th>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.998147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_rolling_mean_3h</th>\n",
       "      <td>0.031957</td>\n",
       "      <td>0.999091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_lag_2</th>\n",
       "      <td>0.031528</td>\n",
       "      <td>0.997007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_lag_3</th>\n",
       "      <td>0.030912</td>\n",
       "      <td>0.995824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_rolling_mean_7h</th>\n",
       "      <td>0.030866</td>\n",
       "      <td>0.997387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_max_consumption</th>\n",
       "      <td>0.028234</td>\n",
       "      <td>0.999275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_mean_consumption</th>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.959298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roomid</th>\n",
       "      <td>0.025531</td>\n",
       "      <td>0.033831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_median_consumption</th>\n",
       "      <td>0.024412</td>\n",
       "      <td>0.940512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_cos</th>\n",
       "      <td>0.024399</td>\n",
       "      <td>0.001915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_min_consumption</th>\n",
       "      <td>0.023974</td>\n",
       "      <td>0.886737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homeid</th>\n",
       "      <td>0.023286</td>\n",
       "      <td>0.035255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                electric_total_consumption_kWh  \\\n",
       "electric_total_consumption_kWh                        1.000000   \n",
       "electric_rolling_mean_3h                              0.910031   \n",
       "electric_lag_1                                        0.815291   \n",
       "electric_median_consumption                           0.803903   \n",
       "electric_rolling_mean_7h                              0.795197   \n",
       "std_consumption                                       0.775032   \n",
       "electric_lag_2                                        0.705972   \n",
       "electric_max_consumption                              0.693017   \n",
       "electric_lag_3                                        0.629546   \n",
       "electric_min_consumption                              0.516674   \n",
       "income_band_mid                                       0.154421   \n",
       "hour                                                  0.117716   \n",
       "hour_sin                                              0.114016   \n",
       "median_value                                          0.066262   \n",
       "education_map                                         0.054028   \n",
       "gas_total_consumption_kWh                             0.032211   \n",
       "gas_lag_1                                             0.032051   \n",
       "gas_rolling_mean_3h                                   0.031957   \n",
       "gas_lag_2                                             0.031528   \n",
       "gas_lag_3                                             0.030912   \n",
       "gas_rolling_mean_7h                                   0.030866   \n",
       "gas_max_consumption                                   0.028234   \n",
       "gas_mean_consumption                                  0.025710   \n",
       "roomid                                                0.025531   \n",
       "gas_median_consumption                                0.024412   \n",
       "hour_cos                                              0.024399   \n",
       "gas_min_consumption                                   0.023974   \n",
       "homeid                                                0.023286   \n",
       "\n",
       "                                gas_total_consumption_kWh  \n",
       "electric_total_consumption_kWh                   0.032211  \n",
       "electric_rolling_mean_3h                         0.033895  \n",
       "electric_lag_1                                   0.031146  \n",
       "electric_median_consumption                      0.025041  \n",
       "electric_rolling_mean_7h                         0.035802  \n",
       "std_consumption                                  0.024053  \n",
       "electric_lag_2                                   0.030551  \n",
       "electric_max_consumption                         0.042800  \n",
       "electric_lag_3                                   0.030036  \n",
       "electric_min_consumption                         0.039670  \n",
       "income_band_mid                                  0.034560  \n",
       "hour                                             0.001607  \n",
       "hour_sin                                         0.000538  \n",
       "median_value                                     0.003078  \n",
       "education_map                                    0.012456  \n",
       "gas_total_consumption_kWh                        1.000000  \n",
       "gas_lag_1                                        0.998147  \n",
       "gas_rolling_mean_3h                              0.999091  \n",
       "gas_lag_2                                        0.997007  \n",
       "gas_lag_3                                        0.995824  \n",
       "gas_rolling_mean_7h                              0.997387  \n",
       "gas_max_consumption                              0.999275  \n",
       "gas_mean_consumption                             0.959298  \n",
       "roomid                                           0.033831  \n",
       "gas_median_consumption                           0.940512  \n",
       "hour_cos                                         0.001915  \n",
       "gas_min_consumption                              0.886737  \n",
       "homeid                                           0.035255  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vollständige Korrelation mit allen spalten berechnen\n",
    "correlation_matrix_all = df_clean.corr()\n",
    "\n",
    "# Korrelation der Features mit den Zielvariablen (Strom und Gasverbtauch)\n",
    "correlation_target_all = correlation_matrix_all[['electric_total_consumption_kWh', 'gas_total_consumption_kWh']]\n",
    "\n",
    "# Sortieren nach Stärke der Korrelation\n",
    "correlation_target_all_sorted = correlation_target_all.abs().sort_values(by=['electric_total_consumption_kWh', 'gas_total_consumption_kWh'], ascending=False)\n",
    "\n",
    "# Korrelationsergebnisse anzeigen\n",
    "print(\"Full Feature Correlation:\")\n",
    "display(correlation_target_all_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering & Scaling ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2FWybG4Qgqyw",
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original values before scaling:\n",
      "electric_min: 0.00\n",
      "electric_max: 11.65\n",
      "gas_min: 0.00\n",
      "gas_max: 3649.58\n",
      "\n",
      " Verifying scalers:\n",
      "\n",
      "Electric scaler:\n",
      "Data min: 0.00\n",
      "Data max: 11.65\n",
      "Scale: 0.085833\n",
      "\n",
      "Gas scaler:\n",
      "Data min: 0.00\n",
      "Data max: 3649.58\n",
      "Scale: 0.000274\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "\n",
    "# Create scalers directory\n",
    "Path('scalers').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store original unscaled data\n",
    "df_original = df_clean.copy()\n",
    "\n",
    "# Extract hour and create cyclical features\n",
    "df_clean['hour'] = df_clean.index.hour\n",
    "df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)\n",
    "df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)\n",
    "\n",
    "# Create lag features using original values\n",
    "for lag in range(1, 4):\n",
    "    df_clean[f'electric_lag_{lag}'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].shift(lag)\n",
    "    df_clean[f'gas_lag_{lag}'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].shift(lag)\n",
    "\n",
    "# Create rolling means using original values\n",
    "df_clean['electric_rolling_mean_3h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df_clean['electric_rolling_mean_7h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "df_clean['gas_rolling_mean_3h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df_clean['gas_rolling_mean_7h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = df_clean.ffill().bfill()\n",
    "\n",
    "# Get original min/max values from unscaled data\n",
    "scaling_params = {\n",
    "    \"electric_min\": df_original[\"electric_total_consumption_kWh\"].min(),\n",
    "    \"electric_max\": df_original[\"electric_total_consumption_kWh\"].max(),\n",
    "    \"gas_min\": df_original[\"gas_total_consumption_kWh\"].min(),\n",
    "    \"gas_max\": df_original[\"gas_total_consumption_kWh\"].max(),\n",
    "}\n",
    "\n",
    "print(\"\\n Original values before scaling:\")\n",
    "for key, value in scaling_params.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "# Initialize scalers with fixed range\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_electric = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_gas = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale features (excluding target variables)\n",
    "feature_columns = [col for col in df_clean.columns if col not in ['electric_total_consumption_kWh', 'gas_total_consumption_kWh', 'homeid', 'hour']]\n",
    "df_clean[feature_columns] = scaler_features.fit_transform(df_clean[feature_columns])\n",
    "\n",
    "# Scale target variables using original data\n",
    "df_clean['electric_total_consumption_kWh'] = scaler_electric.fit_transform(df_original[['electric_total_consumption_kWh']])\n",
    "df_clean['gas_total_consumption_kWh'] = scaler_gas.fit_transform(df_original[['gas_total_consumption_kWh']])\n",
    "\n",
    "# Save scalers\n",
    "dump(scaler_features, 'scalers/scaler_features.pkl')\n",
    "dump(scaler_electric, 'scalers/scaler_electric.pkl')\n",
    "dump(scaler_gas, 'scalers/scaler_gas.pkl')\n",
    "dump(scaling_params, 'scalers/scaling_params.pkl')  # Speichert Originalwerte für spätere Rückskalierung\n",
    "\n",
    "# Verify scaling\n",
    "print(\"\\n Verifying scalers:\")\n",
    "for name, scaler in [('Electric', scaler_electric), ('Gas', scaler_gas)]:\n",
    "    print(f\"\\n{name} scaler:\")\n",
    "    print(f\"Data min: {scaler.data_min_[0]:.2f}\")\n",
    "    print(f\"Data max: {scaler.data_max_[0]:.2f}\")\n",
    "    print(f\"Scale: {scaler.scale_[0]:.6f}\")\n",
    "\n",
    "# Save preprocessed data\n",
    "#df_clean.to_parquet('lstm_preprocessed_data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8LZ_5OCgqyw"
   },
   "source": [
    "### 3. Trainings und Testdatensätze erstellen ###\n",
    "Erstellung von X & y für das LSTM:\n",
    "- time_steps = 90 (Vergangene 90 Werte werden für Vorhersagen genutzt)\n",
    "- create_memmap_array() nutzt Memory-Mapped-Files → Speicheroptimierung\n",
    "- process_data_efficiently() verarbeitet die Daten in Chunks → verhindert RAM-Überlastung\n",
    "- Zielwerte (y) korrekt gesetzt auf die nächsten 90 Schritte\n",
    "\n",
    "Train-Test-Split (80%-20%):\n",
    "- Kein Shuffle, damit die Zeitreihenstruktur erhalten bleibt.\n",
    "- Saubere Trennung für beide Zielvariablen (electric_total_consumption_kWh & gas_total_consumption_kWh).\n",
    "- Finale Shapes werden überprüft und ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9serancgqyw",
    "outputId": "5bdd6e36-12ae-498d-c95b-b489b718085b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing electric data...\n",
      "Progress: 0.0%\n",
      "Progress: 0.3%\n",
      "Progress: 0.6%\n",
      "Progress: 0.9%\n",
      "Progress: 1.2%\n",
      "Progress: 1.5%\n",
      "Progress: 1.8%\n",
      "Progress: 2.1%\n",
      "Progress: 2.4%\n",
      "Progress: 2.7%\n",
      "Progress: 3.0%\n",
      "Progress: 3.4%\n",
      "Progress: 3.7%\n",
      "Progress: 4.0%\n",
      "Progress: 4.3%\n",
      "Progress: 4.6%\n",
      "Progress: 4.9%\n",
      "Progress: 5.2%\n",
      "Progress: 5.5%\n",
      "Progress: 5.8%\n",
      "Progress: 6.1%\n",
      "Progress: 6.4%\n",
      "Progress: 6.7%\n",
      "Progress: 7.0%\n",
      "Progress: 7.3%\n",
      "Progress: 7.6%\n",
      "Progress: 7.9%\n",
      "Progress: 8.2%\n",
      "Progress: 8.5%\n",
      "Progress: 8.8%\n",
      "Progress: 9.1%\n",
      "Progress: 9.4%\n",
      "Progress: 9.7%\n",
      "Progress: 10.1%\n",
      "Progress: 10.4%\n",
      "Progress: 10.7%\n",
      "Progress: 11.0%\n",
      "Progress: 11.3%\n",
      "Progress: 11.6%\n",
      "Progress: 11.9%\n",
      "Progress: 12.2%\n",
      "Progress: 12.5%\n",
      "Progress: 12.8%\n",
      "Progress: 13.1%\n",
      "Progress: 13.4%\n",
      "Progress: 13.7%\n",
      "Progress: 14.0%\n",
      "Progress: 14.3%\n",
      "Progress: 14.6%\n",
      "Progress: 14.9%\n",
      "Progress: 15.2%\n",
      "Progress: 15.5%\n",
      "Progress: 15.8%\n",
      "Progress: 16.1%\n",
      "Progress: 16.4%\n",
      "Progress: 16.8%\n",
      "Progress: 17.1%\n",
      "Progress: 17.4%\n",
      "Progress: 17.7%\n",
      "Progress: 18.0%\n",
      "Progress: 18.3%\n",
      "Progress: 18.6%\n",
      "Progress: 18.9%\n",
      "Progress: 19.2%\n",
      "Progress: 19.5%\n",
      "Progress: 19.8%\n",
      "Progress: 20.1%\n",
      "Progress: 20.4%\n",
      "Progress: 20.7%\n",
      "Progress: 21.0%\n",
      "Progress: 21.3%\n",
      "Progress: 21.6%\n",
      "Progress: 21.9%\n",
      "Progress: 22.2%\n",
      "Progress: 22.5%\n",
      "Progress: 22.8%\n",
      "Progress: 23.1%\n",
      "Progress: 23.5%\n",
      "Progress: 23.8%\n",
      "Progress: 24.1%\n",
      "Progress: 24.4%\n",
      "Progress: 24.7%\n",
      "Progress: 25.0%\n",
      "Progress: 25.3%\n",
      "Progress: 25.6%\n",
      "Progress: 25.9%\n",
      "Progress: 26.2%\n",
      "Progress: 26.5%\n",
      "Progress: 26.8%\n",
      "Progress: 27.1%\n",
      "Progress: 27.4%\n",
      "Progress: 27.7%\n",
      "Progress: 28.0%\n",
      "Progress: 28.3%\n",
      "Progress: 28.6%\n",
      "Progress: 28.9%\n",
      "Progress: 29.2%\n",
      "Progress: 29.5%\n",
      "Progress: 29.8%\n",
      "Progress: 30.2%\n",
      "Progress: 30.5%\n",
      "Progress: 30.8%\n",
      "Progress: 31.1%\n",
      "Progress: 31.4%\n",
      "Progress: 31.7%\n",
      "Progress: 32.0%\n",
      "Progress: 32.3%\n",
      "Progress: 32.6%\n",
      "Progress: 32.9%\n",
      "Progress: 33.2%\n",
      "Progress: 33.5%\n",
      "Progress: 33.8%\n",
      "Progress: 34.1%\n",
      "Progress: 34.4%\n",
      "Progress: 34.7%\n",
      "Progress: 35.0%\n",
      "Progress: 35.3%\n",
      "Progress: 35.6%\n",
      "Progress: 35.9%\n",
      "Progress: 36.2%\n",
      "Progress: 36.6%\n",
      "Progress: 36.9%\n",
      "Progress: 37.2%\n",
      "Progress: 37.5%\n",
      "Progress: 37.8%\n",
      "Progress: 38.1%\n",
      "Progress: 38.4%\n",
      "Progress: 38.7%\n",
      "Progress: 39.0%\n",
      "Progress: 39.3%\n",
      "Progress: 39.6%\n",
      "Progress: 39.9%\n",
      "Progress: 40.2%\n",
      "Progress: 40.5%\n",
      "Progress: 40.8%\n",
      "Progress: 41.1%\n",
      "Progress: 41.4%\n",
      "Progress: 41.7%\n",
      "Progress: 42.0%\n",
      "Progress: 42.3%\n",
      "Progress: 42.6%\n",
      "Progress: 42.9%\n",
      "Progress: 43.3%\n",
      "Progress: 43.6%\n",
      "Progress: 43.9%\n",
      "Progress: 44.2%\n",
      "Progress: 44.5%\n",
      "Progress: 44.8%\n",
      "Progress: 45.1%\n",
      "Progress: 45.4%\n",
      "Progress: 45.7%\n",
      "Progress: 46.0%\n",
      "Progress: 46.3%\n",
      "Progress: 46.6%\n",
      "Progress: 46.9%\n",
      "Progress: 47.2%\n",
      "Progress: 47.5%\n",
      "Progress: 47.8%\n",
      "Progress: 48.1%\n",
      "Progress: 48.4%\n",
      "Progress: 48.7%\n",
      "Progress: 49.0%\n",
      "Progress: 49.3%\n",
      "Progress: 49.6%\n",
      "Progress: 50.0%\n",
      "Progress: 50.3%\n",
      "Progress: 50.6%\n",
      "Progress: 50.9%\n",
      "Progress: 51.2%\n",
      "Progress: 51.5%\n",
      "Progress: 51.8%\n",
      "Progress: 52.1%\n",
      "Progress: 52.4%\n",
      "Progress: 52.7%\n",
      "Progress: 53.0%\n",
      "Progress: 53.3%\n",
      "Progress: 53.6%\n",
      "Progress: 53.9%\n",
      "Progress: 54.2%\n",
      "Progress: 54.5%\n",
      "Progress: 54.8%\n",
      "Progress: 55.1%\n",
      "Progress: 55.4%\n",
      "Progress: 55.7%\n",
      "Progress: 56.0%\n",
      "Progress: 56.3%\n",
      "Progress: 56.7%\n",
      "Progress: 57.0%\n",
      "Progress: 57.3%\n",
      "Progress: 57.6%\n",
      "Progress: 57.9%\n",
      "Progress: 58.2%\n",
      "Progress: 58.5%\n",
      "Progress: 58.8%\n",
      "Progress: 59.1%\n",
      "Progress: 59.4%\n",
      "Progress: 59.7%\n",
      "Progress: 60.0%\n",
      "Progress: 60.3%\n",
      "Progress: 60.6%\n",
      "Progress: 60.9%\n",
      "Progress: 61.2%\n",
      "Progress: 61.5%\n",
      "Progress: 61.8%\n",
      "Progress: 62.1%\n",
      "Progress: 62.4%\n",
      "Progress: 62.7%\n",
      "Progress: 63.0%\n",
      "Progress: 63.4%\n",
      "Progress: 63.7%\n",
      "Progress: 64.0%\n",
      "Progress: 64.3%\n",
      "Progress: 64.6%\n",
      "Progress: 64.9%\n",
      "Progress: 65.2%\n",
      "Progress: 65.5%\n",
      "Progress: 65.8%\n",
      "Progress: 66.1%\n",
      "Progress: 66.4%\n",
      "Progress: 66.7%\n",
      "Progress: 67.0%\n",
      "Progress: 67.3%\n",
      "Progress: 67.6%\n",
      "Progress: 67.9%\n",
      "Progress: 68.2%\n",
      "Progress: 68.5%\n",
      "Progress: 68.8%\n",
      "Progress: 69.1%\n",
      "Progress: 69.4%\n",
      "Progress: 69.8%\n",
      "Progress: 70.1%\n",
      "Progress: 70.4%\n",
      "Progress: 70.7%\n",
      "Progress: 71.0%\n",
      "Progress: 71.3%\n",
      "Progress: 71.6%\n",
      "Progress: 71.9%\n",
      "Progress: 72.2%\n",
      "Progress: 72.5%\n",
      "Progress: 72.8%\n",
      "Progress: 73.1%\n",
      "Progress: 73.4%\n",
      "Progress: 73.7%\n",
      "Progress: 74.0%\n",
      "Progress: 74.3%\n",
      "Progress: 74.6%\n",
      "Progress: 74.9%\n",
      "Progress: 75.2%\n",
      "Progress: 75.5%\n",
      "Progress: 75.8%\n",
      "Progress: 76.1%\n",
      "Progress: 76.5%\n",
      "Progress: 76.8%\n",
      "Progress: 77.1%\n",
      "Progress: 77.4%\n",
      "Progress: 77.7%\n",
      "Progress: 78.0%\n",
      "Progress: 78.3%\n",
      "Progress: 78.6%\n",
      "Progress: 78.9%\n",
      "Progress: 79.2%\n",
      "Progress: 79.5%\n",
      "Progress: 79.8%\n",
      "Progress: 80.1%\n",
      "Progress: 80.4%\n",
      "Progress: 80.7%\n",
      "Progress: 81.0%\n",
      "Progress: 81.3%\n",
      "Progress: 81.6%\n",
      "Progress: 81.9%\n",
      "Progress: 82.2%\n",
      "Progress: 82.5%\n",
      "Progress: 82.8%\n",
      "Progress: 83.2%\n",
      "Progress: 83.5%\n",
      "Progress: 83.8%\n",
      "Progress: 84.1%\n",
      "Progress: 84.4%\n",
      "Progress: 84.7%\n",
      "Progress: 85.0%\n",
      "Progress: 85.3%\n",
      "Progress: 85.6%\n",
      "Progress: 85.9%\n",
      "Progress: 86.2%\n",
      "Progress: 86.5%\n",
      "Progress: 86.8%\n",
      "Progress: 87.1%\n",
      "Progress: 87.4%\n",
      "Progress: 87.7%\n",
      "Progress: 88.0%\n",
      "Progress: 88.3%\n",
      "Progress: 88.6%\n",
      "Progress: 88.9%\n",
      "Progress: 89.2%\n",
      "Progress: 89.5%\n",
      "Progress: 89.9%\n",
      "Progress: 90.2%\n",
      "Progress: 90.5%\n",
      "Progress: 90.8%\n",
      "Progress: 91.1%\n",
      "Progress: 91.4%\n",
      "Progress: 91.7%\n",
      "Progress: 92.0%\n",
      "Progress: 92.3%\n",
      "Progress: 92.6%\n",
      "Progress: 92.9%\n",
      "Progress: 93.2%\n",
      "Progress: 93.5%\n",
      "Progress: 93.8%\n",
      "Progress: 94.1%\n",
      "Progress: 94.4%\n",
      "Progress: 94.7%\n",
      "Progress: 95.0%\n",
      "Progress: 95.3%\n",
      "Progress: 95.6%\n",
      "Progress: 95.9%\n",
      "Progress: 96.2%\n",
      "Progress: 96.6%\n",
      "Progress: 96.9%\n",
      "Progress: 97.2%\n",
      "Progress: 97.5%\n",
      "Progress: 97.8%\n",
      "Progress: 98.1%\n",
      "Progress: 98.4%\n",
      "Progress: 98.7%\n",
      "Progress: 99.0%\n",
      "Progress: 99.3%\n",
      "Progress: 99.6%\n",
      "Progress: 99.9%\n",
      "Processing gas data...\n",
      "Progress: 0.0%\n",
      "Progress: 0.3%\n",
      "Progress: 0.6%\n",
      "Progress: 0.9%\n",
      "Progress: 1.2%\n",
      "Progress: 1.5%\n",
      "Progress: 1.8%\n",
      "Progress: 2.1%\n",
      "Progress: 2.4%\n",
      "Progress: 2.7%\n",
      "Progress: 3.0%\n",
      "Progress: 3.4%\n",
      "Progress: 3.7%\n",
      "Progress: 4.0%\n",
      "Progress: 4.3%\n",
      "Progress: 4.6%\n",
      "Progress: 4.9%\n",
      "Progress: 5.2%\n",
      "Progress: 5.5%\n",
      "Progress: 5.8%\n",
      "Progress: 6.1%\n",
      "Progress: 6.4%\n",
      "Progress: 6.7%\n",
      "Progress: 7.0%\n",
      "Progress: 7.3%\n",
      "Progress: 7.6%\n",
      "Progress: 7.9%\n",
      "Progress: 8.2%\n",
      "Progress: 8.5%\n",
      "Progress: 8.8%\n",
      "Progress: 9.1%\n",
      "Progress: 9.4%\n",
      "Progress: 9.7%\n",
      "Progress: 10.1%\n",
      "Progress: 10.4%\n",
      "Progress: 10.7%\n",
      "Progress: 11.0%\n",
      "Progress: 11.3%\n",
      "Progress: 11.6%\n",
      "Progress: 11.9%\n",
      "Progress: 12.2%\n",
      "Progress: 12.5%\n",
      "Progress: 12.8%\n",
      "Progress: 13.1%\n",
      "Progress: 13.4%\n",
      "Progress: 13.7%\n",
      "Progress: 14.0%\n",
      "Progress: 14.3%\n",
      "Progress: 14.6%\n",
      "Progress: 14.9%\n",
      "Progress: 15.2%\n",
      "Progress: 15.5%\n",
      "Progress: 15.8%\n",
      "Progress: 16.1%\n",
      "Progress: 16.4%\n",
      "Progress: 16.8%\n",
      "Progress: 17.1%\n",
      "Progress: 17.4%\n",
      "Progress: 17.7%\n",
      "Progress: 18.0%\n",
      "Progress: 18.3%\n",
      "Progress: 18.6%\n",
      "Progress: 18.9%\n",
      "Progress: 19.2%\n",
      "Progress: 19.5%\n",
      "Progress: 19.8%\n",
      "Progress: 20.1%\n",
      "Progress: 20.4%\n",
      "Progress: 20.7%\n",
      "Progress: 21.0%\n",
      "Progress: 21.3%\n",
      "Progress: 21.6%\n",
      "Progress: 21.9%\n",
      "Progress: 22.2%\n",
      "Progress: 22.5%\n",
      "Progress: 22.8%\n",
      "Progress: 23.1%\n",
      "Progress: 23.5%\n",
      "Progress: 23.8%\n",
      "Progress: 24.1%\n",
      "Progress: 24.4%\n",
      "Progress: 24.7%\n",
      "Progress: 25.0%\n",
      "Progress: 25.3%\n",
      "Progress: 25.6%\n",
      "Progress: 25.9%\n",
      "Progress: 26.2%\n",
      "Progress: 26.5%\n",
      "Progress: 26.8%\n",
      "Progress: 27.1%\n",
      "Progress: 27.4%\n",
      "Progress: 27.7%\n",
      "Progress: 28.0%\n",
      "Progress: 28.3%\n",
      "Progress: 28.6%\n",
      "Progress: 28.9%\n",
      "Progress: 29.2%\n",
      "Progress: 29.5%\n",
      "Progress: 29.8%\n",
      "Progress: 30.2%\n",
      "Progress: 30.5%\n",
      "Progress: 30.8%\n",
      "Progress: 31.1%\n",
      "Progress: 31.4%\n",
      "Progress: 31.7%\n",
      "Progress: 32.0%\n",
      "Progress: 32.3%\n",
      "Progress: 32.6%\n",
      "Progress: 32.9%\n",
      "Progress: 33.2%\n",
      "Progress: 33.5%\n",
      "Progress: 33.8%\n",
      "Progress: 34.1%\n",
      "Progress: 34.4%\n",
      "Progress: 34.7%\n",
      "Progress: 35.0%\n",
      "Progress: 35.3%\n",
      "Progress: 35.6%\n",
      "Progress: 35.9%\n",
      "Progress: 36.2%\n",
      "Progress: 36.6%\n",
      "Progress: 36.9%\n",
      "Progress: 37.2%\n",
      "Progress: 37.5%\n",
      "Progress: 37.8%\n",
      "Progress: 38.1%\n",
      "Progress: 38.4%\n",
      "Progress: 38.7%\n",
      "Progress: 39.0%\n",
      "Progress: 39.3%\n",
      "Progress: 39.6%\n",
      "Progress: 39.9%\n",
      "Progress: 40.2%\n",
      "Progress: 40.5%\n",
      "Progress: 40.8%\n",
      "Progress: 41.1%\n",
      "Progress: 41.4%\n",
      "Progress: 41.7%\n",
      "Progress: 42.0%\n",
      "Progress: 42.3%\n",
      "Progress: 42.6%\n",
      "Progress: 42.9%\n",
      "Progress: 43.3%\n",
      "Progress: 43.6%\n",
      "Progress: 43.9%\n",
      "Progress: 44.2%\n",
      "Progress: 44.5%\n",
      "Progress: 44.8%\n",
      "Progress: 45.1%\n",
      "Progress: 45.4%\n",
      "Progress: 45.7%\n",
      "Progress: 46.0%\n",
      "Progress: 46.3%\n",
      "Progress: 46.6%\n",
      "Progress: 46.9%\n",
      "Progress: 47.2%\n",
      "Progress: 47.5%\n",
      "Progress: 47.8%\n",
      "Progress: 48.1%\n",
      "Progress: 48.4%\n",
      "Progress: 48.7%\n",
      "Progress: 49.0%\n",
      "Progress: 49.3%\n",
      "Progress: 49.6%\n",
      "Progress: 50.0%\n",
      "Progress: 50.3%\n",
      "Progress: 50.6%\n",
      "Progress: 50.9%\n",
      "Progress: 51.2%\n",
      "Progress: 51.5%\n",
      "Progress: 51.8%\n",
      "Progress: 52.1%\n",
      "Progress: 52.4%\n",
      "Progress: 52.7%\n",
      "Progress: 53.0%\n",
      "Progress: 53.3%\n",
      "Progress: 53.6%\n",
      "Progress: 53.9%\n",
      "Progress: 54.2%\n",
      "Progress: 54.5%\n",
      "Progress: 54.8%\n",
      "Progress: 55.1%\n",
      "Progress: 55.4%\n",
      "Progress: 55.7%\n",
      "Progress: 56.0%\n",
      "Progress: 56.3%\n",
      "Progress: 56.7%\n",
      "Progress: 57.0%\n",
      "Progress: 57.3%\n",
      "Progress: 57.6%\n",
      "Progress: 57.9%\n",
      "Progress: 58.2%\n",
      "Progress: 58.5%\n",
      "Progress: 58.8%\n",
      "Progress: 59.1%\n",
      "Progress: 59.4%\n",
      "Progress: 59.7%\n",
      "Progress: 60.0%\n",
      "Progress: 60.3%\n",
      "Progress: 60.6%\n",
      "Progress: 60.9%\n",
      "Progress: 61.2%\n",
      "Progress: 61.5%\n",
      "Progress: 61.8%\n",
      "Progress: 62.1%\n",
      "Progress: 62.4%\n",
      "Progress: 62.7%\n",
      "Progress: 63.0%\n",
      "Progress: 63.4%\n",
      "Progress: 63.7%\n",
      "Progress: 64.0%\n",
      "Progress: 64.3%\n",
      "Progress: 64.6%\n",
      "Progress: 64.9%\n",
      "Progress: 65.2%\n",
      "Progress: 65.5%\n",
      "Progress: 65.8%\n",
      "Progress: 66.1%\n",
      "Progress: 66.4%\n",
      "Progress: 66.7%\n",
      "Progress: 67.0%\n",
      "Progress: 67.3%\n",
      "Progress: 67.6%\n",
      "Progress: 67.9%\n",
      "Progress: 68.2%\n",
      "Progress: 68.5%\n",
      "Progress: 68.8%\n",
      "Progress: 69.1%\n",
      "Progress: 69.4%\n",
      "Progress: 69.8%\n",
      "Progress: 70.1%\n",
      "Progress: 70.4%\n",
      "Progress: 70.7%\n",
      "Progress: 71.0%\n",
      "Progress: 71.3%\n",
      "Progress: 71.6%\n",
      "Progress: 71.9%\n",
      "Progress: 72.2%\n",
      "Progress: 72.5%\n",
      "Progress: 72.8%\n",
      "Progress: 73.1%\n",
      "Progress: 73.4%\n",
      "Progress: 73.7%\n",
      "Progress: 74.0%\n",
      "Progress: 74.3%\n",
      "Progress: 74.6%\n",
      "Progress: 74.9%\n",
      "Progress: 75.2%\n",
      "Progress: 75.5%\n",
      "Progress: 75.8%\n",
      "Progress: 76.1%\n",
      "Progress: 76.5%\n",
      "Progress: 76.8%\n",
      "Progress: 77.1%\n",
      "Progress: 77.4%\n",
      "Progress: 77.7%\n",
      "Progress: 78.0%\n",
      "Progress: 78.3%\n",
      "Progress: 78.6%\n",
      "Progress: 78.9%\n",
      "Progress: 79.2%\n",
      "Progress: 79.5%\n",
      "Progress: 79.8%\n",
      "Progress: 80.1%\n",
      "Progress: 80.4%\n",
      "Progress: 80.7%\n",
      "Progress: 81.0%\n",
      "Progress: 81.3%\n",
      "Progress: 81.6%\n",
      "Progress: 81.9%\n",
      "Progress: 82.2%\n",
      "Progress: 82.5%\n",
      "Progress: 82.8%\n",
      "Progress: 83.2%\n",
      "Progress: 83.5%\n",
      "Progress: 83.8%\n",
      "Progress: 84.1%\n",
      "Progress: 84.4%\n",
      "Progress: 84.7%\n",
      "Progress: 85.0%\n",
      "Progress: 85.3%\n",
      "Progress: 85.6%\n",
      "Progress: 85.9%\n",
      "Progress: 86.2%\n",
      "Progress: 86.5%\n",
      "Progress: 86.8%\n",
      "Progress: 87.1%\n",
      "Progress: 87.4%\n",
      "Progress: 87.7%\n",
      "Progress: 88.0%\n",
      "Progress: 88.3%\n",
      "Progress: 88.6%\n",
      "Progress: 88.9%\n",
      "Progress: 89.2%\n",
      "Progress: 89.5%\n",
      "Progress: 89.9%\n",
      "Progress: 90.2%\n",
      "Progress: 90.5%\n",
      "Progress: 90.8%\n",
      "Progress: 91.1%\n",
      "Progress: 91.4%\n",
      "Progress: 91.7%\n",
      "Progress: 92.0%\n",
      "Progress: 92.3%\n",
      "Progress: 92.6%\n",
      "Progress: 92.9%\n",
      "Progress: 93.2%\n",
      "Progress: 93.5%\n",
      "Progress: 93.8%\n",
      "Progress: 94.1%\n",
      "Progress: 94.4%\n",
      "Progress: 94.7%\n",
      "Progress: 95.0%\n",
      "Progress: 95.3%\n",
      "Progress: 95.6%\n",
      "Progress: 95.9%\n",
      "Progress: 96.2%\n",
      "Progress: 96.6%\n",
      "Progress: 96.9%\n",
      "Progress: 97.2%\n",
      "Progress: 97.5%\n",
      "Progress: 97.8%\n",
      "Progress: 98.1%\n",
      "Progress: 98.4%\n",
      "Progress: 98.7%\n",
      "Progress: 99.0%\n",
      "Progress: 99.3%\n",
      "Progress: 99.6%\n",
      "Progress: 99.9%\n",
      "🏠 Haushalte im Training (Gas): 254\n",
      "🏠 Haushalte im Test (Gas): 247\n",
      "📉 Verlust an Haushalten: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X_train_electric': (1313250, 90, 24),\n",
       " 'X_test_electric': (328313, 90, 24),\n",
       " 'X_train_gas': (1313250, 90, 24),\n",
       " 'X_test_gas': (328313, 90, 24)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters and columns\n",
    "time_steps = 90\n",
    "\n",
    "# Features and target definition (Strom und Gas)\n",
    "feature_columns = [col for col in df_clean.columns if col not in ['electric_total_consumption_kWh', 'gas_total_consumption_kWh', 'homeid', 'roomid']]\n",
    "target_column_electric = 'electric_total_consumption_kWh'\n",
    "target_column_gas = 'gas_total_consumption_kWh'\n",
    "\n",
    "def create_memmap_array(shape, filename, dtype='float32'):\n",
    "    \"\"\"Create memory-mapped array\"\"\"\n",
    "    path = Path('temp_arrays')\n",
    "    path.mkdir(exist_ok=True)\n",
    "    return np.memmap(path / filename, dtype=dtype, mode='w+', shape=shape)\n",
    "\n",
    "def process_data_efficiently(df_clean, target_column, feature_columns, time_steps, prefix):\n",
    "    \"\"\"Process data with disk-based storage\"\"\"\n",
    "    total_sequences = len(df_clean) - time_steps\n",
    "    n_features = len(feature_columns)\n",
    "    \n",
    "    # Create memory-mapped arrays\n",
    "    X = create_memmap_array((total_sequences, time_steps, n_features), f'{prefix}_X.mmap')\n",
    "    y = create_memmap_array((total_sequences,), f'{prefix}_y.mmap')\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    chunk_size = 500\n",
    "    feature_data = df_clean[feature_columns].values\n",
    "    target_data = df_clean[target_column].values\n",
    "    \n",
    "    print(f\"Processing {prefix} data...\")\n",
    "    for chunk_start in range(0, total_sequences, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_sequences)\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            X[i] = feature_data[i:i + time_steps]\n",
    "            y[i] = target_data[i + time_steps]\n",
    "            \n",
    "        if chunk_start % (chunk_size * 10) == 0:\n",
    "            print(f\"Progress: {chunk_start/total_sequences*100:.1f}%\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Process both electric and gas data\n",
    "time_steps = 90\n",
    "feature_columns = [col for col in df_clean.columns if col not in \n",
    "                  ['electric_total_consumption_kWh', 'gas_total_consumption_kWh', 'homeid', 'roomid']]\n",
    "\n",
    "# Process electric data\n",
    "X_electric, y_electric = process_data_efficiently(\n",
    "    df_clean, \n",
    "    'electric_total_consumption_kWh',\n",
    "    feature_columns,\n",
    "    time_steps,\n",
    "    'electric'\n",
    ")\n",
    "\n",
    "# Process gas data\n",
    "X_gas, y_gas = process_data_efficiently(\n",
    "    df_clean,\n",
    "    'gas_total_consumption_kWh',\n",
    "    feature_columns,\n",
    "    time_steps,\n",
    "    'gas'\n",
    ")\n",
    "\n",
    "# Split datasets\n",
    "X_train_electric, X_test_electric, y_train_electric, y_test_electric = train_test_split(\n",
    "    X_electric, y_electric, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "X_train_gas, X_test_gas, y_train_gas, y_test_gas = train_test_split(\n",
    "    X_gas, y_gas, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Prüfen, wie viele Haushalte im Trainings- und Testset enthalten sind\n",
    "train_households_gas = df_clean.loc[df_clean.index[-len(y_train_gas):], 'homeid'].nunique()\n",
    "test_households_gas = df_clean.loc[df_clean.index[-len(y_test_gas):], 'homeid'].nunique()\n",
    "\n",
    "print(f\" Haushalte im Training (Gas): {train_households_gas}\")\n",
    "print(f\" Haushalte im Test (Gas): {test_households_gas}\")\n",
    "print(f\" Verlust an Haushalten: {254 - test_households_gas}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show shapes of the data\n",
    "train_test_summary = {\n",
    "    'X_train_electric': X_train_electric.shape,\n",
    "    'X_test_electric': X_test_electric.shape,\n",
    "    'X_train_gas': X_train_gas.shape,\n",
    "    'X_test_gas': X_test_gas.shape,\n",
    "}\n",
    "train_test_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E5KwcVmgqyx"
   },
   "source": [
    "**4. LSTM Modell erstellen**\n",
    "1. Daten in das LSTM Format bringen (X_train, y_train)\n",
    "2. LSTM schichten definieren (Tensorflow)\n",
    "3. Modell kompilieren und trainieren\n",
    "4. Hyperparameter-Tuning (z.B Anzahl Neuronen, Learning Rate,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Add dropout parameter to LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0  # Only apply dropout with multiple layers\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# Check and configure MPS device for M2 Mac\n",
    "has_mps = torch.backends.mps.is_available()\n",
    "if has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not found, using CPU\")\n",
    "\n",
    "# Improved data preparation with explicit float32\n",
    "def prepare_data_for_training(X, y, batch_size=64):\n",
    "    \"\"\"Convert numpy arrays to PyTorch DataLoader with float32 precision\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X).to(torch.float32)\n",
    "    y_tensor = torch.FloatTensor(y).reshape(-1, 1).to(torch.float32)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Early stopping handler\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Improved training function with early stopping\n",
    "def train_model(model, train_loader, model_name, num_epochs=10):\n",
    "    \"\"\"Train model with early stopping and best model saving\"\"\"\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    early_stopping = EarlyStopping(patience=2)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=f'Training {model_name}', position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        batch_pbar = tqdm(train_loader, \n",
    "                         desc=f'Epoch {epoch+1}/{num_epochs}',\n",
    "                         leave=False, \n",
    "                         position=1)\n",
    "        \n",
    "        for inputs, targets in batch_pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_pbar.set_postfix({'loss': f'{epoch_loss:.4f}'})\n",
    "\n",
    "        # Save best model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, os.path.join(checkpoint_dir, f'{model_name}_best.pt'))\n",
    "\n",
    "        # Early stopping check\n",
    "        early_stopping(epoch_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n{model_name} training complete! Best loss: {best_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Helper function to load trained models\n",
    "def load_trained_model(model, model_name):\n",
    "    \"\"\"Load best model from checkpoint\"\"\"\n",
    "    checkpoint_path = os.path.join('checkpoints', f'{model_name}_best.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Training pipeline\n",
    "print(\"Preparing data loaders...\")\n",
    "train_loader_electric = prepare_data_for_training(X_train_electric, y_train_electric)\n",
    "train_loader_gas = prepare_data_for_training(X_train_gas, y_train_gas)\n",
    "\n",
    "# Initialize and train models\n",
    "print(\"\\nTraining Electric Model...\")\n",
    "model_electric = LSTMModel(input_size=len(feature_columns), \n",
    "                          hidden_size=32, \n",
    "                          num_layers=2, \n",
    "                          output_size=1).to(device)\n",
    "model_electric = train_model(model_electric, train_loader_electric, \"electric\")\n",
    "\n",
    "print(\"\\nTraining Gas Model...\")\n",
    "model_gas = LSTMModel(input_size=len(feature_columns), \n",
    "                      hidden_size=32, \n",
    "                      num_layers=2, \n",
    "                      output_size=1).to(device)\n",
    "model_gas = train_model(model_gas, train_loader_gas, \"gas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_78qhv1gqyx"
   },
   "source": [
    "**5. Modell evaluieren & Vorhersagen interpretieren**\n",
    "1. Vorhersagen auf Testdaten durchführen\n",
    "2. Metriken berechnen (RMSE, MAE, R^2)\n",
    "3. XAI mit SHAP oder LIME anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from joblib import load\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Device setup with error handling\n",
    "def get_device():\n",
    "    \"\"\"Set up device with proper error handling\"\"\"\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            print(\"MPS not available, falling back to CPU\")\n",
    "            return torch.device(\"cpu\")\n",
    "    except:\n",
    "        print(\"Error checking MPS, falling back to CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def calculate_metrics(predictions, actuals, model_name=None):\n",
    "    \"\"\"\n",
    "    Calculate metrics with improved MAPE/SMAPE handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Basic metrics\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        # Enhanced MAPE calculation with protection against small values\n",
    "        mask = np.abs(actuals) > np.percentile(np.abs(actuals), 5)  # Werte unter dem 5. Perzentil filtern\n",
    "        if np.any(mask):\n",
    "            mape = np.mean(np.abs((actuals[mask] - predictions[mask]) / \n",
    "                                 np.maximum(actuals[mask], 1e-10))) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "            print(f\"⚠️ Warning: No valid values for MAPE calculation in {model_name}\")\n",
    "        \n",
    "        # SMAPE calculation (more robust against zero/small values)\n",
    "        denominator = (np.abs(actuals) + np.abs(predictions))\n",
    "        mask_smape = denominator >= 1e-10\n",
    "        if np.any(mask_smape):\n",
    "            smape = 100 * np.mean(2 * np.abs(predictions[mask_smape] - actuals[mask_smape]) / \n",
    "                                denominator[mask_smape])\n",
    "        else:\n",
    "            smape = np.nan\n",
    "        \n",
    "        # Baseline comparison\n",
    "        baseline_mse = mean_squared_error(actuals, np.full_like(actuals, np.mean(actuals)))\n",
    "        if mse > baseline_mse and model_name:\n",
    "            print(f\"⚠️ Warning: {model_name} MSE ({mse:.2f}) is higher than baseline MSE ({baseline_mse:.2f})\")\n",
    "        \n",
    "        metrics = {\n",
    "            'RMSE': float(f\"{rmse:.4f}\"),\n",
    "            'MSE': float(f\"{mse:.4f}\"),\n",
    "            'MAE': float(f\"{mae:.4f}\"),\n",
    "            'R2': float(f\"{r2:.4f}\"),\n",
    "            'MAPE': float(f\"{mape:.4f}\") if not np.isnan(mape) else np.nan,\n",
    "            'SMAPE': float(f\"{smape:.4f}\") if not np.isnan(smape) else np.nan\n",
    "        }\n",
    "        \n",
    "        # Print additional insights\n",
    "        if model_name:\n",
    "            print(f\"\\nDetailed metrics for {model_name}:\")\n",
    "            print(f\"- MAPE: {metrics['MAPE']:.2f}% (traditional)\")\n",
    "            print(f\"- SMAPE: {metrics['SMAPE']:.2f}% (symmetric)\")\n",
    "            print(f\"- Values near zero: {(~mask).sum()} of {len(actuals)}\")\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error calculating metrics: {e}\")\n",
    "        return None\n",
    "# Improved inverse scaling with error handling\n",
    "def inverse_scale_predictions(predictions, actuals, scaler):\n",
    "    \"\"\"Inverse transform predictions and actuals with error checking\"\"\"\n",
    "    try:\n",
    "        pred_orig = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "        act_orig = scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "        return pred_orig, act_orig\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inverse scaling: {e}\")\n",
    "        return predictions.flatten(), actuals.flatten()\n",
    "\n",
    "# Enhanced model evaluation with additional visualizations\n",
    "def evaluate_model(model, model_name, test_loader, scaler):\n",
    "    \"\"\"Evaluate model with enhanced metrics and visualizations\"\"\"\n",
    "    print(f\"\\nEvaluating {model_name} model...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    # Prediction loop with error handling\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            for inputs, targets in tqdm(test_loader, desc=\"Making predictions\"):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "                actuals.append(targets.cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    # Process results\n",
    "    predictions = np.concatenate(predictions)\n",
    "    actuals = np.concatenate(actuals)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    predictions, actuals = inverse_scale_predictions(predictions, actuals, scaler)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(predictions, actuals)\n",
    "    \n",
    "    # Create formatted metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': list(metrics.keys()),\n",
    "        'Value': [f\"{v:.4f}\" for v in metrics.values()]\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMetrics for {model_name} Energy Consumption:\")\n",
    "    print(tabulate(metrics_df, headers='keys', tablefmt='pipe', showindex=False))\n",
    "    \n",
    "    return predictions, actuals, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_per_household(predictions, actuals, household_ids, timestamps, \n",
    "                                 model_name, window_size=24, max_houses=5, \n",
    "                                 show_plots=False, save_plots=True):\n",
    "    \"\"\"\n",
    "    Plot predictions vs actual values for individual households with enhanced visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : array-like\n",
    "        Model predictions\n",
    "    actuals : array-like\n",
    "        Actual values\n",
    "    household_ids : array-like\n",
    "        Array of household IDs\n",
    "    timestamps : array-like\n",
    "        Array of timestamps\n",
    "    model_name : str\n",
    "        Name of the model for plot titles\n",
    "    window_size : int, default=24\n",
    "        Window size for moving average smoothing\n",
    "    max_houses : int, default=5\n",
    "        Maximum number of houses to plot\n",
    "    show_plots : bool, default=False\n",
    "        Whether to display plots in notebook\n",
    "    save_plots : bool, default=True\n",
    "        Whether to save plots to disk\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not all(len(arr) > 0 for arr in [predictions, actuals, household_ids, timestamps]):\n",
    "            raise ValueError(\"Empty input arrays detected\")\n",
    "            \n",
    "        # Convert inputs to numpy arrays\n",
    "        min_len = min(len(predictions), len(actuals), len(household_ids), len(timestamps))\n",
    "        predictions = np.array(predictions[:min_len])\n",
    "        actuals = np.array(actuals[:min_len])\n",
    "        household_ids = np.array(household_ids[:min_len])\n",
    "        timestamps = np.array(timestamps[:min_len])\n",
    "        \n",
    "        # Get unique households\n",
    "        unique_homes = np.unique(household_ids)\n",
    "        print(f\" Found {len(unique_homes)} unique households\")\n",
    "        \n",
    "        if save_plots:\n",
    "            plot_dir = Path('../reports/figures')\n",
    "            plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Set style and color scheme\n",
    "        plt.style.use('ggplot')\n",
    "        colors = {\n",
    "            'actual': '#2ecc71',\n",
    "            'predicted': '#e74c3c',\n",
    "            'error': '#3498db',\n",
    "            'baseline': '#95a5a6'\n",
    "        }\n",
    "        \n",
    "        # Track skipped households\n",
    "        skipped_homes = []\n",
    "        \n",
    "        for home in unique_homes[:max_houses]:\n",
    "            indices = np.where(household_ids == home)[0]\n",
    "            \n",
    "            if len(indices) < window_size:\n",
    "                skipped_homes.append(home)\n",
    "                continue\n",
    "                \n",
    "            # Create figure with enhanced layout\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), \n",
    "                                         height_ratios=[3, 1], \n",
    "                                         gridspec_kw={'hspace': 0.3})\n",
    "            \n",
    "            # Data preparation\n",
    "            time_series = pd.to_datetime(timestamps[indices])\n",
    "            actual_series = pd.Series(actuals[indices], index=time_series)\n",
    "            pred_series = pd.Series(predictions[indices], index=time_series)\n",
    "            \n",
    "            # Smoothing with error handling\n",
    "            actual_smooth = actual_series.rolling(window=window_size, \n",
    "                                               center=True, \n",
    "                                               min_periods=1).mean()\n",
    "            pred_smooth = pred_series.rolling(window=window_size, \n",
    "                                           center=True, \n",
    "                                           min_periods=1).mean()\n",
    "            \n",
    "            # Main consumption plot\n",
    "            ax1.plot(actual_smooth, label=\"Actual\", color=colors['actual'], linewidth=2)\n",
    "            ax1.plot(pred_smooth, label=\"Predicted\", color=colors['predicted'], linewidth=2)\n",
    "            ax1.set_title(f\"{model_name} Consumption Predictions - Household {home}\", \n",
    "                         fontsize=14, pad=20)\n",
    "            ax1.set_ylabel(\"Consumption (kWh)\", fontsize=12)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.legend(fontsize=10, loc='upper right')\n",
    "            \n",
    "            # Enhanced error plot\n",
    "            error = pred_series - actual_series\n",
    "            ax2.plot(time_series, error, color=colors['error'], \n",
    "                    alpha=0.6, label='Prediction Error')\n",
    "            ax2.axhline(y=0, color=colors['baseline'], \n",
    "                       linestyle='--', alpha=0.5)\n",
    "            \n",
    "            # Dynamic y-axis limits with padding\n",
    "            error_margin = (error.max() - error.min()) * 0.1\n",
    "            ax2.set_ylim(error.min() - error_margin, \n",
    "                        error.max() + error_margin)\n",
    "            \n",
    "            ax2.set_xlabel(\"Time\", fontsize=12)\n",
    "            ax2.set_ylabel(\"Error (kWh)\", fontsize=12)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.legend(fontsize=10)\n",
    "            \n",
    "            # Consistent x-axis formatting\n",
    "            for ax in [ax1, ax2]:\n",
    "                ax.tick_params(axis='both', labelsize=10)\n",
    "                plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_plots:\n",
    "                save_path = plot_dir / f'{model_name.lower()}_household_{home}_predictions.png'\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Saved plot for household {home}\")\n",
    "            \n",
    "            if show_plots:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "        \n",
    "        if skipped_homes:\n",
    "            print(f\"\\n Skipped {len(skipped_homes)} households due to insufficient data:\")\n",
    "            print(f\"   {skipped_homes}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Error during plotting: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def prepare_evaluation_data():\n",
    "    \"\"\"Prepare data and models for evaluation\"\"\"\n",
    "    try:\n",
    "        # Load scalers\n",
    "        scalers = {\n",
    "            'Electric': load('scalers/scaler_electric.pkl'),\n",
    "            'Gas': load('scalers/scaler_gas.pkl')\n",
    "        }\n",
    "        \n",
    "        # Initialize models\n",
    "        models = {\n",
    "            'Electric': LSTMModel(input_size=len(feature_columns), \n",
    "                                hidden_size=32, \n",
    "                                num_layers=2, \n",
    "                                output_size=1).to(device),\n",
    "            'Gas': LSTMModel(input_size=len(feature_columns), \n",
    "                           hidden_size=32, \n",
    "                           num_layers=2, \n",
    "                           output_size=1).to(device)\n",
    "        }\n",
    "        \n",
    "        # Prepare test dataloaders\n",
    "        test_data = {\n",
    "            'Electric': (X_test_electric, y_test_electric),\n",
    "            'Gas': (X_test_gas, y_test_gas)\n",
    "        }\n",
    "        \n",
    "        test_loaders = {\n",
    "            name: torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(\n",
    "                    torch.FloatTensor(X_test),\n",
    "                    torch.FloatTensor(y_test)\n",
    "                ),\n",
    "                batch_size=16,\n",
    "                shuffle=False\n",
    "            ) for name, (X_test, y_test) in test_data.items()\n",
    "        }\n",
    "        \n",
    "        # Extract household IDs and timestamps\n",
    "        metadata = {\n",
    "            name: {\n",
    "                'household_ids': df_clean.loc[df_clean.index[-len(y_test):], 'homeid'].values[:len(y_test)],\n",
    "                'timestamps': df_clean.index[-len(y_test):].values[:len(y_test)]\n",
    "            } for name, (_, y_test) in test_data.items()\n",
    "        }\n",
    "        \n",
    "        return models, scalers, test_loaders, metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error preparing evaluation data: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_model_evaluation():\n",
    "    \"\"\"Run evaluation for both LSTM models with enhanced error handling\"\"\"\n",
    "    print(\" Preparing evaluation data...\")\n",
    "    evaluation_data = prepare_evaluation_data()\n",
    "    if evaluation_data is None:\n",
    "        return\n",
    "    \n",
    "    models, scalers, test_loaders, metadata = evaluation_data\n",
    "    results = {}\n",
    "    \n",
    "    # Create reports directory\n",
    "    reports_dir = Path('../reports/tables')\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n Evaluating {name} model...\")\n",
    "        try:\n",
    "            model = load_trained_model(model, name.lower())\n",
    "            predictions, actuals, raw_metrics = evaluate_model(\n",
    "                model=model,\n",
    "                model_name=name,\n",
    "                test_loader=test_loaders[name],\n",
    "                scaler=scalers[name]\n",
    "            )\n",
    "            \n",
    "            if predictions is not None:\n",
    "                # Calculate metrics with enhanced error handling\n",
    "                metrics = calculate_metrics(predictions, actuals, name)\n",
    "                if metrics is None:\n",
    "                    continue\n",
    "                    \n",
    "                results[name] = {\n",
    "                    'predictions': predictions,\n",
    "                    'actuals': actuals,\n",
    "                    'metrics': metrics\n",
    "                }\n",
    "                \n",
    "                # Plot predictions if data is valid\n",
    "                if len(predictions) > 0:\n",
    "                    plot_predictions_per_household(\n",
    "                        predictions=predictions,\n",
    "                        actuals=actuals,\n",
    "                        household_ids=metadata[name]['household_ids'],\n",
    "                        timestamps=metadata[name]['timestamps'],\n",
    "                        model_name=name,\n",
    "                        show_plots=True\n",
    "                    )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" Error evaluating {name} model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save and display results with proper DataFrame construction\n",
    "    if results:\n",
    "        try:\n",
    "            # Convert metrics to proper DataFrame format\n",
    "            final_metrics = pd.DataFrame.from_records(\n",
    "                [results[name]['metrics'] for name in results],\n",
    "                index=results.keys()\n",
    "            )\n",
    "            \n",
    "            # Save metrics\n",
    "            final_metrics.to_csv(reports_dir / 'model_metrics.csv')\n",
    "            \n",
    "            # Display results\n",
    "            print(\"\\n Final Model Comparison:\")\n",
    "            print(tabulate(final_metrics, headers='keys', tablefmt='pipe', showindex=True))\n",
    "            \n",
    "            # Additional analysis for poor performance\n",
    "            for name in results:\n",
    "                metrics = results[name]['metrics']\n",
    "                if metrics['R2'] < 0:\n",
    "                    print(f\"\\n Poor performance detected for {name} model:\")\n",
    "                    print(f\"   R² score: {metrics['R2']:.4f}\")\n",
    "                    print(\"   Consider model retraining or feature engineering\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Error saving/displaying results: {e}\")\n",
    "    else:\n",
    "        print(\"\\n No results to display\")\n",
    "\n",
    "# Run evaluation\n",
    "if __name__ == \"__main__\":\n",
    "   results = run_model_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensure results dictionary is initialized\n",
    "if not 'results' in locals() or results is None:\n",
    "    results = {}\n",
    "\n",
    "# Make predictions for Electric model\n",
    "model_electric = models['Electric']\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_test_electric),\n",
    "        torch.FloatTensor(y_test_electric)\n",
    "    ),\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "model_electric.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_electric(inputs)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        actuals.append(targets.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "actuals = np.concatenate(actuals)\n",
    "\n",
    "# Store results\n",
    "results['Electric'] = {\n",
    "    'predictions': scaler_electric.inverse_transform(predictions.reshape(-1, 1)).flatten(),\n",
    "    'actuals': scaler_electric.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "}\n",
    "\n",
    "# Get test period data\n",
    "test_length = len(results['Electric']['predictions'])\n",
    "test_timestamps = df_clean.index[-test_length:]\n",
    "test_household_ids = df_clean['homeid'].values[-test_length:]\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions_per_household(\n",
    "    predictions=results['Electric']['predictions'],\n",
    "    actuals=results['Electric']['actuals'],\n",
    "    household_ids=test_household_ids,\n",
    "    timestamps=test_timestamps,\n",
    "    model_name=\"Electric\",\n",
    "    window_size=24,\n",
    "    max_houses=3,\n",
    "    show_plots=True,\n",
    "    save_plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from joblib import load\n",
    "# from tqdm.auto import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pathlib import Path\n",
    "# from tabulate import tabulate\n",
    "\n",
    "# # 🔹 **Gerät konfigurieren**\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # 🔹 **Funktion für Vorhersagen**\n",
    "# def make_predictions(model, test_loader):\n",
    "#     \"\"\"Generate predictions using the model\"\"\"\n",
    "#     model.eval()\n",
    "#     predictions, actuals = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in tqdm(test_loader, desc=\"Making predictions\"):\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             predictions.append(outputs.cpu().numpy())\n",
    "#             actuals.append(targets.cpu().numpy())\n",
    "#     return np.concatenate(predictions), np.concatenate(actuals)\n",
    "\n",
    "# # 🔹 **Modelle aus Checkpoints laden**\n",
    "# def load_model_weights(model, model_name, epoch):\n",
    "#     \"\"\"Load model weights with proper path handling\"\"\"\n",
    "#     checkpoint_dir = Path('checkpoints')\n",
    "#     checkpoint_path = checkpoint_dir / f'{model_name}_epoch{epoch}.pt'\n",
    "\n",
    "#     if not checkpoint_path.exists():\n",
    "#         raise FileNotFoundError(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "\n",
    "#     print(f'Loading model weights from: {checkpoint_path}')\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # 🔹 **Zielvariablen rückskalieren**\n",
    "# def inverse_scale_predictions(predictions, actuals, scaler):\n",
    "#     \"\"\"Convert scaled predictions back to original units\"\"\"\n",
    "#     pred_orig = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "#     act_orig = scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "#     return pred_orig, act_orig\n",
    "\n",
    "# # 🔹 **Metriken berechnen**\n",
    "# def calculate_metrics(predictions, actuals):\n",
    "#     \"\"\"Calculate metrics\"\"\"\n",
    "#     rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "#     mae = mean_absolute_error(actuals, predictions)\n",
    "#     r2 = r2_score(actuals, predictions)\n",
    "#     return rmse, mae, r2\n",
    "\n",
    "# def plot_predictions_per_household(predictions, actuals, household_ids, timestamps, model_name, window_size=24):\n",
    "#     \"\"\"Plot predictions for individual households\"\"\"\n",
    "#     min_len = min(len(predictions), len(actuals), len(household_ids), len(timestamps))\n",
    "#     predictions = predictions[:min_len]\n",
    "#     actuals = actuals[:min_len]\n",
    "#     household_ids = household_ids[:min_len]\n",
    "#     timestamps = timestamps[:min_len]\n",
    "\n",
    "#     unique_homes = np.unique(household_ids)\n",
    "#     print(f\"Total unique households: {len(unique_homes)}\")\n",
    "\n",
    "#     plot_dir = Path('../reports/figures')\n",
    "#     plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     for home in unique_homes[:5]:  # Plot only the first 5 households\n",
    "#         indices = np.where(household_ids == home)[0]\n",
    "#         if len(indices) == 0:\n",
    "#             continue\n",
    "\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         time_series = pd.to_datetime(timestamps[indices])\n",
    "        \n",
    "#         # Check if timestamps are unique\n",
    "#         if len(set(time_series)) != len(time_series):\n",
    "#             print(f\"Warning: Duplicate timestamps found for household {home}\")\n",
    "\n",
    "#         actual_smooth = pd.Series(actuals[indices], index=time_series).rolling(window=window_size).mean()\n",
    "#         pred_smooth = pd.Series(predictions[indices], index=time_series).rolling(window=window_size).mean()\n",
    "\n",
    "#         plt.plot(actual_smooth, label=\"Actual (Smoothed)\", color='blue', alpha=0.6)\n",
    "#         plt.plot(pred_smooth, label=\"Predicted (Smoothed)\", color='red', alpha=0.6)\n",
    "#         plt.title(f\"{model_name} Predictions for Household {home}\")\n",
    "#         plt.xlabel(\"Time\")\n",
    "#         plt.ylabel(\"Consumption (Wh)\")\n",
    "#         plt.legend()\n",
    "#         plt.grid(True, alpha=0.3)\n",
    "#         plt.xticks(rotation=45)\n",
    "\n",
    "#         save_path = plot_dir / f'{model_name.lower()}_household_{home}_predictions.png'\n",
    "#         plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "\n",
    "# # 🔹 **Modell-Evaluation**\n",
    "# def evaluate_model(model, model_name, test_loader, scaler, household_ids, timestamps):\n",
    "#     \"\"\"Evaluate model performance\"\"\"\n",
    "#     print(f\"\\nEvaluating {model_name} model...\")\n",
    "\n",
    "#     scaled_pred, scaled_act = make_predictions(model, test_loader)\n",
    "#     predictions, actuals = inverse_scale_predictions(scaled_pred, scaled_act, scaler)\n",
    "#     rmse, mae, r2 = calculate_metrics(predictions, actuals)\n",
    "\n",
    "#     metrics_df = pd.DataFrame({\n",
    "#         'Metric': ['RMSE', 'MAE', 'R²'],\n",
    "#         'Value': [f\"{rmse:.2f} Wh\", f\"{mae:.2f} Wh\", f\"{r2:.4f}\"]\n",
    "#     })\n",
    "\n",
    "#     print(f\"\\nMetrics for {model_name} Energy Consumption:\")\n",
    "#     print(tabulate(metrics_df, headers='keys', tablefmt='pipe', showindex=False))\n",
    "\n",
    "#     plot_predictions_per_household(predictions, actuals, household_ids, timestamps, model_name)\n",
    "\n",
    "#     return predictions, actuals, {'model': model_name, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "# # 🔹 **Scaler laden & validieren**\n",
    "# def load_and_verify_scalers():\n",
    "#     \"\"\"Load scalers and verify their existence\"\"\"\n",
    "#     try:\n",
    "#         scaler_electric = load('scalers/scaler_electric.pkl')\n",
    "#         scaler_gas = load('scalers/scaler_gas.pkl')\n",
    "#         print(\"Successfully loaded scalers\")\n",
    "\n",
    "#         return scaler_electric, scaler_gas\n",
    "#     except FileNotFoundError:\n",
    "#         raise FileNotFoundError(\"Scalers not found. Please run preprocessing first.\")\n",
    "#     except Exception as e:\n",
    "#         raise Exception(f\"Error loading scalers: {e}\")\n",
    "\n",
    "# # Scaler laden\n",
    "# scaler_electric, scaler_gas = load_and_verify_scalers()\n",
    "\n",
    "# # 🔹 **Modelle initialisieren**\n",
    "# input_size = len(feature_columns)\n",
    "# print(f\"Model input size: {input_size}\")\n",
    "\n",
    "# models = {\n",
    "#     'Electric': LSTMModel(input_size=input_size, hidden_size=32, num_layers=2, output_size=1).to(device),\n",
    "#     'Gas': LSTMModel(input_size=input_size, hidden_size=32, num_layers=2, output_size=1).to(device)\n",
    "# }\n",
    "\n",
    "# scalers = {\n",
    "#     'Electric': scaler_electric,\n",
    "#     'Gas': scaler_gas\n",
    "# }\n",
    "\n",
    "# # Haushalts-IDs & Timestamps extrahieren\n",
    "# household_ids = {\n",
    "#     'Electric': df_clean.loc[df_clean.index[-len(y_test_electric):], 'homeid'].values[:len(y_test_electric)],\n",
    "#     'Gas': df_clean.loc[df_clean.index[-len(y_test_gas):], 'homeid'].values[:len(y_test_gas)]\n",
    "# }\n",
    "\n",
    "# timestamps = {\n",
    "#     'Electric': df_clean.index[-len(y_test_electric):].values[:len(y_test_electric)],\n",
    "#     'Gas': df_clean.index[-len(y_test_gas):].values[:len(y_test_gas)]\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 🔹 **Modelle evaluieren**\n",
    "# all_metrics = []\n",
    "# for name, model in models.items():\n",
    "#     model = load_model_weights(model, name.lower(), epoch=6)\n",
    "#     predictions, actuals, metrics = evaluate_model(\n",
    "#         model, name, test_loaders[name],\n",
    "#         scalers[name], household_ids[name], timestamps[name]\n",
    "#     )\n",
    "#     results[name] = {'predictions': predictions, 'actuals': actuals, 'metrics': metrics}\n",
    "#     all_metrics.append(metrics)\n",
    "\n",
    "# # 🔹 **Metriken speichern**\n",
    "# final_metrics = pd.DataFrame(all_metrics)\n",
    "# metrics_dir = Path('../reports/tables')\n",
    "# metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "# final_metrics.to_csv(metrics_dir / 'model_metrics.csv', index=False)\n",
    "\n",
    "# print(\"\\nFinal Model Comparison:\")\n",
    "# print(tabulate(final_metrics, headers='keys', tablefmt='pipe', showindex=False))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Predictive_Analytics_Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sudo-Oliver/Predictive-Analytics-Private/blob/main/notebooks/LSTM%20Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7R7RQPWgqys"
   },
   "source": [
    "**1. Daten laden und vorbereiten**\n",
    "1. Laden der Daten in einen Dataframe\n",
    "2. Zeitspalte umwandeln (Unix-Timestamp -> Datetime)\n",
    "3. nach homeid gruppieren (jeder Haushalt hat seine eigene Zeitreihe)\n",
    "4. Sortieren nach Zeit innerhalb des Haushalts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPGwAXsugqyt",
    "outputId": "3c20df7b-85f1-4803-d5cc-6b5abf07b366"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import gdown\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import dump, load\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Metal plugin available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Metal GPU will be used\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Metal plugin available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Configure memory growth\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(\"Metal GPU will be used\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oD_P2DPgqyv"
   },
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     \"\"\"Clean and preprocess sensor data\"\"\"\n",
    "#     # Convert Unix timestamp to datetime\n",
    "#     df['timestamp_local'] = pd.to_datetime(df['timestamp_local'], unit='ms')\n",
    "\n",
    "#     # Set timestamp_local as index\n",
    "#     df.set_index('timestamp_local', inplace=True)\n",
    "\n",
    "#     # Sort by homeid and timestamp_local\n",
    "#     df = df.sort_values(by=['homeid', 'timestamp_local'])\n",
    "\n",
    "#     # Remove specified columns\n",
    "#     columns_to_drop = [\n",
    "#         'sensorid', 'median_temperature', '_room',\n",
    "#         'sensorid_room', 'measured_entity',\n",
    "#         'sensorid_electric', 'sensorid_gas'\n",
    "#     ]\n",
    "#     df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully: (1641653, 23) rows\n",
      "\n",
      "Cleaning data...\n",
      "\n",
      "Creating features...\n",
      "Handling missing values...\n",
      "\n",
      "Data shape after preprocessing: (1641653, 28)\n",
      "Index type: <class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
      "Feature columns: ['homeid', 'electric_min_consumption', 'electric_max_consumption', 'std_consumption', 'electric_median_consumption', 'electric_total_consumption_kWh', 'gas_mean_consumption', 'gas_min_consumption', 'gas_max_consumption', 'gas_median_consumption', 'gas_total_consumption_kWh', 'median_value', 'roomid', 'income_band_mid', 'education_map', 'hour', 'hour_sin', 'hour_cos', 'electric_lag_1', 'gas_lag_1', 'electric_lag_2', 'gas_lag_2', 'electric_lag_3', 'gas_lag_3', 'electric_rolling_mean_3h', 'electric_rolling_mean_7h', 'gas_rolling_mean_3h', 'gas_rolling_mean_7h']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "timestamp_local",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "homeid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "electric_min_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_max_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_median_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_total_consumption_kWh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_mean_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_min_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_max_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_median_consumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_total_consumption_kWh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "median_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roomid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "income_band_mid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "education_map",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "hour_sin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hour_cos",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_rolling_mean_3h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "electric_rolling_mean_7h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_rolling_mean_3h",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_rolling_mean_7h",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "aae9d69f-7445-47b9-b321-b88530054609",
       "rows": [
        [
         "2016-09-20 09:00:00",
         "47",
         "0.069",
         "0.335",
         "0.0339046921378041",
         "0.194",
         "0.1798068371317398",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.224",
         "20.72",
         "652.0",
         "0.0",
         "8.0",
         "9",
         "0.7071067811865476",
         "-0.7071067811865475",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.17669019228437363",
         "0.17045690258964122",
         "0.21",
         "0.182"
        ],
        [
         "2016-09-20 10:00:00",
         "47",
         "0.068875",
         "0.45837500000000003",
         "0.03587545170777",
         "0.18762500000000001",
         "0.1766901922843736",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.21",
         "20.695",
         "652.0",
         "0.0",
         "8.0",
         "10",
         "0.49999999999999994",
         "-0.8660254037844387",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.17669019228437363",
         "0.17045690258964122",
         "0.21",
         "0.182"
        ],
        [
         "2016-09-20 11:00:00",
         "47",
         "0.06875",
         "0.58175",
         "0.0378462112777359",
         "0.18125",
         "0.17357354743700742",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.196",
         "20.669999999999998",
         "652.0",
         "0.0",
         "8.0",
         "11",
         "0.258819045102521",
         "-0.9659258262890682",
         "0.1766901922843736",
         "0.21",
         "0.1798068371317398",
         "0.224",
         "0.1798068371317398",
         "0.224",
         "0.17669019228437363",
         "0.17045690258964122",
         "0.21",
         "0.182"
        ],
        [
         "2016-09-20 12:00:00",
         "47",
         "0.068625",
         "0.705125",
         "0.0398169708477018",
         "0.174875",
         "0.17045690258964122",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.182",
         "20.645",
         "652.0",
         "0.0",
         "8.0",
         "12",
         "1.2246467991473532e-16",
         "-1.0",
         "0.17357354743700742",
         "0.196",
         "0.1766901922843736",
         "0.21",
         "0.1798068371317398",
         "0.224",
         "0.1735735474370074",
         "0.17045690258964122",
         "0.19600000000000004",
         "0.182"
        ],
        [
         "2016-09-20 13:00:00",
         "47",
         "0.0685",
         "0.8285",
         "0.0417877304176677",
         "0.16849999999999998",
         "0.167340257742275",
         "0.112",
         "0.112",
         "0.112",
         "0.112",
         "0.168",
         "20.619999999999997",
         "652.0",
         "0.0",
         "8.0",
         "13",
         "-0.2588190451025208",
         "-0.9659258262890683",
         "0.17045690258964122",
         "0.182",
         "0.17357354743700742",
         "0.196",
         "0.1766901922843736",
         "0.21",
         "0.17045690258964122",
         "0.17045690258964122",
         "0.18200000000000002",
         "0.182"
        ]
       ],
       "shape": {
        "columns": 28,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homeid</th>\n",
       "      <th>electric_min_consumption</th>\n",
       "      <th>electric_max_consumption</th>\n",
       "      <th>std_consumption</th>\n",
       "      <th>electric_median_consumption</th>\n",
       "      <th>electric_total_consumption_kWh</th>\n",
       "      <th>gas_mean_consumption</th>\n",
       "      <th>gas_min_consumption</th>\n",
       "      <th>gas_max_consumption</th>\n",
       "      <th>gas_median_consumption</th>\n",
       "      <th>...</th>\n",
       "      <th>electric_lag_1</th>\n",
       "      <th>gas_lag_1</th>\n",
       "      <th>electric_lag_2</th>\n",
       "      <th>gas_lag_2</th>\n",
       "      <th>electric_lag_3</th>\n",
       "      <th>gas_lag_3</th>\n",
       "      <th>electric_rolling_mean_3h</th>\n",
       "      <th>electric_rolling_mean_7h</th>\n",
       "      <th>gas_rolling_mean_3h</th>\n",
       "      <th>gas_rolling_mean_7h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp_local</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-09-20 09:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.033905</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 10:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068875</td>\n",
       "      <td>0.458375</td>\n",
       "      <td>0.035875</td>\n",
       "      <td>0.187625</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 11:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.581750</td>\n",
       "      <td>0.037846</td>\n",
       "      <td>0.181250</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 12:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068625</td>\n",
       "      <td>0.705125</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.174875</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 13:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.828500</td>\n",
       "      <td>0.041788</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.167340</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     homeid  electric_min_consumption  \\\n",
       "timestamp_local                                         \n",
       "2016-09-20 09:00:00      47                  0.069000   \n",
       "2016-09-20 10:00:00      47                  0.068875   \n",
       "2016-09-20 11:00:00      47                  0.068750   \n",
       "2016-09-20 12:00:00      47                  0.068625   \n",
       "2016-09-20 13:00:00      47                  0.068500   \n",
       "\n",
       "                     electric_max_consumption  std_consumption  \\\n",
       "timestamp_local                                                  \n",
       "2016-09-20 09:00:00                  0.335000         0.033905   \n",
       "2016-09-20 10:00:00                  0.458375         0.035875   \n",
       "2016-09-20 11:00:00                  0.581750         0.037846   \n",
       "2016-09-20 12:00:00                  0.705125         0.039817   \n",
       "2016-09-20 13:00:00                  0.828500         0.041788   \n",
       "\n",
       "                     electric_median_consumption  \\\n",
       "timestamp_local                                    \n",
       "2016-09-20 09:00:00                     0.194000   \n",
       "2016-09-20 10:00:00                     0.187625   \n",
       "2016-09-20 11:00:00                     0.181250   \n",
       "2016-09-20 12:00:00                     0.174875   \n",
       "2016-09-20 13:00:00                     0.168500   \n",
       "\n",
       "                     electric_total_consumption_kWh  gas_mean_consumption  \\\n",
       "timestamp_local                                                             \n",
       "2016-09-20 09:00:00                        0.179807                 0.112   \n",
       "2016-09-20 10:00:00                        0.176690                 0.112   \n",
       "2016-09-20 11:00:00                        0.173574                 0.112   \n",
       "2016-09-20 12:00:00                        0.170457                 0.112   \n",
       "2016-09-20 13:00:00                        0.167340                 0.112   \n",
       "\n",
       "                     gas_min_consumption  gas_max_consumption  \\\n",
       "timestamp_local                                                 \n",
       "2016-09-20 09:00:00                0.112                0.112   \n",
       "2016-09-20 10:00:00                0.112                0.112   \n",
       "2016-09-20 11:00:00                0.112                0.112   \n",
       "2016-09-20 12:00:00                0.112                0.112   \n",
       "2016-09-20 13:00:00                0.112                0.112   \n",
       "\n",
       "                     gas_median_consumption  ...  electric_lag_1  gas_lag_1  \\\n",
       "timestamp_local                              ...                              \n",
       "2016-09-20 09:00:00                   0.112  ...        0.179807      0.224   \n",
       "2016-09-20 10:00:00                   0.112  ...        0.179807      0.224   \n",
       "2016-09-20 11:00:00                   0.112  ...        0.176690      0.210   \n",
       "2016-09-20 12:00:00                   0.112  ...        0.173574      0.196   \n",
       "2016-09-20 13:00:00                   0.112  ...        0.170457      0.182   \n",
       "\n",
       "                     electric_lag_2  gas_lag_2  electric_lag_3  gas_lag_3  \\\n",
       "timestamp_local                                                             \n",
       "2016-09-20 09:00:00        0.179807      0.224        0.179807      0.224   \n",
       "2016-09-20 10:00:00        0.179807      0.224        0.179807      0.224   \n",
       "2016-09-20 11:00:00        0.179807      0.224        0.179807      0.224   \n",
       "2016-09-20 12:00:00        0.176690      0.210        0.179807      0.224   \n",
       "2016-09-20 13:00:00        0.173574      0.196        0.176690      0.210   \n",
       "\n",
       "                     electric_rolling_mean_3h  electric_rolling_mean_7h  \\\n",
       "timestamp_local                                                           \n",
       "2016-09-20 09:00:00                  0.176690                  0.170457   \n",
       "2016-09-20 10:00:00                  0.176690                  0.170457   \n",
       "2016-09-20 11:00:00                  0.176690                  0.170457   \n",
       "2016-09-20 12:00:00                  0.173574                  0.170457   \n",
       "2016-09-20 13:00:00                  0.170457                  0.170457   \n",
       "\n",
       "                     gas_rolling_mean_3h  gas_rolling_mean_7h  \n",
       "timestamp_local                                                \n",
       "2016-09-20 09:00:00                0.210                0.182  \n",
       "2016-09-20 10:00:00                0.210                0.182  \n",
       "2016-09-20 11:00:00                0.210                0.182  \n",
       "2016-09-20 12:00:00                0.196                0.182  \n",
       "2016-09-20 13:00:00                0.182                0.182  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "import gdown\n",
    "\n",
    "def load_processed_data():\n",
    "    \"\"\"Load preprocessed sensor data with fallback to Drive download\"\"\"\n",
    "    file_id = \"1KHQCVfwTxm5bjjITS8WMm9P3M12ETVsR\"\n",
    "    \n",
    "    download_path = Path('data/processed')\n",
    "    download_path.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = download_path / 'final_processed_data3.parquet'\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(\"Downloading from Google Drive...\")\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(url, str(file_path), quiet=False)\n",
    "    \n",
    "    if file_path.exists():\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(f\"Data loaded successfully: {df.shape} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not load or download data file\")\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and preprocess sensor data\"\"\"\n",
    "    # Convert Unix timestamp to datetime if needed\n",
    "    if 'timestamp_local' in df.columns:\n",
    "        df['timestamp_local'] = pd.to_datetime(df['timestamp_local'], unit='ms')\n",
    "        # Sort before setting index\n",
    "        df = df.sort_values(by=['homeid', 'timestamp_local'])\n",
    "        # Then set index\n",
    "        df.set_index('timestamp_local', inplace=True)\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    columns_to_drop = [\n",
    "        'sensorid', 'median_temperature', '_room',\n",
    "        'sensorid_room', 'measured_entity',\n",
    "        'sensorid_electric', 'sensorid_gas'\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    \n",
    "    # First rename consumption columns from Wh to kWh\n",
    "    df = df.rename(columns={\n",
    "        'electric_total_consumption_Wh': 'electric_total_consumption_kWh',\n",
    "        'gas_total_consumption_Wh': 'gas_total_consumption_kWh'\n",
    "    })\n",
    "    \n",
    "    # Replace negative values with 0 in gas columns\n",
    "    gas_columns = [\n",
    "        'gas_mean_consumption', \n",
    "        'gas_min_consumption', \n",
    "        'gas_max_consumption',\n",
    "        'gas_median_consumption', \n",
    "        'gas_total_consumption_kWh'\n",
    "    ]\n",
    "    \n",
    "    # Use clip to replace negative values with 0 (more efficient than applymap)\n",
    "    df[gas_columns] = df[gas_columns].clip(lower=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_features(df, df_original):\n",
    "    \"\"\"Create time-based and lag features\"\"\"\n",
    "    # Verify datetime index\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame must have DatetimeIndex\")\n",
    "    \n",
    "    # Cyclical time features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    # Lag features using original values\n",
    "    for lag in range(1, 4):\n",
    "        df[f'electric_lag_{lag}'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].shift(lag)\n",
    "        df[f'gas_lag_{lag}'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].shift(lag)\n",
    "    \n",
    "    # Rolling means using original values\n",
    "    df['electric_rolling_mean_3h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "    df['electric_rolling_mean_7h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    df['gas_rolling_mean_3h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "    df['gas_rolling_mean_7h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main preprocessing pipeline\n",
    "print(\"Loading data...\")\n",
    "df = load_processed_data()\n",
    "\n",
    "print(\"\\nCleaning data...\")\n",
    "df_clean = clean_data(df.copy())\n",
    "df_original = df_clean.copy()  # Keep unmodified copy for feature creation\n",
    "\n",
    "print(\"\\nCreating features...\")\n",
    "df_clean = create_features(df_clean, df_original)\n",
    "\n",
    "print(\"Handling missing values...\")\n",
    "df_clean = df_clean.ffill().bfill()\n",
    "\n",
    "# Verify data\n",
    "print(\"\\nData shape after preprocessing:\", df_clean.shape)\n",
    "print(\"Index type:\", type(df_clean.index))\n",
    "print(\"Feature columns:\", df_clean.columns.tolist())\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "bvXa2w8Ggqyv",
    "outputId": "d8654319-74fb-40c1-c690-abf5f0c0556f"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# from joblib import dump, load\n",
    "# import gdown\n",
    "\n",
    "# def load_processed_data():\n",
    "#     \"\"\"Load preprocessed sensor data with fallback to Drive download\"\"\"\n",
    "#     file_id = \"1KHQCVfwTxm5bjjITS8WMm9P3M12ETVsR\"\n",
    "    \n",
    "#     download_path = Path('data/processed')\n",
    "#     download_path.mkdir(parents=True, exist_ok=True)\n",
    "#     file_path = download_path / 'final_processed_data3.parquet'\n",
    "    \n",
    "#     if not file_path.exists():\n",
    "#         print(\"Downloading from Google Drive...\")\n",
    "#         url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "#         gdown.download(url, str(file_path), quiet=False)\n",
    "    \n",
    "#     if file_path.exists():\n",
    "#         df = pd.read_parquet(file_path)\n",
    "#         print(f\"Data loaded successfully: {df.shape} rows\")\n",
    "#         return df\n",
    "#     else:\n",
    "#         raise FileNotFoundError(\"Could not load or download data file\")\n",
    "\n",
    "# def create_features(df, df_original):\n",
    "#     \"\"\"Create time-based and lag features\"\"\"\n",
    "#     # Cyclical time features\n",
    "#     df['hour'] = df.index.hour\n",
    "#     df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "#     df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "#     # Lag features using original values\n",
    "#     for lag in range(1, 4):\n",
    "#         df[f'electric_lag_{lag}'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].shift(lag)\n",
    "#         df[f'gas_lag_{lag}'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].shift(lag)\n",
    "    \n",
    "#     # Rolling means using original values\n",
    "#     df['electric_rolling_mean_3h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "#     df['electric_rolling_mean_7h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "#     df['gas_rolling_mean_3h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "#     df['gas_rolling_mean_7h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Load data and create copies\n",
    "# print(\"Loading data...\")\n",
    "# df = load_processed_data()\n",
    "# df_clean = df.copy()\n",
    "# df_original = df.copy()  # Keep unmodified copy for feature creation\n",
    "\n",
    "# # Create features\n",
    "# print(\"\\nCreating features...\")\n",
    "# df_clean = create_features(df_clean, df_original)\n",
    "\n",
    "# # Handle missing values\n",
    "# print(\"Handling missing values...\")\n",
    "# df_clean = df_clean.ffill().bfill()\n",
    "\n",
    "# # Verify data\n",
    "# print(\"\\nData shape after preprocessing:\", df_clean.shape)\n",
    "# print(\"\\nFeature columns:\", df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "PSojNn9jgqyw",
    "outputId": "1a694ec4-2460-4986-e79e-4b6beda59b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Feature Correlation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "electric_total_consumption_kWh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gas_total_consumption_kWh",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "caeabfbb-ed19-4a3f-9d08-59fcbdf8b96e",
       "rows": [
        [
         "electric_total_consumption_kWh",
         "1.0",
         "0.032210991188142836"
        ],
        [
         "electric_rolling_mean_3h",
         "0.9100313160756388",
         "0.033895102836715846"
        ],
        [
         "electric_lag_1",
         "0.8152911899689684",
         "0.031145631508973527"
        ],
        [
         "electric_median_consumption",
         "0.8039026321537704",
         "0.025040525478488743"
        ],
        [
         "electric_rolling_mean_7h",
         "0.7951973862485043",
         "0.03580226184882135"
        ],
        [
         "std_consumption",
         "0.7750318839212954",
         "0.024052570289109152"
        ],
        [
         "electric_lag_2",
         "0.7059717833876442",
         "0.03055058605861383"
        ],
        [
         "electric_max_consumption",
         "0.6930172949513878",
         "0.04279976159803156"
        ],
        [
         "electric_lag_3",
         "0.6295458169890661",
         "0.03003627871681092"
        ],
        [
         "electric_min_consumption",
         "0.5166742860716043",
         "0.0396700797804552"
        ],
        [
         "income_band_mid",
         "0.15442070841511235",
         "0.03456014100539602"
        ],
        [
         "hour",
         "0.11771607001849929",
         "0.0016066641661523786"
        ],
        [
         "hour_sin",
         "0.11401568646886993",
         "0.0005382683243789035"
        ],
        [
         "median_value",
         "0.06626155582748322",
         "0.0030776618529730765"
        ],
        [
         "education_map",
         "0.05402817584363957",
         "0.012455997045961453"
        ],
        [
         "gas_total_consumption_kWh",
         "0.032210991188142836",
         "1.0"
        ],
        [
         "gas_lag_1",
         "0.03205067975127647",
         "0.9981468984709488"
        ],
        [
         "gas_rolling_mean_3h",
         "0.03195737862919121",
         "0.9990907342402742"
        ],
        [
         "gas_lag_2",
         "0.03152847189690646",
         "0.9970069222733274"
        ],
        [
         "gas_lag_3",
         "0.030912347607689",
         "0.9958242448619099"
        ],
        [
         "gas_rolling_mean_7h",
         "0.030865889960066666",
         "0.9973870585928848"
        ],
        [
         "gas_max_consumption",
         "0.02823352450427548",
         "0.9992745930927395"
        ],
        [
         "gas_mean_consumption",
         "0.025709881120095607",
         "0.9592977179253811"
        ],
        [
         "roomid",
         "0.025531138578427186",
         "0.033830527921338734"
        ],
        [
         "gas_median_consumption",
         "0.024411967434895307",
         "0.9405117332540934"
        ],
        [
         "hour_cos",
         "0.02439850631881646",
         "0.0019150250130588956"
        ],
        [
         "gas_min_consumption",
         "0.02397354698486005",
         "0.886737172191856"
        ],
        [
         "homeid",
         "0.023285963778932616",
         "0.03525501444483065"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 28
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>electric_total_consumption_kWh</th>\n",
       "      <th>gas_total_consumption_kWh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>electric_total_consumption_kWh</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_rolling_mean_3h</th>\n",
       "      <td>0.910031</td>\n",
       "      <td>0.033895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_lag_1</th>\n",
       "      <td>0.815291</td>\n",
       "      <td>0.031146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_median_consumption</th>\n",
       "      <td>0.803903</td>\n",
       "      <td>0.025041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_rolling_mean_7h</th>\n",
       "      <td>0.795197</td>\n",
       "      <td>0.035802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_consumption</th>\n",
       "      <td>0.775032</td>\n",
       "      <td>0.024053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_lag_2</th>\n",
       "      <td>0.705972</td>\n",
       "      <td>0.030551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_max_consumption</th>\n",
       "      <td>0.693017</td>\n",
       "      <td>0.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_lag_3</th>\n",
       "      <td>0.629546</td>\n",
       "      <td>0.030036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_min_consumption</th>\n",
       "      <td>0.516674</td>\n",
       "      <td>0.039670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income_band_mid</th>\n",
       "      <td>0.154421</td>\n",
       "      <td>0.034560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>0.117716</td>\n",
       "      <td>0.001607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_sin</th>\n",
       "      <td>0.114016</td>\n",
       "      <td>0.000538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_value</th>\n",
       "      <td>0.066262</td>\n",
       "      <td>0.003078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_map</th>\n",
       "      <td>0.054028</td>\n",
       "      <td>0.012456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_total_consumption_kWh</th>\n",
       "      <td>0.032211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_lag_1</th>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.998147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_rolling_mean_3h</th>\n",
       "      <td>0.031957</td>\n",
       "      <td>0.999091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_lag_2</th>\n",
       "      <td>0.031528</td>\n",
       "      <td>0.997007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_lag_3</th>\n",
       "      <td>0.030912</td>\n",
       "      <td>0.995824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_rolling_mean_7h</th>\n",
       "      <td>0.030866</td>\n",
       "      <td>0.997387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_max_consumption</th>\n",
       "      <td>0.028234</td>\n",
       "      <td>0.999275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_mean_consumption</th>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.959298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roomid</th>\n",
       "      <td>0.025531</td>\n",
       "      <td>0.033831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_median_consumption</th>\n",
       "      <td>0.024412</td>\n",
       "      <td>0.940512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour_cos</th>\n",
       "      <td>0.024399</td>\n",
       "      <td>0.001915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_min_consumption</th>\n",
       "      <td>0.023974</td>\n",
       "      <td>0.886737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homeid</th>\n",
       "      <td>0.023286</td>\n",
       "      <td>0.035255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                electric_total_consumption_kWh  \\\n",
       "electric_total_consumption_kWh                        1.000000   \n",
       "electric_rolling_mean_3h                              0.910031   \n",
       "electric_lag_1                                        0.815291   \n",
       "electric_median_consumption                           0.803903   \n",
       "electric_rolling_mean_7h                              0.795197   \n",
       "std_consumption                                       0.775032   \n",
       "electric_lag_2                                        0.705972   \n",
       "electric_max_consumption                              0.693017   \n",
       "electric_lag_3                                        0.629546   \n",
       "electric_min_consumption                              0.516674   \n",
       "income_band_mid                                       0.154421   \n",
       "hour                                                  0.117716   \n",
       "hour_sin                                              0.114016   \n",
       "median_value                                          0.066262   \n",
       "education_map                                         0.054028   \n",
       "gas_total_consumption_kWh                             0.032211   \n",
       "gas_lag_1                                             0.032051   \n",
       "gas_rolling_mean_3h                                   0.031957   \n",
       "gas_lag_2                                             0.031528   \n",
       "gas_lag_3                                             0.030912   \n",
       "gas_rolling_mean_7h                                   0.030866   \n",
       "gas_max_consumption                                   0.028234   \n",
       "gas_mean_consumption                                  0.025710   \n",
       "roomid                                                0.025531   \n",
       "gas_median_consumption                                0.024412   \n",
       "hour_cos                                              0.024399   \n",
       "gas_min_consumption                                   0.023974   \n",
       "homeid                                                0.023286   \n",
       "\n",
       "                                gas_total_consumption_kWh  \n",
       "electric_total_consumption_kWh                   0.032211  \n",
       "electric_rolling_mean_3h                         0.033895  \n",
       "electric_lag_1                                   0.031146  \n",
       "electric_median_consumption                      0.025041  \n",
       "electric_rolling_mean_7h                         0.035802  \n",
       "std_consumption                                  0.024053  \n",
       "electric_lag_2                                   0.030551  \n",
       "electric_max_consumption                         0.042800  \n",
       "electric_lag_3                                   0.030036  \n",
       "electric_min_consumption                         0.039670  \n",
       "income_band_mid                                  0.034560  \n",
       "hour                                             0.001607  \n",
       "hour_sin                                         0.000538  \n",
       "median_value                                     0.003078  \n",
       "education_map                                    0.012456  \n",
       "gas_total_consumption_kWh                        1.000000  \n",
       "gas_lag_1                                        0.998147  \n",
       "gas_rolling_mean_3h                              0.999091  \n",
       "gas_lag_2                                        0.997007  \n",
       "gas_lag_3                                        0.995824  \n",
       "gas_rolling_mean_7h                              0.997387  \n",
       "gas_max_consumption                              0.999275  \n",
       "gas_mean_consumption                             0.959298  \n",
       "roomid                                           0.033831  \n",
       "gas_median_consumption                           0.940512  \n",
       "hour_cos                                         0.001915  \n",
       "gas_min_consumption                              0.886737  \n",
       "homeid                                           0.035255  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vollständige Korrelation mit allen spalten berechnen\n",
    "correlation_matrix_all = df_clean.corr()\n",
    "\n",
    "# Korrelation der Features mit den Zielvariablen (Strom und Gasverbtauch)\n",
    "correlation_target_all = correlation_matrix_all[['electric_total_consumption_kWh', 'gas_total_consumption_kWh']]\n",
    "\n",
    "# Sortieren nach Stärke der Korrelation\n",
    "correlation_target_all_sorted = correlation_target_all.abs().sort_values(by=['electric_total_consumption_kWh', 'gas_total_consumption_kWh'], ascending=False)\n",
    "\n",
    "# Korrelationsergebnisse anzeigen\n",
    "print(\"Full Feature Correlation:\")\n",
    "display(correlation_target_all_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Feature Engineering & Scaling ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2FWybG4Qgqyw",
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original values before scaling:\n",
      "electric_min: 0.00\n",
      "electric_max: 11.65\n",
      "gas_min: 0.00\n",
      "gas_max: 3649.58\n",
      "\n",
      " Verifying scalers:\n",
      "\n",
      "Electric scaler:\n",
      "Data min: 0.00\n",
      "Data max: 11.65\n",
      "Scale: 0.085833\n",
      "\n",
      "Gas scaler:\n",
      "Data min: 0.00\n",
      "Data max: 3649.58\n",
      "Scale: 0.000274\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "\n",
    "# Create scalers directory\n",
    "Path('scalers').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store original unscaled data\n",
    "df_original = df_clean.copy()\n",
    "\n",
    "# Extract hour and create cyclical features\n",
    "df_clean['hour'] = df_clean.index.hour\n",
    "df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)\n",
    "df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)\n",
    "\n",
    "# Create lag features using original values\n",
    "for lag in range(1, 4):\n",
    "    df_clean[f'electric_lag_{lag}'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].shift(lag)\n",
    "    df_clean[f'gas_lag_{lag}'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].shift(lag)\n",
    "\n",
    "# Create rolling means using original values\n",
    "df_clean['electric_rolling_mean_3h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df_clean['electric_rolling_mean_7h'] = df_original.groupby('homeid')['electric_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "df_clean['gas_rolling_mean_3h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df_clean['gas_rolling_mean_7h'] = df_original.groupby('homeid')['gas_total_consumption_kWh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = df_clean.ffill().bfill()\n",
    "\n",
    "# Get original min/max values from unscaled data\n",
    "scaling_params = {\n",
    "    \"electric_min\": df_original[\"electric_total_consumption_kWh\"].min(),\n",
    "    \"electric_max\": df_original[\"electric_total_consumption_kWh\"].max(),\n",
    "    \"gas_min\": df_original[\"gas_total_consumption_kWh\"].min(),\n",
    "    \"gas_max\": df_original[\"gas_total_consumption_kWh\"].max(),\n",
    "}\n",
    "\n",
    "print(\"\\n Original values before scaling:\")\n",
    "for key, value in scaling_params.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "# Initialize scalers with fixed range\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_electric = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_gas = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale features (excluding target variables)\n",
    "feature_columns = [col for col in df_clean.columns if col not in ['electric_total_consumption_kWh', 'gas_total_consumption_kWh', 'homeid', 'hour']]\n",
    "df_clean[feature_columns] = scaler_features.fit_transform(df_clean[feature_columns])\n",
    "\n",
    "# Scale target variables using original data\n",
    "df_clean['electric_total_consumption_kWh'] = scaler_electric.fit_transform(df_original[['electric_total_consumption_kWh']])\n",
    "df_clean['gas_total_consumption_kWh'] = scaler_gas.fit_transform(df_original[['gas_total_consumption_kWh']])\n",
    "\n",
    "# Save scalers\n",
    "dump(scaler_features, 'scalers/scaler_features.pkl')\n",
    "dump(scaler_electric, 'scalers/scaler_electric.pkl')\n",
    "dump(scaler_gas, 'scalers/scaler_gas.pkl')\n",
    "dump(scaling_params, 'scalers/scaling_params.pkl')  # Speichert Originalwerte für spätere Rückskalierung\n",
    "\n",
    "# Verify scaling\n",
    "print(\"\\n Verifying scalers:\")\n",
    "for name, scaler in [('Electric', scaler_electric), ('Gas', scaler_gas)]:\n",
    "    print(f\"\\n{name} scaler:\")\n",
    "    print(f\"Data min: {scaler.data_min_[0]:.2f}\")\n",
    "    print(f\"Data max: {scaler.data_max_[0]:.2f}\")\n",
    "    print(f\"Scale: {scaler.scale_[0]:.6f}\")\n",
    "\n",
    "# Save preprocessed data\n",
    "#df_clean.to_parquet('lstm_preprocessed_data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8LZ_5OCgqyw"
   },
   "source": [
    "### 3. Trainings und Testdatensätze erstellen ###\n",
    "Erstellung von X & y für das LSTM:\n",
    "- time_steps = 90 (Vergangene 90 Werte werden für Vorhersagen genutzt)\n",
    "- create_memmap_array() nutzt Memory-Mapped-Files → Speicheroptimierung\n",
    "- process_data_efficiently() verarbeitet die Daten in Chunks → verhindert RAM-Überlastung\n",
    "- Zielwerte (y) korrekt gesetzt auf die nächsten 90 Schritte\n",
    "\n",
    "Train-Test-Split (80%-20%):\n",
    "- Kein Shuffle, damit die Zeitreihenstruktur erhalten bleibt.\n",
    "- Saubere Trennung für beide Zielvariablen (electric_total_consumption_kWh & gas_total_consumption_kWh).\n",
    "- Finale Shapes werden überprüft und ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9serancgqyw",
    "outputId": "5bdd6e36-12ae-498d-c95b-b489b718085b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing electric data...\n",
      "Progress: 0.0%\n",
      "Progress: 0.3%\n",
      "Progress: 0.6%\n",
      "Progress: 0.9%\n",
      "Progress: 1.2%\n",
      "Progress: 1.5%\n",
      "Progress: 1.8%\n",
      "Progress: 2.1%\n",
      "Progress: 2.4%\n",
      "Progress: 2.7%\n",
      "Progress: 3.0%\n",
      "Progress: 3.4%\n",
      "Progress: 3.7%\n",
      "Progress: 4.0%\n",
      "Progress: 4.3%\n",
      "Progress: 4.6%\n",
      "Progress: 4.9%\n",
      "Progress: 5.2%\n",
      "Progress: 5.5%\n",
      "Progress: 5.8%\n",
      "Progress: 6.1%\n",
      "Progress: 6.4%\n",
      "Progress: 6.7%\n",
      "Progress: 7.0%\n",
      "Progress: 7.3%\n",
      "Progress: 7.6%\n",
      "Progress: 7.9%\n",
      "Progress: 8.2%\n",
      "Progress: 8.5%\n",
      "Progress: 8.8%\n",
      "Progress: 9.1%\n",
      "Progress: 9.4%\n",
      "Progress: 9.7%\n",
      "Progress: 10.1%\n",
      "Progress: 10.4%\n",
      "Progress: 10.7%\n",
      "Progress: 11.0%\n",
      "Progress: 11.3%\n",
      "Progress: 11.6%\n",
      "Progress: 11.9%\n",
      "Progress: 12.2%\n",
      "Progress: 12.5%\n",
      "Progress: 12.8%\n",
      "Progress: 13.1%\n",
      "Progress: 13.4%\n",
      "Progress: 13.7%\n",
      "Progress: 14.0%\n",
      "Progress: 14.3%\n",
      "Progress: 14.6%\n",
      "Progress: 14.9%\n",
      "Progress: 15.2%\n",
      "Progress: 15.5%\n",
      "Progress: 15.8%\n",
      "Progress: 16.1%\n",
      "Progress: 16.4%\n",
      "Progress: 16.8%\n",
      "Progress: 17.1%\n",
      "Progress: 17.4%\n",
      "Progress: 17.7%\n",
      "Progress: 18.0%\n",
      "Progress: 18.3%\n",
      "Progress: 18.6%\n",
      "Progress: 18.9%\n",
      "Progress: 19.2%\n",
      "Progress: 19.5%\n",
      "Progress: 19.8%\n",
      "Progress: 20.1%\n",
      "Progress: 20.4%\n",
      "Progress: 20.7%\n",
      "Progress: 21.0%\n",
      "Progress: 21.3%\n",
      "Progress: 21.6%\n",
      "Progress: 21.9%\n",
      "Progress: 22.2%\n",
      "Progress: 22.5%\n",
      "Progress: 22.8%\n",
      "Progress: 23.1%\n",
      "Progress: 23.5%\n",
      "Progress: 23.8%\n",
      "Progress: 24.1%\n",
      "Progress: 24.4%\n",
      "Progress: 24.7%\n",
      "Progress: 25.0%\n",
      "Progress: 25.3%\n",
      "Progress: 25.6%\n",
      "Progress: 25.9%\n",
      "Progress: 26.2%\n",
      "Progress: 26.5%\n",
      "Progress: 26.8%\n",
      "Progress: 27.1%\n",
      "Progress: 27.4%\n",
      "Progress: 27.7%\n",
      "Progress: 28.0%\n",
      "Progress: 28.3%\n",
      "Progress: 28.6%\n",
      "Progress: 28.9%\n",
      "Progress: 29.2%\n",
      "Progress: 29.5%\n",
      "Progress: 29.8%\n",
      "Progress: 30.2%\n",
      "Progress: 30.5%\n",
      "Progress: 30.8%\n",
      "Progress: 31.1%\n",
      "Progress: 31.4%\n",
      "Progress: 31.7%\n",
      "Progress: 32.0%\n",
      "Progress: 32.3%\n",
      "Progress: 32.6%\n",
      "Progress: 32.9%\n",
      "Progress: 33.2%\n",
      "Progress: 33.5%\n",
      "Progress: 33.8%\n",
      "Progress: 34.1%\n",
      "Progress: 34.4%\n",
      "Progress: 34.7%\n",
      "Progress: 35.0%\n",
      "Progress: 35.3%\n",
      "Progress: 35.6%\n",
      "Progress: 35.9%\n",
      "Progress: 36.2%\n",
      "Progress: 36.6%\n",
      "Progress: 36.9%\n",
      "Progress: 37.2%\n",
      "Progress: 37.5%\n",
      "Progress: 37.8%\n",
      "Progress: 38.1%\n",
      "Progress: 38.4%\n",
      "Progress: 38.7%\n",
      "Progress: 39.0%\n",
      "Progress: 39.3%\n",
      "Progress: 39.6%\n",
      "Progress: 39.9%\n",
      "Progress: 40.2%\n",
      "Progress: 40.5%\n",
      "Progress: 40.8%\n",
      "Progress: 41.1%\n",
      "Progress: 41.4%\n",
      "Progress: 41.7%\n",
      "Progress: 42.0%\n",
      "Progress: 42.3%\n",
      "Progress: 42.6%\n",
      "Progress: 42.9%\n",
      "Progress: 43.3%\n",
      "Progress: 43.6%\n",
      "Progress: 43.9%\n",
      "Progress: 44.2%\n",
      "Progress: 44.5%\n",
      "Progress: 44.8%\n",
      "Progress: 45.1%\n",
      "Progress: 45.4%\n",
      "Progress: 45.7%\n",
      "Progress: 46.0%\n",
      "Progress: 46.3%\n",
      "Progress: 46.6%\n",
      "Progress: 46.9%\n",
      "Progress: 47.2%\n",
      "Progress: 47.5%\n",
      "Progress: 47.8%\n",
      "Progress: 48.1%\n",
      "Progress: 48.4%\n",
      "Progress: 48.7%\n",
      "Progress: 49.0%\n",
      "Progress: 49.3%\n",
      "Progress: 49.6%\n",
      "Progress: 50.0%\n",
      "Progress: 50.3%\n",
      "Progress: 50.6%\n",
      "Progress: 50.9%\n",
      "Progress: 51.2%\n",
      "Progress: 51.5%\n",
      "Progress: 51.8%\n",
      "Progress: 52.1%\n",
      "Progress: 52.4%\n",
      "Progress: 52.7%\n",
      "Progress: 53.0%\n",
      "Progress: 53.3%\n",
      "Progress: 53.6%\n",
      "Progress: 53.9%\n",
      "Progress: 54.2%\n",
      "Progress: 54.5%\n",
      "Progress: 54.8%\n",
      "Progress: 55.1%\n",
      "Progress: 55.4%\n",
      "Progress: 55.7%\n",
      "Progress: 56.0%\n",
      "Progress: 56.3%\n",
      "Progress: 56.7%\n",
      "Progress: 57.0%\n",
      "Progress: 57.3%\n",
      "Progress: 57.6%\n",
      "Progress: 57.9%\n",
      "Progress: 58.2%\n",
      "Progress: 58.5%\n",
      "Progress: 58.8%\n",
      "Progress: 59.1%\n",
      "Progress: 59.4%\n",
      "Progress: 59.7%\n",
      "Progress: 60.0%\n",
      "Progress: 60.3%\n",
      "Progress: 60.6%\n",
      "Progress: 60.9%\n",
      "Progress: 61.2%\n",
      "Progress: 61.5%\n",
      "Progress: 61.8%\n",
      "Progress: 62.1%\n",
      "Progress: 62.4%\n",
      "Progress: 62.7%\n",
      "Progress: 63.0%\n",
      "Progress: 63.4%\n",
      "Progress: 63.7%\n",
      "Progress: 64.0%\n",
      "Progress: 64.3%\n",
      "Progress: 64.6%\n",
      "Progress: 64.9%\n",
      "Progress: 65.2%\n",
      "Progress: 65.5%\n",
      "Progress: 65.8%\n",
      "Progress: 66.1%\n",
      "Progress: 66.4%\n",
      "Progress: 66.7%\n",
      "Progress: 67.0%\n",
      "Progress: 67.3%\n",
      "Progress: 67.6%\n",
      "Progress: 67.9%\n",
      "Progress: 68.2%\n",
      "Progress: 68.5%\n",
      "Progress: 68.8%\n",
      "Progress: 69.1%\n",
      "Progress: 69.4%\n",
      "Progress: 69.8%\n",
      "Progress: 70.1%\n",
      "Progress: 70.4%\n",
      "Progress: 70.7%\n",
      "Progress: 71.0%\n",
      "Progress: 71.3%\n",
      "Progress: 71.6%\n",
      "Progress: 71.9%\n",
      "Progress: 72.2%\n",
      "Progress: 72.5%\n",
      "Progress: 72.8%\n",
      "Progress: 73.1%\n",
      "Progress: 73.4%\n",
      "Progress: 73.7%\n",
      "Progress: 74.0%\n",
      "Progress: 74.3%\n",
      "Progress: 74.6%\n",
      "Progress: 74.9%\n",
      "Progress: 75.2%\n",
      "Progress: 75.5%\n",
      "Progress: 75.8%\n",
      "Progress: 76.1%\n",
      "Progress: 76.5%\n",
      "Progress: 76.8%\n",
      "Progress: 77.1%\n",
      "Progress: 77.4%\n",
      "Progress: 77.7%\n",
      "Progress: 78.0%\n",
      "Progress: 78.3%\n",
      "Progress: 78.6%\n",
      "Progress: 78.9%\n",
      "Progress: 79.2%\n",
      "Progress: 79.5%\n",
      "Progress: 79.8%\n",
      "Progress: 80.1%\n",
      "Progress: 80.4%\n",
      "Progress: 80.7%\n",
      "Progress: 81.0%\n",
      "Progress: 81.3%\n",
      "Progress: 81.6%\n",
      "Progress: 81.9%\n",
      "Progress: 82.2%\n",
      "Progress: 82.5%\n",
      "Progress: 82.8%\n",
      "Progress: 83.2%\n",
      "Progress: 83.5%\n",
      "Progress: 83.8%\n",
      "Progress: 84.1%\n",
      "Progress: 84.4%\n",
      "Progress: 84.7%\n",
      "Progress: 85.0%\n",
      "Progress: 85.3%\n",
      "Progress: 85.6%\n",
      "Progress: 85.9%\n",
      "Progress: 86.2%\n",
      "Progress: 86.5%\n",
      "Progress: 86.8%\n",
      "Progress: 87.1%\n",
      "Progress: 87.4%\n",
      "Progress: 87.7%\n",
      "Progress: 88.0%\n",
      "Progress: 88.3%\n",
      "Progress: 88.6%\n",
      "Progress: 88.9%\n",
      "Progress: 89.2%\n",
      "Progress: 89.5%\n",
      "Progress: 89.9%\n",
      "Progress: 90.2%\n",
      "Progress: 90.5%\n",
      "Progress: 90.8%\n",
      "Progress: 91.1%\n",
      "Progress: 91.4%\n",
      "Progress: 91.7%\n",
      "Progress: 92.0%\n",
      "Progress: 92.3%\n",
      "Progress: 92.6%\n",
      "Progress: 92.9%\n",
      "Progress: 93.2%\n",
      "Progress: 93.5%\n",
      "Progress: 93.8%\n",
      "Progress: 94.1%\n",
      "Progress: 94.4%\n",
      "Progress: 94.7%\n",
      "Progress: 95.0%\n",
      "Progress: 95.3%\n",
      "Progress: 95.6%\n",
      "Progress: 95.9%\n",
      "Progress: 96.2%\n",
      "Progress: 96.6%\n",
      "Progress: 96.9%\n",
      "Progress: 97.2%\n",
      "Progress: 97.5%\n",
      "Progress: 97.8%\n",
      "Progress: 98.1%\n",
      "Progress: 98.4%\n",
      "Progress: 98.7%\n",
      "Progress: 99.0%\n",
      "Progress: 99.3%\n",
      "Progress: 99.6%\n",
      "Progress: 99.9%\n",
      "Processing gas data...\n",
      "Progress: 0.0%\n",
      "Progress: 0.3%\n",
      "Progress: 0.6%\n",
      "Progress: 0.9%\n",
      "Progress: 1.2%\n",
      "Progress: 1.5%\n",
      "Progress: 1.8%\n",
      "Progress: 2.1%\n",
      "Progress: 2.4%\n",
      "Progress: 2.7%\n",
      "Progress: 3.0%\n",
      "Progress: 3.4%\n",
      "Progress: 3.7%\n",
      "Progress: 4.0%\n",
      "Progress: 4.3%\n",
      "Progress: 4.6%\n",
      "Progress: 4.9%\n",
      "Progress: 5.2%\n",
      "Progress: 5.5%\n",
      "Progress: 5.8%\n",
      "Progress: 6.1%\n",
      "Progress: 6.4%\n",
      "Progress: 6.7%\n",
      "Progress: 7.0%\n",
      "Progress: 7.3%\n",
      "Progress: 7.6%\n",
      "Progress: 7.9%\n",
      "Progress: 8.2%\n",
      "Progress: 8.5%\n",
      "Progress: 8.8%\n",
      "Progress: 9.1%\n",
      "Progress: 9.4%\n",
      "Progress: 9.7%\n",
      "Progress: 10.1%\n",
      "Progress: 10.4%\n",
      "Progress: 10.7%\n",
      "Progress: 11.0%\n",
      "Progress: 11.3%\n",
      "Progress: 11.6%\n",
      "Progress: 11.9%\n",
      "Progress: 12.2%\n",
      "Progress: 12.5%\n",
      "Progress: 12.8%\n",
      "Progress: 13.1%\n",
      "Progress: 13.4%\n",
      "Progress: 13.7%\n",
      "Progress: 14.0%\n",
      "Progress: 14.3%\n",
      "Progress: 14.6%\n",
      "Progress: 14.9%\n",
      "Progress: 15.2%\n",
      "Progress: 15.5%\n",
      "Progress: 15.8%\n",
      "Progress: 16.1%\n",
      "Progress: 16.4%\n",
      "Progress: 16.8%\n",
      "Progress: 17.1%\n",
      "Progress: 17.4%\n",
      "Progress: 17.7%\n",
      "Progress: 18.0%\n",
      "Progress: 18.3%\n",
      "Progress: 18.6%\n",
      "Progress: 18.9%\n",
      "Progress: 19.2%\n",
      "Progress: 19.5%\n",
      "Progress: 19.8%\n",
      "Progress: 20.1%\n",
      "Progress: 20.4%\n",
      "Progress: 20.7%\n",
      "Progress: 21.0%\n",
      "Progress: 21.3%\n",
      "Progress: 21.6%\n",
      "Progress: 21.9%\n",
      "Progress: 22.2%\n",
      "Progress: 22.5%\n",
      "Progress: 22.8%\n",
      "Progress: 23.1%\n",
      "Progress: 23.5%\n",
      "Progress: 23.8%\n",
      "Progress: 24.1%\n",
      "Progress: 24.4%\n",
      "Progress: 24.7%\n",
      "Progress: 25.0%\n",
      "Progress: 25.3%\n",
      "Progress: 25.6%\n",
      "Progress: 25.9%\n",
      "Progress: 26.2%\n",
      "Progress: 26.5%\n",
      "Progress: 26.8%\n",
      "Progress: 27.1%\n",
      "Progress: 27.4%\n",
      "Progress: 27.7%\n",
      "Progress: 28.0%\n",
      "Progress: 28.3%\n",
      "Progress: 28.6%\n",
      "Progress: 28.9%\n",
      "Progress: 29.2%\n",
      "Progress: 29.5%\n",
      "Progress: 29.8%\n",
      "Progress: 30.2%\n",
      "Progress: 30.5%\n",
      "Progress: 30.8%\n",
      "Progress: 31.1%\n",
      "Progress: 31.4%\n",
      "Progress: 31.7%\n",
      "Progress: 32.0%\n",
      "Progress: 32.3%\n",
      "Progress: 32.6%\n",
      "Progress: 32.9%\n",
      "Progress: 33.2%\n",
      "Progress: 33.5%\n",
      "Progress: 33.8%\n",
      "Progress: 34.1%\n",
      "Progress: 34.4%\n",
      "Progress: 34.7%\n",
      "Progress: 35.0%\n",
      "Progress: 35.3%\n",
      "Progress: 35.6%\n",
      "Progress: 35.9%\n",
      "Progress: 36.2%\n",
      "Progress: 36.6%\n",
      "Progress: 36.9%\n",
      "Progress: 37.2%\n",
      "Progress: 37.5%\n",
      "Progress: 37.8%\n",
      "Progress: 38.1%\n",
      "Progress: 38.4%\n",
      "Progress: 38.7%\n",
      "Progress: 39.0%\n",
      "Progress: 39.3%\n",
      "Progress: 39.6%\n",
      "Progress: 39.9%\n",
      "Progress: 40.2%\n",
      "Progress: 40.5%\n",
      "Progress: 40.8%\n",
      "Progress: 41.1%\n",
      "Progress: 41.4%\n",
      "Progress: 41.7%\n",
      "Progress: 42.0%\n",
      "Progress: 42.3%\n",
      "Progress: 42.6%\n",
      "Progress: 42.9%\n",
      "Progress: 43.3%\n",
      "Progress: 43.6%\n",
      "Progress: 43.9%\n",
      "Progress: 44.2%\n",
      "Progress: 44.5%\n",
      "Progress: 44.8%\n",
      "Progress: 45.1%\n",
      "Progress: 45.4%\n",
      "Progress: 45.7%\n",
      "Progress: 46.0%\n",
      "Progress: 46.3%\n",
      "Progress: 46.6%\n",
      "Progress: 46.9%\n",
      "Progress: 47.2%\n",
      "Progress: 47.5%\n",
      "Progress: 47.8%\n",
      "Progress: 48.1%\n",
      "Progress: 48.4%\n",
      "Progress: 48.7%\n",
      "Progress: 49.0%\n",
      "Progress: 49.3%\n",
      "Progress: 49.6%\n",
      "Progress: 50.0%\n",
      "Progress: 50.3%\n",
      "Progress: 50.6%\n",
      "Progress: 50.9%\n",
      "Progress: 51.2%\n",
      "Progress: 51.5%\n",
      "Progress: 51.8%\n",
      "Progress: 52.1%\n",
      "Progress: 52.4%\n",
      "Progress: 52.7%\n",
      "Progress: 53.0%\n",
      "Progress: 53.3%\n",
      "Progress: 53.6%\n",
      "Progress: 53.9%\n",
      "Progress: 54.2%\n",
      "Progress: 54.5%\n",
      "Progress: 54.8%\n",
      "Progress: 55.1%\n",
      "Progress: 55.4%\n",
      "Progress: 55.7%\n",
      "Progress: 56.0%\n",
      "Progress: 56.3%\n",
      "Progress: 56.7%\n",
      "Progress: 57.0%\n",
      "Progress: 57.3%\n",
      "Progress: 57.6%\n",
      "Progress: 57.9%\n",
      "Progress: 58.2%\n",
      "Progress: 58.5%\n",
      "Progress: 58.8%\n",
      "Progress: 59.1%\n",
      "Progress: 59.4%\n",
      "Progress: 59.7%\n",
      "Progress: 60.0%\n",
      "Progress: 60.3%\n",
      "Progress: 60.6%\n",
      "Progress: 60.9%\n",
      "Progress: 61.2%\n",
      "Progress: 61.5%\n",
      "Progress: 61.8%\n",
      "Progress: 62.1%\n",
      "Progress: 62.4%\n",
      "Progress: 62.7%\n",
      "Progress: 63.0%\n",
      "Progress: 63.4%\n",
      "Progress: 63.7%\n",
      "Progress: 64.0%\n",
      "Progress: 64.3%\n",
      "Progress: 64.6%\n",
      "Progress: 64.9%\n",
      "Progress: 65.2%\n",
      "Progress: 65.5%\n",
      "Progress: 65.8%\n",
      "Progress: 66.1%\n",
      "Progress: 66.4%\n",
      "Progress: 66.7%\n",
      "Progress: 67.0%\n",
      "Progress: 67.3%\n",
      "Progress: 67.6%\n",
      "Progress: 67.9%\n",
      "Progress: 68.2%\n",
      "Progress: 68.5%\n",
      "Progress: 68.8%\n",
      "Progress: 69.1%\n",
      "Progress: 69.4%\n",
      "Progress: 69.8%\n",
      "Progress: 70.1%\n",
      "Progress: 70.4%\n",
      "Progress: 70.7%\n",
      "Progress: 71.0%\n",
      "Progress: 71.3%\n",
      "Progress: 71.6%\n",
      "Progress: 71.9%\n",
      "Progress: 72.2%\n",
      "Progress: 72.5%\n",
      "Progress: 72.8%\n",
      "Progress: 73.1%\n",
      "Progress: 73.4%\n",
      "Progress: 73.7%\n",
      "Progress: 74.0%\n",
      "Progress: 74.3%\n",
      "Progress: 74.6%\n",
      "Progress: 74.9%\n",
      "Progress: 75.2%\n",
      "Progress: 75.5%\n",
      "Progress: 75.8%\n",
      "Progress: 76.1%\n",
      "Progress: 76.5%\n",
      "Progress: 76.8%\n",
      "Progress: 77.1%\n",
      "Progress: 77.4%\n",
      "Progress: 77.7%\n",
      "Progress: 78.0%\n",
      "Progress: 78.3%\n",
      "Progress: 78.6%\n",
      "Progress: 78.9%\n",
      "Progress: 79.2%\n",
      "Progress: 79.5%\n",
      "Progress: 79.8%\n",
      "Progress: 80.1%\n",
      "Progress: 80.4%\n",
      "Progress: 80.7%\n",
      "Progress: 81.0%\n",
      "Progress: 81.3%\n",
      "Progress: 81.6%\n",
      "Progress: 81.9%\n",
      "Progress: 82.2%\n",
      "Progress: 82.5%\n",
      "Progress: 82.8%\n",
      "Progress: 83.2%\n",
      "Progress: 83.5%\n",
      "Progress: 83.8%\n",
      "Progress: 84.1%\n",
      "Progress: 84.4%\n",
      "Progress: 84.7%\n",
      "Progress: 85.0%\n",
      "Progress: 85.3%\n",
      "Progress: 85.6%\n",
      "Progress: 85.9%\n",
      "Progress: 86.2%\n",
      "Progress: 86.5%\n",
      "Progress: 86.8%\n",
      "Progress: 87.1%\n",
      "Progress: 87.4%\n",
      "Progress: 87.7%\n",
      "Progress: 88.0%\n",
      "Progress: 88.3%\n",
      "Progress: 88.6%\n",
      "Progress: 88.9%\n",
      "Progress: 89.2%\n",
      "Progress: 89.5%\n",
      "Progress: 89.9%\n",
      "Progress: 90.2%\n",
      "Progress: 90.5%\n",
      "Progress: 90.8%\n",
      "Progress: 91.1%\n",
      "Progress: 91.4%\n",
      "Progress: 91.7%\n",
      "Progress: 92.0%\n",
      "Progress: 92.3%\n",
      "Progress: 92.6%\n",
      "Progress: 92.9%\n",
      "Progress: 93.2%\n",
      "Progress: 93.5%\n",
      "Progress: 93.8%\n",
      "Progress: 94.1%\n",
      "Progress: 94.4%\n",
      "Progress: 94.7%\n",
      "Progress: 95.0%\n",
      "Progress: 95.3%\n",
      "Progress: 95.6%\n",
      "Progress: 95.9%\n",
      "Progress: 96.2%\n",
      "Progress: 96.6%\n",
      "Progress: 96.9%\n",
      "Progress: 97.2%\n",
      "Progress: 97.5%\n",
      "Progress: 97.8%\n",
      "Progress: 98.1%\n",
      "Progress: 98.4%\n",
      "Progress: 98.7%\n",
      "Progress: 99.0%\n",
      "Progress: 99.3%\n",
      "Progress: 99.6%\n",
      "Progress: 99.9%\n",
      "🏠 Haushalte im Training (Gas): 254\n",
      "🏠 Haushalte im Test (Gas): 247\n",
      "📉 Verlust an Haushalten: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X_train_electric': (1313250, 90, 24),\n",
       " 'X_test_electric': (328313, 90, 24),\n",
       " 'X_train_gas': (1313250, 90, 24),\n",
       " 'X_test_gas': (328313, 90, 24)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters and columns\n",
    "time_steps = 90\n",
    "\n",
    "# Features and target definition (Strom und Gas)\n",
    "feature_columns = [col for col in df_clean.columns if col not in ['electric_total_consumption_kWh', 'gas_total_consumption_kWh', 'homeid', 'roomid']]\n",
    "target_column_electric = 'electric_total_consumption_kWh'\n",
    "target_column_gas = 'gas_total_consumption_kWh'\n",
    "\n",
    "def create_memmap_array(shape, filename, dtype='float32'):\n",
    "    \"\"\"Create memory-mapped array\"\"\"\n",
    "    path = Path('temp_arrays')\n",
    "    path.mkdir(exist_ok=True)\n",
    "    return np.memmap(path / filename, dtype=dtype, mode='w+', shape=shape)\n",
    "\n",
    "def process_data_efficiently(df_clean, target_column, feature_columns, time_steps, prefix):\n",
    "    \"\"\"Process data with disk-based storage\"\"\"\n",
    "    total_sequences = len(df_clean) - time_steps\n",
    "    n_features = len(feature_columns)\n",
    "    \n",
    "    # Create memory-mapped arrays\n",
    "    X = create_memmap_array((total_sequences, time_steps, n_features), f'{prefix}_X.mmap')\n",
    "    y = create_memmap_array((total_sequences,), f'{prefix}_y.mmap')\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    chunk_size = 500\n",
    "    feature_data = df_clean[feature_columns].values\n",
    "    target_data = df_clean[target_column].values\n",
    "    \n",
    "    print(f\"Processing {prefix} data...\")\n",
    "    for chunk_start in range(0, total_sequences, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_sequences)\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            X[i] = feature_data[i:i + time_steps]\n",
    "            y[i] = target_data[i + time_steps]\n",
    "            \n",
    "        if chunk_start % (chunk_size * 10) == 0:\n",
    "            print(f\"Progress: {chunk_start/total_sequences*100:.1f}%\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Process both electric and gas data\n",
    "time_steps = 90\n",
    "feature_columns = [col for col in df_clean.columns if col not in \n",
    "                  ['electric_total_consumption_kWh', 'gas_total_consumption_kWh', 'homeid', 'roomid']]\n",
    "\n",
    "# Process electric data\n",
    "X_electric, y_electric = process_data_efficiently(\n",
    "    df_clean, \n",
    "    'electric_total_consumption_kWh',\n",
    "    feature_columns,\n",
    "    time_steps,\n",
    "    'electric'\n",
    ")\n",
    "\n",
    "# Process gas data\n",
    "X_gas, y_gas = process_data_efficiently(\n",
    "    df_clean,\n",
    "    'gas_total_consumption_kWh',\n",
    "    feature_columns,\n",
    "    time_steps,\n",
    "    'gas'\n",
    ")\n",
    "\n",
    "# Split datasets\n",
    "X_train_electric, X_test_electric, y_train_electric, y_test_electric = train_test_split(\n",
    "    X_electric, y_electric, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "X_train_gas, X_test_gas, y_train_gas, y_test_gas = train_test_split(\n",
    "    X_gas, y_gas, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Prüfen, wie viele Haushalte im Trainings- und Testset enthalten sind\n",
    "train_households_gas = df_clean.loc[df_clean.index[-len(y_train_gas):], 'homeid'].nunique()\n",
    "test_households_gas = df_clean.loc[df_clean.index[-len(y_test_gas):], 'homeid'].nunique()\n",
    "\n",
    "print(f\" Haushalte im Training (Gas): {train_households_gas}\")\n",
    "print(f\" Haushalte im Test (Gas): {test_households_gas}\")\n",
    "print(f\" Verlust an Haushalten: {254 - test_households_gas}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show shapes of the data\n",
    "train_test_summary = {\n",
    "    'X_train_electric': X_train_electric.shape,\n",
    "    'X_test_electric': X_test_electric.shape,\n",
    "    'X_train_gas': X_train_gas.shape,\n",
    "    'X_test_gas': X_test_gas.shape,\n",
    "}\n",
    "train_test_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E5KwcVmgqyx"
   },
   "source": [
    "**4. LSTM Modell erstellen**\n",
    "1. Daten in das LSTM Format bringen (X_train, y_train)\n",
    "2. LSTM schichten definieren (Tensorflow)\n",
    "3. Modell kompilieren und trainieren\n",
    "4. Hyperparameter-Tuning (z.B Anzahl Neuronen, Learning Rate,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Add dropout parameter to LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0  # Only apply dropout with multiple layers\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# Check and configure MPS device for M2 Mac\n",
    "has_mps = torch.backends.mps.is_available()\n",
    "if has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not found, using CPU\")\n",
    "\n",
    "# Improved data preparation with explicit float32\n",
    "def prepare_data_for_training(X, y, batch_size=64):\n",
    "    \"\"\"Convert numpy arrays to PyTorch DataLoader with float32 precision\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X).to(torch.float32)\n",
    "    y_tensor = torch.FloatTensor(y).reshape(-1, 1).to(torch.float32)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Early stopping handler\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Improved training function with early stopping\n",
    "def train_model(model, train_loader, model_name, num_epochs=10):\n",
    "    \"\"\"Train model with early stopping and best model saving\"\"\"\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    early_stopping = EarlyStopping(patience=2)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=f'Training {model_name}', position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        batch_pbar = tqdm(train_loader, \n",
    "                         desc=f'Epoch {epoch+1}/{num_epochs}',\n",
    "                         leave=False, \n",
    "                         position=1)\n",
    "        \n",
    "        for inputs, targets in batch_pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_pbar.set_postfix({'loss': f'{epoch_loss:.4f}'})\n",
    "\n",
    "        # Save best model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, os.path.join(checkpoint_dir, f'{model_name}_best.pt'))\n",
    "\n",
    "        # Early stopping check\n",
    "        early_stopping(epoch_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n{model_name} training complete! Best loss: {best_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Helper function to load trained models\n",
    "def load_trained_model(model, model_name):\n",
    "    \"\"\"Load best model from checkpoint\"\"\"\n",
    "    checkpoint_path = os.path.join('checkpoints', f'{model_name}_best.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Training pipeline\n",
    "print(\"Preparing data loaders...\")\n",
    "train_loader_electric = prepare_data_for_training(X_train_electric, y_train_electric)\n",
    "train_loader_gas = prepare_data_for_training(X_train_gas, y_train_gas)\n",
    "\n",
    "# Initialize and train models\n",
    "print(\"\\nTraining Electric Model...\")\n",
    "model_electric = LSTMModel(input_size=len(feature_columns), \n",
    "                          hidden_size=32, \n",
    "                          num_layers=2, \n",
    "                          output_size=1).to(device)\n",
    "model_electric = train_model(model_electric, train_loader_electric, \"electric\")\n",
    "\n",
    "print(\"\\nTraining Gas Model...\")\n",
    "model_gas = LSTMModel(input_size=len(feature_columns), \n",
    "                      hidden_size=32, \n",
    "                      num_layers=2, \n",
    "                      output_size=1).to(device)\n",
    "model_gas = train_model(model_gas, train_loader_gas, \"gas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_78qhv1gqyx"
   },
   "source": [
    "**5. Modell evaluieren & Vorhersagen interpretieren**\n",
    "1. Vorhersagen auf Testdaten durchführen\n",
    "2. Metriken berechnen (RMSE, MAE, R^2)\n",
    "3. XAI mit SHAP oder LIME anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from joblib import load\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Device setup with error handling\n",
    "def get_device():\n",
    "    \"\"\"Set up device with proper error handling\"\"\"\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            print(\"MPS not available, falling back to CPU\")\n",
    "            return torch.device(\"cpu\")\n",
    "    except:\n",
    "        print(\"Error checking MPS, falling back to CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def calculate_metrics(predictions, actuals, model_name=None):\n",
    "    \"\"\"\n",
    "    Calculate metrics with improved MAPE/SMAPE handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Basic metrics\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        # Enhanced MAPE calculation with protection against small values\n",
    "        mask = np.abs(actuals) > np.percentile(np.abs(actuals), 5)  # Werte unter dem 5. Perzentil filtern\n",
    "        if np.any(mask):\n",
    "            mape = np.mean(np.abs((actuals[mask] - predictions[mask]) / \n",
    "                                 np.maximum(actuals[mask], 1e-10))) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "            print(f\"⚠️ Warning: No valid values for MAPE calculation in {model_name}\")\n",
    "        \n",
    "        # SMAPE calculation (more robust against zero/small values)\n",
    "        denominator = (np.abs(actuals) + np.abs(predictions))\n",
    "        mask_smape = denominator >= 1e-10\n",
    "        if np.any(mask_smape):\n",
    "            smape = 100 * np.mean(2 * np.abs(predictions[mask_smape] - actuals[mask_smape]) / \n",
    "                                denominator[mask_smape])\n",
    "        else:\n",
    "            smape = np.nan\n",
    "        \n",
    "        # Baseline comparison\n",
    "        baseline_mse = mean_squared_error(actuals, np.full_like(actuals, np.mean(actuals)))\n",
    "        if mse > baseline_mse and model_name:\n",
    "            print(f\"⚠️ Warning: {model_name} MSE ({mse:.2f}) is higher than baseline MSE ({baseline_mse:.2f})\")\n",
    "        \n",
    "        metrics = {\n",
    "            'RMSE': float(f\"{rmse:.4f}\"),\n",
    "            'MSE': float(f\"{mse:.4f}\"),\n",
    "            'MAE': float(f\"{mae:.4f}\"),\n",
    "            'R2': float(f\"{r2:.4f}\"),\n",
    "            'MAPE': float(f\"{mape:.4f}\") if not np.isnan(mape) else np.nan,\n",
    "            'SMAPE': float(f\"{smape:.4f}\") if not np.isnan(smape) else np.nan\n",
    "        }\n",
    "        \n",
    "        # Print additional insights\n",
    "        if model_name:\n",
    "            print(f\"\\nDetailed metrics for {model_name}:\")\n",
    "            print(f\"- MAPE: {metrics['MAPE']:.2f}% (traditional)\")\n",
    "            print(f\"- SMAPE: {metrics['SMAPE']:.2f}% (symmetric)\")\n",
    "            print(f\"- Values near zero: {(~mask).sum()} of {len(actuals)}\")\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error calculating metrics: {e}\")\n",
    "        return None\n",
    "# Improved inverse scaling with error handling\n",
    "def inverse_scale_predictions(predictions, actuals, scaler):\n",
    "    \"\"\"Inverse transform predictions and actuals with error checking\"\"\"\n",
    "    try:\n",
    "        pred_orig = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "        act_orig = scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "        return pred_orig, act_orig\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inverse scaling: {e}\")\n",
    "        return predictions.flatten(), actuals.flatten()\n",
    "\n",
    "# Enhanced model evaluation with additional visualizations\n",
    "def evaluate_model(model, model_name, test_loader, scaler):\n",
    "    \"\"\"Evaluate model with enhanced metrics and visualizations\"\"\"\n",
    "    print(f\"\\nEvaluating {model_name} model...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    # Prediction loop with error handling\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            for inputs, targets in tqdm(test_loader, desc=\"Making predictions\"):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "                actuals.append(targets.cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    # Process results\n",
    "    predictions = np.concatenate(predictions)\n",
    "    actuals = np.concatenate(actuals)\n",
    "    \n",
    "    # Inverse scaling\n",
    "    predictions, actuals = inverse_scale_predictions(predictions, actuals, scaler)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(predictions, actuals)\n",
    "    \n",
    "    # Create formatted metrics DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': list(metrics.keys()),\n",
    "        'Value': [f\"{v:.4f}\" for v in metrics.values()]\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMetrics for {model_name} Energy Consumption:\")\n",
    "    print(tabulate(metrics_df, headers='keys', tablefmt='pipe', showindex=False))\n",
    "    \n",
    "    return predictions, actuals, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_per_household(predictions, actuals, household_ids, timestamps, \n",
    "                                 model_name, window_size=24, max_houses=5, \n",
    "                                 show_plots=False, save_plots=True):\n",
    "    \"\"\"\n",
    "    Plot predictions vs actual values for individual households with enhanced visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : array-like\n",
    "        Model predictions\n",
    "    actuals : array-like\n",
    "        Actual values\n",
    "    household_ids : array-like\n",
    "        Array of household IDs\n",
    "    timestamps : array-like\n",
    "        Array of timestamps\n",
    "    model_name : str\n",
    "        Name of the model for plot titles\n",
    "    window_size : int, default=24\n",
    "        Window size for moving average smoothing\n",
    "    max_houses : int, default=5\n",
    "        Maximum number of houses to plot\n",
    "    show_plots : bool, default=False\n",
    "        Whether to display plots in notebook\n",
    "    save_plots : bool, default=True\n",
    "        Whether to save plots to disk\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not all(len(arr) > 0 for arr in [predictions, actuals, household_ids, timestamps]):\n",
    "            raise ValueError(\"Empty input arrays detected\")\n",
    "            \n",
    "        # Convert inputs to numpy arrays\n",
    "        min_len = min(len(predictions), len(actuals), len(household_ids), len(timestamps))\n",
    "        predictions = np.array(predictions[:min_len])\n",
    "        actuals = np.array(actuals[:min_len])\n",
    "        household_ids = np.array(household_ids[:min_len])\n",
    "        timestamps = np.array(timestamps[:min_len])\n",
    "        \n",
    "        # Get unique households\n",
    "        unique_homes = np.unique(household_ids)\n",
    "        print(f\" Found {len(unique_homes)} unique households\")\n",
    "        \n",
    "        if save_plots:\n",
    "            plot_dir = Path('../reports/figures')\n",
    "            plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Set style and color scheme\n",
    "        plt.style.use('ggplot')\n",
    "        colors = {\n",
    "            'actual': '#2ecc71',\n",
    "            'predicted': '#e74c3c',\n",
    "            'error': '#3498db',\n",
    "            'baseline': '#95a5a6'\n",
    "        }\n",
    "        \n",
    "        # Track skipped households\n",
    "        skipped_homes = []\n",
    "        \n",
    "        for home in unique_homes[:max_houses]:\n",
    "            indices = np.where(household_ids == home)[0]\n",
    "            \n",
    "            if len(indices) < window_size:\n",
    "                skipped_homes.append(home)\n",
    "                continue\n",
    "                \n",
    "            # Create figure with enhanced layout\n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), \n",
    "                                         height_ratios=[3, 1], \n",
    "                                         gridspec_kw={'hspace': 0.3})\n",
    "            \n",
    "            # Data preparation\n",
    "            time_series = pd.to_datetime(timestamps[indices])\n",
    "            actual_series = pd.Series(actuals[indices], index=time_series)\n",
    "            pred_series = pd.Series(predictions[indices], index=time_series)\n",
    "            \n",
    "            # Smoothing with error handling\n",
    "            actual_smooth = actual_series.rolling(window=window_size, \n",
    "                                               center=True, \n",
    "                                               min_periods=1).mean()\n",
    "            pred_smooth = pred_series.rolling(window=window_size, \n",
    "                                           center=True, \n",
    "                                           min_periods=1).mean()\n",
    "            \n",
    "            # Main consumption plot\n",
    "            ax1.plot(actual_smooth, label=\"Actual\", color=colors['actual'], linewidth=2)\n",
    "            ax1.plot(pred_smooth, label=\"Predicted\", color=colors['predicted'], linewidth=2)\n",
    "            ax1.set_title(f\"{model_name} Consumption Predictions - Household {home}\", \n",
    "                         fontsize=14, pad=20)\n",
    "            ax1.set_ylabel(\"Consumption (kWh)\", fontsize=12)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.legend(fontsize=10, loc='upper right')\n",
    "            \n",
    "            # Enhanced error plot\n",
    "            error = pred_series - actual_series\n",
    "            ax2.plot(time_series, error, color=colors['error'], \n",
    "                    alpha=0.6, label='Prediction Error')\n",
    "            ax2.axhline(y=0, color=colors['baseline'], \n",
    "                       linestyle='--', alpha=0.5)\n",
    "            \n",
    "            # Dynamic y-axis limits with padding\n",
    "            error_margin = (error.max() - error.min()) * 0.1\n",
    "            ax2.set_ylim(error.min() - error_margin, \n",
    "                        error.max() + error_margin)\n",
    "            \n",
    "            ax2.set_xlabel(\"Time\", fontsize=12)\n",
    "            ax2.set_ylabel(\"Error (kWh)\", fontsize=12)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.legend(fontsize=10)\n",
    "            \n",
    "            # Consistent x-axis formatting\n",
    "            for ax in [ax1, ax2]:\n",
    "                ax.tick_params(axis='both', labelsize=10)\n",
    "                plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_plots:\n",
    "                save_path = plot_dir / f'{model_name.lower()}_household_{home}_predictions.png'\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Saved plot for household {home}\")\n",
    "            \n",
    "            if show_plots:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "        \n",
    "        if skipped_homes:\n",
    "            print(f\"\\n Skipped {len(skipped_homes)} households due to insufficient data:\")\n",
    "            print(f\"   {skipped_homes}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Error during plotting: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def prepare_evaluation_data():\n",
    "    \"\"\"Prepare data and models for evaluation\"\"\"\n",
    "    try:\n",
    "        # Load scalers\n",
    "        scalers = {\n",
    "            'Electric': load('scalers/scaler_electric.pkl'),\n",
    "            'Gas': load('scalers/scaler_gas.pkl')\n",
    "        }\n",
    "        \n",
    "        # Initialize models\n",
    "        models = {\n",
    "            'Electric': LSTMModel(input_size=len(feature_columns), \n",
    "                                hidden_size=32, \n",
    "                                num_layers=2, \n",
    "                                output_size=1).to(device),\n",
    "            'Gas': LSTMModel(input_size=len(feature_columns), \n",
    "                           hidden_size=32, \n",
    "                           num_layers=2, \n",
    "                           output_size=1).to(device)\n",
    "        }\n",
    "        \n",
    "        # Prepare test dataloaders\n",
    "        test_data = {\n",
    "            'Electric': (X_test_electric, y_test_electric),\n",
    "            'Gas': (X_test_gas, y_test_gas)\n",
    "        }\n",
    "        \n",
    "        test_loaders = {\n",
    "            name: torch.utils.data.DataLoader(\n",
    "                torch.utils.data.TensorDataset(\n",
    "                    torch.FloatTensor(X_test),\n",
    "                    torch.FloatTensor(y_test)\n",
    "                ),\n",
    "                batch_size=16,\n",
    "                shuffle=False\n",
    "            ) for name, (X_test, y_test) in test_data.items()\n",
    "        }\n",
    "        \n",
    "        # Extract household IDs and timestamps\n",
    "        metadata = {\n",
    "            name: {\n",
    "                'household_ids': df_clean.loc[df_clean.index[-len(y_test):], 'homeid'].values[:len(y_test)],\n",
    "                'timestamps': df_clean.index[-len(y_test):].values[:len(y_test)]\n",
    "            } for name, (_, y_test) in test_data.items()\n",
    "        }\n",
    "        \n",
    "        return models, scalers, test_loaders, metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error preparing evaluation data: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_model_evaluation():\n",
    "    \"\"\"Run evaluation for both LSTM models with enhanced error handling\"\"\"\n",
    "    print(\" Preparing evaluation data...\")\n",
    "    evaluation_data = prepare_evaluation_data()\n",
    "    if evaluation_data is None:\n",
    "        return\n",
    "    \n",
    "    models, scalers, test_loaders, metadata = evaluation_data\n",
    "    results = {}\n",
    "    \n",
    "    # Create reports directory\n",
    "    reports_dir = Path('../reports/tables')\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n Evaluating {name} model...\")\n",
    "        try:\n",
    "            model = load_trained_model(model, name.lower())\n",
    "            predictions, actuals, raw_metrics = evaluate_model(\n",
    "                model=model,\n",
    "                model_name=name,\n",
    "                test_loader=test_loaders[name],\n",
    "                scaler=scalers[name]\n",
    "            )\n",
    "            \n",
    "            if predictions is not None:\n",
    "                # Calculate metrics with enhanced error handling\n",
    "                metrics = calculate_metrics(predictions, actuals, name)\n",
    "                if metrics is None:\n",
    "                    continue\n",
    "                    \n",
    "                results[name] = {\n",
    "                    'predictions': predictions,\n",
    "                    'actuals': actuals,\n",
    "                    'metrics': metrics\n",
    "                }\n",
    "                \n",
    "                # Plot predictions if data is valid\n",
    "                if len(predictions) > 0:\n",
    "                    plot_predictions_per_household(\n",
    "                        predictions=predictions,\n",
    "                        actuals=actuals,\n",
    "                        household_ids=metadata[name]['household_ids'],\n",
    "                        timestamps=metadata[name]['timestamps'],\n",
    "                        model_name=name,\n",
    "                        show_plots=True\n",
    "                    )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" Error evaluating {name} model: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save and display results with proper DataFrame construction\n",
    "    if results:\n",
    "        try:\n",
    "            # Convert metrics to proper DataFrame format\n",
    "            final_metrics = pd.DataFrame.from_records(\n",
    "                [results[name]['metrics'] for name in results],\n",
    "                index=results.keys()\n",
    "            )\n",
    "            \n",
    "            # Save metrics\n",
    "            final_metrics.to_csv(reports_dir / 'model_metrics.csv')\n",
    "            \n",
    "            # Display results\n",
    "            print(\"\\n Final Model Comparison:\")\n",
    "            print(tabulate(final_metrics, headers='keys', tablefmt='pipe', showindex=True))\n",
    "            \n",
    "            # Additional analysis for poor performance\n",
    "            for name in results:\n",
    "                metrics = results[name]['metrics']\n",
    "                if metrics['R2'] < 0:\n",
    "                    print(f\"\\n Poor performance detected for {name} model:\")\n",
    "                    print(f\"   R² score: {metrics['R2']:.4f}\")\n",
    "                    print(\"   Consider model retraining or feature engineering\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Error saving/displaying results: {e}\")\n",
    "    else:\n",
    "        print(\"\\n No results to display\")\n",
    "\n",
    "# Run evaluation\n",
    "if __name__ == \"__main__\":\n",
    "   results = run_model_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensure results dictionary is initialized\n",
    "if not 'results' in locals() or results is None:\n",
    "    results = {}\n",
    "\n",
    "# Make predictions for Electric model\n",
    "model_electric = models['Electric']\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_test_electric),\n",
    "        torch.FloatTensor(y_test_electric)\n",
    "    ),\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "model_electric.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_electric(inputs)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        actuals.append(targets.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "actuals = np.concatenate(actuals)\n",
    "\n",
    "# Store results\n",
    "results['Electric'] = {\n",
    "    'predictions': scaler_electric.inverse_transform(predictions.reshape(-1, 1)).flatten(),\n",
    "    'actuals': scaler_electric.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "}\n",
    "\n",
    "# Get test period data\n",
    "test_length = len(results['Electric']['predictions'])\n",
    "test_timestamps = df_clean.index[-test_length:]\n",
    "test_household_ids = df_clean['homeid'].values[-test_length:]\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions_per_household(\n",
    "    predictions=results['Electric']['predictions'],\n",
    "    actuals=results['Electric']['actuals'],\n",
    "    household_ids=test_household_ids,\n",
    "    timestamps=test_timestamps,\n",
    "    model_name=\"Electric\",\n",
    "    window_size=24,\n",
    "    max_houses=3,\n",
    "    show_plots=True,\n",
    "    save_plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from joblib import load\n",
    "# from tqdm.auto import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# from pathlib import Path\n",
    "# from tabulate import tabulate\n",
    "\n",
    "# # 🔹 **Gerät konfigurieren**\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # 🔹 **Funktion für Vorhersagen**\n",
    "# def make_predictions(model, test_loader):\n",
    "#     \"\"\"Generate predictions using the model\"\"\"\n",
    "#     model.eval()\n",
    "#     predictions, actuals = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in tqdm(test_loader, desc=\"Making predictions\"):\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             predictions.append(outputs.cpu().numpy())\n",
    "#             actuals.append(targets.cpu().numpy())\n",
    "#     return np.concatenate(predictions), np.concatenate(actuals)\n",
    "\n",
    "# # 🔹 **Modelle aus Checkpoints laden**\n",
    "# def load_model_weights(model, model_name, epoch):\n",
    "#     \"\"\"Load model weights with proper path handling\"\"\"\n",
    "#     checkpoint_dir = Path('checkpoints')\n",
    "#     checkpoint_path = checkpoint_dir / f'{model_name}_epoch{epoch}.pt'\n",
    "\n",
    "#     if not checkpoint_path.exists():\n",
    "#         raise FileNotFoundError(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "\n",
    "#     print(f'Loading model weights from: {checkpoint_path}')\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # 🔹 **Zielvariablen rückskalieren**\n",
    "# def inverse_scale_predictions(predictions, actuals, scaler):\n",
    "#     \"\"\"Convert scaled predictions back to original units\"\"\"\n",
    "#     pred_orig = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "#     act_orig = scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "#     return pred_orig, act_orig\n",
    "\n",
    "# # 🔹 **Metriken berechnen**\n",
    "# def calculate_metrics(predictions, actuals):\n",
    "#     \"\"\"Calculate metrics\"\"\"\n",
    "#     rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "#     mae = mean_absolute_error(actuals, predictions)\n",
    "#     r2 = r2_score(actuals, predictions)\n",
    "#     return rmse, mae, r2\n",
    "\n",
    "# def plot_predictions_per_household(predictions, actuals, household_ids, timestamps, model_name, window_size=24):\n",
    "#     \"\"\"Plot predictions for individual households\"\"\"\n",
    "#     min_len = min(len(predictions), len(actuals), len(household_ids), len(timestamps))\n",
    "#     predictions = predictions[:min_len]\n",
    "#     actuals = actuals[:min_len]\n",
    "#     household_ids = household_ids[:min_len]\n",
    "#     timestamps = timestamps[:min_len]\n",
    "\n",
    "#     unique_homes = np.unique(household_ids)\n",
    "#     print(f\"Total unique households: {len(unique_homes)}\")\n",
    "\n",
    "#     plot_dir = Path('../reports/figures')\n",
    "#     plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     for home in unique_homes[:5]:  # Plot only the first 5 households\n",
    "#         indices = np.where(household_ids == home)[0]\n",
    "#         if len(indices) == 0:\n",
    "#             continue\n",
    "\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         time_series = pd.to_datetime(timestamps[indices])\n",
    "        \n",
    "#         # Check if timestamps are unique\n",
    "#         if len(set(time_series)) != len(time_series):\n",
    "#             print(f\"Warning: Duplicate timestamps found for household {home}\")\n",
    "\n",
    "#         actual_smooth = pd.Series(actuals[indices], index=time_series).rolling(window=window_size).mean()\n",
    "#         pred_smooth = pd.Series(predictions[indices], index=time_series).rolling(window=window_size).mean()\n",
    "\n",
    "#         plt.plot(actual_smooth, label=\"Actual (Smoothed)\", color='blue', alpha=0.6)\n",
    "#         plt.plot(pred_smooth, label=\"Predicted (Smoothed)\", color='red', alpha=0.6)\n",
    "#         plt.title(f\"{model_name} Predictions for Household {home}\")\n",
    "#         plt.xlabel(\"Time\")\n",
    "#         plt.ylabel(\"Consumption (Wh)\")\n",
    "#         plt.legend()\n",
    "#         plt.grid(True, alpha=0.3)\n",
    "#         plt.xticks(rotation=45)\n",
    "\n",
    "#         save_path = plot_dir / f'{model_name.lower()}_household_{home}_predictions.png'\n",
    "#         plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "\n",
    "# # 🔹 **Modell-Evaluation**\n",
    "# def evaluate_model(model, model_name, test_loader, scaler, household_ids, timestamps):\n",
    "#     \"\"\"Evaluate model performance\"\"\"\n",
    "#     print(f\"\\nEvaluating {model_name} model...\")\n",
    "\n",
    "#     scaled_pred, scaled_act = make_predictions(model, test_loader)\n",
    "#     predictions, actuals = inverse_scale_predictions(scaled_pred, scaled_act, scaler)\n",
    "#     rmse, mae, r2 = calculate_metrics(predictions, actuals)\n",
    "\n",
    "#     metrics_df = pd.DataFrame({\n",
    "#         'Metric': ['RMSE', 'MAE', 'R²'],\n",
    "#         'Value': [f\"{rmse:.2f} Wh\", f\"{mae:.2f} Wh\", f\"{r2:.4f}\"]\n",
    "#     })\n",
    "\n",
    "#     print(f\"\\nMetrics for {model_name} Energy Consumption:\")\n",
    "#     print(tabulate(metrics_df, headers='keys', tablefmt='pipe', showindex=False))\n",
    "\n",
    "#     plot_predictions_per_household(predictions, actuals, household_ids, timestamps, model_name)\n",
    "\n",
    "#     return predictions, actuals, {'model': model_name, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "# # 🔹 **Scaler laden & validieren**\n",
    "# def load_and_verify_scalers():\n",
    "#     \"\"\"Load scalers and verify their existence\"\"\"\n",
    "#     try:\n",
    "#         scaler_electric = load('scalers/scaler_electric.pkl')\n",
    "#         scaler_gas = load('scalers/scaler_gas.pkl')\n",
    "#         print(\"Successfully loaded scalers\")\n",
    "\n",
    "#         return scaler_electric, scaler_gas\n",
    "#     except FileNotFoundError:\n",
    "#         raise FileNotFoundError(\"Scalers not found. Please run preprocessing first.\")\n",
    "#     except Exception as e:\n",
    "#         raise Exception(f\"Error loading scalers: {e}\")\n",
    "\n",
    "# # Scaler laden\n",
    "# scaler_electric, scaler_gas = load_and_verify_scalers()\n",
    "\n",
    "# # 🔹 **Modelle initialisieren**\n",
    "# input_size = len(feature_columns)\n",
    "# print(f\"Model input size: {input_size}\")\n",
    "\n",
    "# models = {\n",
    "#     'Electric': LSTMModel(input_size=input_size, hidden_size=32, num_layers=2, output_size=1).to(device),\n",
    "#     'Gas': LSTMModel(input_size=input_size, hidden_size=32, num_layers=2, output_size=1).to(device)\n",
    "# }\n",
    "\n",
    "# scalers = {\n",
    "#     'Electric': scaler_electric,\n",
    "#     'Gas': scaler_gas\n",
    "# }\n",
    "\n",
    "# # Haushalts-IDs & Timestamps extrahieren\n",
    "# household_ids = {\n",
    "#     'Electric': df_clean.loc[df_clean.index[-len(y_test_electric):], 'homeid'].values[:len(y_test_electric)],\n",
    "#     'Gas': df_clean.loc[df_clean.index[-len(y_test_gas):], 'homeid'].values[:len(y_test_gas)]\n",
    "# }\n",
    "\n",
    "# timestamps = {\n",
    "#     'Electric': df_clean.index[-len(y_test_electric):].values[:len(y_test_electric)],\n",
    "#     'Gas': df_clean.index[-len(y_test_gas):].values[:len(y_test_gas)]\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 🔹 **Modelle evaluieren**\n",
    "# all_metrics = []\n",
    "# for name, model in models.items():\n",
    "#     model = load_model_weights(model, name.lower(), epoch=6)\n",
    "#     predictions, actuals, metrics = evaluate_model(\n",
    "#         model, name, test_loaders[name],\n",
    "#         scalers[name], household_ids[name], timestamps[name]\n",
    "#     )\n",
    "#     results[name] = {'predictions': predictions, 'actuals': actuals, 'metrics': metrics}\n",
    "#     all_metrics.append(metrics)\n",
    "\n",
    "# # 🔹 **Metriken speichern**\n",
    "# final_metrics = pd.DataFrame(all_metrics)\n",
    "# metrics_dir = Path('../reports/tables')\n",
    "# metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "# final_metrics.to_csv(metrics_dir / 'model_metrics.csv', index=False)\n",
    "\n",
    "# print(\"\\nFinal Model Comparison:\")\n",
    "# print(tabulate(final_metrics, headers='keys', tablefmt='pipe', showindex=False))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Predictive_Analytics_Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
