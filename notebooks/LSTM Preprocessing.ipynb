{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sudo-Oliver/Predictive-Analytics-Private/blob/main/notebooks/LSTM%20Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7R7RQPWgqys"
   },
   "source": [
    "**1. Daten laden und vorbereiten**\n",
    "1. Laden der Daten in einen Dataframe\n",
    "2. Zeitspalte umwandeln (Unix-Timestamp -> Datetime)\n",
    "3. nach homeid gruppieren (jeder Haushalt hat seine eigene Zeitreihe)\n",
    "4. Sortieren nach Zeit innerhalb des Haushalts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPGwAXsugqyt",
    "outputId": "3c20df7b-85f1-4803-d5cc-6b5abf07b366"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import gdown\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Metal plugin available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Metal GPU will be used\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Metal plugin available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Configure memory growth\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(\"Metal GPU will be used\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4oD_P2DPgqyv"
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean and preprocess sensor data\"\"\"\n",
    "    # Convert Unix timestamp to datetime\n",
    "    df['timestamp_local'] = pd.to_datetime(df['timestamp_local'], unit='ms')\n",
    "\n",
    "    # Set timestamp_local as index\n",
    "    df.set_index('timestamp_local', inplace=True)\n",
    "\n",
    "    # Sort by homeid and timestamp_local\n",
    "    df = df.sort_values(by=['homeid', 'timestamp_local'])\n",
    "\n",
    "    # Remove specified columns\n",
    "    columns_to_drop = [\n",
    "        'sensorid', 'median_temperature', '_room',\n",
    "        'sensorid_room', 'measured_entity',\n",
    "        'sensorid_electric', 'sensorid_gas'\n",
    "    ]\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "bvXa2w8Ggqyv",
    "outputId": "d8654319-74fb-40c1-c690-abf5f0c0556f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully: (1641653, 23) rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homeid</th>\n",
       "      <th>electric_min_consumption</th>\n",
       "      <th>electric_max_consumption</th>\n",
       "      <th>std_consumption</th>\n",
       "      <th>electric_median_consumption</th>\n",
       "      <th>electric_total_consumption_Wh</th>\n",
       "      <th>gas_mean_consumption</th>\n",
       "      <th>gas_min_consumption</th>\n",
       "      <th>gas_max_consumption</th>\n",
       "      <th>gas_median_consumption</th>\n",
       "      <th>gas_total_consumption_Wh</th>\n",
       "      <th>median_value</th>\n",
       "      <th>roomid</th>\n",
       "      <th>income_band_mid</th>\n",
       "      <th>education_map</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp_local</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-09-20 09:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.033905</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.179807</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.224</td>\n",
       "      <td>20.720</td>\n",
       "      <td>652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 10:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068875</td>\n",
       "      <td>0.458375</td>\n",
       "      <td>0.035875</td>\n",
       "      <td>0.187625</td>\n",
       "      <td>0.176690</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.210</td>\n",
       "      <td>20.695</td>\n",
       "      <td>652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 11:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.581750</td>\n",
       "      <td>0.037846</td>\n",
       "      <td>0.181250</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.196</td>\n",
       "      <td>20.670</td>\n",
       "      <td>652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 12:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068625</td>\n",
       "      <td>0.705125</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.174875</td>\n",
       "      <td>0.170457</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.182</td>\n",
       "      <td>20.645</td>\n",
       "      <td>652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20 13:00:00</th>\n",
       "      <td>47</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.828500</td>\n",
       "      <td>0.041788</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.167340</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.168</td>\n",
       "      <td>20.620</td>\n",
       "      <td>652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     homeid  electric_min_consumption  \\\n",
       "timestamp_local                                         \n",
       "2016-09-20 09:00:00      47                  0.069000   \n",
       "2016-09-20 10:00:00      47                  0.068875   \n",
       "2016-09-20 11:00:00      47                  0.068750   \n",
       "2016-09-20 12:00:00      47                  0.068625   \n",
       "2016-09-20 13:00:00      47                  0.068500   \n",
       "\n",
       "                     electric_max_consumption  std_consumption  \\\n",
       "timestamp_local                                                  \n",
       "2016-09-20 09:00:00                  0.335000         0.033905   \n",
       "2016-09-20 10:00:00                  0.458375         0.035875   \n",
       "2016-09-20 11:00:00                  0.581750         0.037846   \n",
       "2016-09-20 12:00:00                  0.705125         0.039817   \n",
       "2016-09-20 13:00:00                  0.828500         0.041788   \n",
       "\n",
       "                     electric_median_consumption  \\\n",
       "timestamp_local                                    \n",
       "2016-09-20 09:00:00                     0.194000   \n",
       "2016-09-20 10:00:00                     0.187625   \n",
       "2016-09-20 11:00:00                     0.181250   \n",
       "2016-09-20 12:00:00                     0.174875   \n",
       "2016-09-20 13:00:00                     0.168500   \n",
       "\n",
       "                     electric_total_consumption_Wh  gas_mean_consumption  \\\n",
       "timestamp_local                                                            \n",
       "2016-09-20 09:00:00                       0.179807                 0.112   \n",
       "2016-09-20 10:00:00                       0.176690                 0.112   \n",
       "2016-09-20 11:00:00                       0.173574                 0.112   \n",
       "2016-09-20 12:00:00                       0.170457                 0.112   \n",
       "2016-09-20 13:00:00                       0.167340                 0.112   \n",
       "\n",
       "                     gas_min_consumption  gas_max_consumption  \\\n",
       "timestamp_local                                                 \n",
       "2016-09-20 09:00:00                0.112                0.112   \n",
       "2016-09-20 10:00:00                0.112                0.112   \n",
       "2016-09-20 11:00:00                0.112                0.112   \n",
       "2016-09-20 12:00:00                0.112                0.112   \n",
       "2016-09-20 13:00:00                0.112                0.112   \n",
       "\n",
       "                     gas_median_consumption  gas_total_consumption_Wh  \\\n",
       "timestamp_local                                                         \n",
       "2016-09-20 09:00:00                   0.112                     0.224   \n",
       "2016-09-20 10:00:00                   0.112                     0.210   \n",
       "2016-09-20 11:00:00                   0.112                     0.196   \n",
       "2016-09-20 12:00:00                   0.112                     0.182   \n",
       "2016-09-20 13:00:00                   0.112                     0.168   \n",
       "\n",
       "                     median_value  roomid  income_band_mid  education_map  \n",
       "timestamp_local                                                            \n",
       "2016-09-20 09:00:00        20.720   652.0              0.0            8.0  \n",
       "2016-09-20 10:00:00        20.695   652.0              0.0            8.0  \n",
       "2016-09-20 11:00:00        20.670   652.0              0.0            8.0  \n",
       "2016-09-20 12:00:00        20.645   652.0              0.0            8.0  \n",
       "2016-09-20 13:00:00        20.620   652.0              0.0            8.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_processed_data():\n",
    "    \"\"\"Load preprocessed sensor data with fallback to Drive download\"\"\"\n",
    "    file_id = \"1KHQCVfwTxm5bjjITS8WMm9P3M12ETVsR\"\n",
    "\n",
    "    download_path = Path('data/processed')\n",
    "    download_path.mkdir(parents=True, exist_ok=True)\n",
    "    file_path = download_path / 'final_processed_data3.parquet'\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(\"Downloading from Google Drive...\")\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(url, str(file_path), quiet=False)\n",
    "\n",
    "    if file_path.exists():\n",
    "        df = pd.read_parquet(file_path)\n",
    "        print(f\"Data loaded successfully: {df.shape} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not load or download data file\")\n",
    "\n",
    "# Load and clean data\n",
    "df = load_processed_data()\n",
    "df_clean = clean_data(df.copy())\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrRCKc5wgqyv"
   },
   "source": [
    "**2. Feature Engineering & Datenbereinigung**\n",
    "1. Zyklische Transformation für Zeitdaten (hour_sin, hour_cos für Stunden)\n",
    "2. Lag-Features erstellen (für vorherige Strom und Gaswerte)\n",
    "3. Rolling-Average-Features (z.B gleitender Mittelwert über 3 oder 7 Zeitschritte)\n",
    "4. Daten normalisieren (Min-Max-Scaling für LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "PSojNn9jgqyw",
    "outputId": "1a694ec4-2460-4986-e79e-4b6beda59b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Feature Correlation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>electric_total_consumption_Wh</th>\n",
       "      <th>gas_total_consumption_Wh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>electric_total_consumption_Wh</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_median_consumption</th>\n",
       "      <td>0.803903</td>\n",
       "      <td>0.025043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_consumption</th>\n",
       "      <td>0.775032</td>\n",
       "      <td>0.024055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_max_consumption</th>\n",
       "      <td>0.693017</td>\n",
       "      <td>0.042803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electric_min_consumption</th>\n",
       "      <td>0.516674</td>\n",
       "      <td>0.039677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income_band_mid</th>\n",
       "      <td>0.154421</td>\n",
       "      <td>0.034544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_value</th>\n",
       "      <td>0.066262</td>\n",
       "      <td>0.003073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_map</th>\n",
       "      <td>0.054028</td>\n",
       "      <td>0.012451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_total_consumption_Wh</th>\n",
       "      <td>0.032214</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_max_consumption</th>\n",
       "      <td>0.028237</td>\n",
       "      <td>0.999275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_mean_consumption</th>\n",
       "      <td>0.025714</td>\n",
       "      <td>0.959300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roomid</th>\n",
       "      <td>0.025531</td>\n",
       "      <td>0.033835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_median_consumption</th>\n",
       "      <td>0.024416</td>\n",
       "      <td>0.940515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gas_min_consumption</th>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.886744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homeid</th>\n",
       "      <td>0.023286</td>\n",
       "      <td>0.035260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               electric_total_consumption_Wh  \\\n",
       "electric_total_consumption_Wh                       1.000000   \n",
       "electric_median_consumption                         0.803903   \n",
       "std_consumption                                     0.775032   \n",
       "electric_max_consumption                            0.693017   \n",
       "electric_min_consumption                            0.516674   \n",
       "income_band_mid                                     0.154421   \n",
       "median_value                                        0.066262   \n",
       "education_map                                       0.054028   \n",
       "gas_total_consumption_Wh                            0.032214   \n",
       "gas_max_consumption                                 0.028237   \n",
       "gas_mean_consumption                                0.025714   \n",
       "roomid                                              0.025531   \n",
       "gas_median_consumption                              0.024416   \n",
       "gas_min_consumption                                 0.023978   \n",
       "homeid                                              0.023286   \n",
       "\n",
       "                               gas_total_consumption_Wh  \n",
       "electric_total_consumption_Wh                  0.032214  \n",
       "electric_median_consumption                    0.025043  \n",
       "std_consumption                                0.024055  \n",
       "electric_max_consumption                       0.042803  \n",
       "electric_min_consumption                       0.039677  \n",
       "income_band_mid                                0.034544  \n",
       "median_value                                   0.003073  \n",
       "education_map                                  0.012451  \n",
       "gas_total_consumption_Wh                       1.000000  \n",
       "gas_max_consumption                            0.999275  \n",
       "gas_mean_consumption                           0.959300  \n",
       "roomid                                         0.033835  \n",
       "gas_median_consumption                         0.940515  \n",
       "gas_min_consumption                            0.886744  \n",
       "homeid                                         0.035260  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vollständige Korrelation mit allen spalten berechnen\n",
    "correlation_matrix_all = df_clean.corr()\n",
    "\n",
    "# Korrelation der Features mit den Zielvariablen (Strom und Gasverbtauch)\n",
    "correlation_target_all = correlation_matrix_all[['electric_total_consumption_Wh', 'gas_total_consumption_Wh']]\n",
    "\n",
    "# Sortieren nach Stärke der Korrelation\n",
    "correlation_target_all_sorted = correlation_target_all.abs().sort_values(by=['electric_total_consumption_Wh', 'gas_total_consumption_Wh'], ascending=False)\n",
    "\n",
    "# Korrelationsergebnisse anzeigen\n",
    "print(\"Full Feature Correlation:\")\n",
    "display(correlation_target_all_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2FWybG4Qgqyw",
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Extract hour from timestamp index\n",
    "df_clean['hour'] = df_clean.index.hour\n",
    "\n",
    "# Create cyclical features\n",
    "df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour']/24)\n",
    "df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour']/24)\n",
    "\n",
    "# Create lag features for electric consumption (t-1, t-2, t-3)\n",
    "for lag in range(1, 4):\n",
    "    df_clean[f'electric_lag_{lag}'] = df_clean.groupby('homeid')['electric_total_consumption_Wh'].shift(lag)\n",
    "    # Create lag features for gas consumption (t-1, t-2, t-3)\n",
    "    df_clean[f'gas_lag_{lag}'] = df_clean.groupby('homeid')['gas_total_consumption_Wh'].shift(lag)\n",
    "\n",
    "# Create rolling means for electric consumption (3 and 7 time steps)\n",
    "df_clean['electric_rolling_mean_3h'] = df_clean.groupby('homeid')['electric_total_consumption_Wh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df_clean['electric_rolling_mean_7h'] = df_clean.groupby('homeid')['electric_total_consumption_Wh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Create rolling means for gas consumption (3 and 7 time steps)\n",
    "df_clean['gas_rolling_mean_3h'] = df_clean.groupby('homeid')['gas_total_consumption_Wh'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df_clean['gas_rolling_mean_7h'] = df_clean.groupby('homeid')['gas_total_consumption_Wh'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Replace deprecated fillna methods with new syntax\n",
    "df_clean = df_clean.ffill()  # Forward fill\n",
    "df_clean = df_clean.bfill()  # Backward fill\n",
    "\n",
    "# Define features to scale\n",
    "scaled_features = ['electric_total_consumption_Wh', 'gas_total_consumption_Wh', 'electric_median_consumption', 'electric_max_consumption', 'electric_min_consumption', 'std_consumption', 'gas_max_consumption', 'gas_min_consumption', 'gas_median_consumption', 'median_value', 'hour_sin', 'hour_cos', 'electric_lag_1', 'electric_lag_2', 'electric_lag_3', 'gas_lag_1', 'gas_lag_2', 'gas_lag_3', 'electric_rolling_mean_3h', 'electric_rolling_mean_7h', 'gas_rolling_mean_3h', 'gas_rolling_mean_7h']\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the selected features\n",
    "df_clean[scaled_features] = scaler.fit_transform(df_clean[scaled_features])\n",
    "\n",
    "#df_clean.to_parquet('lstm_preprocessed_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8LZ_5OCgqyw"
   },
   "source": [
    "**3. Trainings und Testdatensätze erstellen**\n",
    "1. Daten für jeden Haushalt in eine geeignetes Format bringen\n",
    "2. Train-Test-Split: 80% Training 20% Test\n",
    "3. Zeitfenster für LSTM definieren (z.B 24 Stunden zurückblicken um die nächste Stunde vorherzusagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9serancgqyw",
    "outputId": "5bdd6e36-12ae-498d-c95b-b489b718085b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing electric data...\n",
      "Progress: 0.0%\n",
      "Progress: 0.3%\n",
      "Progress: 0.6%\n",
      "Progress: 0.9%\n",
      "Progress: 1.2%\n",
      "Progress: 1.5%\n",
      "Progress: 1.8%\n",
      "Progress: 2.1%\n",
      "Progress: 2.4%\n",
      "Progress: 2.7%\n",
      "Progress: 3.0%\n",
      "Progress: 3.4%\n",
      "Progress: 3.7%\n",
      "Progress: 4.0%\n",
      "Progress: 4.3%\n",
      "Progress: 4.6%\n",
      "Progress: 4.9%\n",
      "Progress: 5.2%\n",
      "Progress: 5.5%\n",
      "Progress: 5.8%\n",
      "Progress: 6.1%\n",
      "Progress: 6.4%\n",
      "Progress: 6.7%\n",
      "Progress: 7.0%\n",
      "Progress: 7.3%\n",
      "Progress: 7.6%\n",
      "Progress: 7.9%\n",
      "Progress: 8.2%\n",
      "Progress: 8.5%\n",
      "Progress: 8.8%\n",
      "Progress: 9.1%\n",
      "Progress: 9.4%\n",
      "Progress: 9.7%\n",
      "Progress: 10.1%\n",
      "Progress: 10.4%\n",
      "Progress: 10.7%\n",
      "Progress: 11.0%\n",
      "Progress: 11.3%\n",
      "Progress: 11.6%\n",
      "Progress: 11.9%\n",
      "Progress: 12.2%\n",
      "Progress: 12.5%\n",
      "Progress: 12.8%\n",
      "Progress: 13.1%\n",
      "Progress: 13.4%\n",
      "Progress: 13.7%\n",
      "Progress: 14.0%\n",
      "Progress: 14.3%\n",
      "Progress: 14.6%\n",
      "Progress: 14.9%\n",
      "Progress: 15.2%\n",
      "Progress: 15.5%\n",
      "Progress: 15.8%\n",
      "Progress: 16.1%\n",
      "Progress: 16.4%\n",
      "Progress: 16.8%\n",
      "Progress: 17.1%\n",
      "Progress: 17.4%\n",
      "Progress: 17.7%\n",
      "Progress: 18.0%\n",
      "Progress: 18.3%\n",
      "Progress: 18.6%\n",
      "Progress: 18.9%\n",
      "Progress: 19.2%\n",
      "Progress: 19.5%\n",
      "Progress: 19.8%\n",
      "Progress: 20.1%\n",
      "Progress: 20.4%\n",
      "Progress: 20.7%\n",
      "Progress: 21.0%\n",
      "Progress: 21.3%\n",
      "Progress: 21.6%\n",
      "Progress: 21.9%\n",
      "Progress: 22.2%\n",
      "Progress: 22.5%\n",
      "Progress: 22.8%\n",
      "Progress: 23.1%\n",
      "Progress: 23.5%\n",
      "Progress: 23.8%\n",
      "Progress: 24.1%\n",
      "Progress: 24.4%\n",
      "Progress: 24.7%\n",
      "Progress: 25.0%\n",
      "Progress: 25.3%\n",
      "Progress: 25.6%\n",
      "Progress: 25.9%\n",
      "Progress: 26.2%\n",
      "Progress: 26.5%\n",
      "Progress: 26.8%\n",
      "Progress: 27.1%\n",
      "Progress: 27.4%\n",
      "Progress: 27.7%\n",
      "Progress: 28.0%\n",
      "Progress: 28.3%\n",
      "Progress: 28.6%\n",
      "Progress: 28.9%\n",
      "Progress: 29.2%\n",
      "Progress: 29.5%\n",
      "Progress: 29.8%\n",
      "Progress: 30.2%\n",
      "Progress: 30.5%\n",
      "Progress: 30.8%\n",
      "Progress: 31.1%\n",
      "Progress: 31.4%\n",
      "Progress: 31.7%\n",
      "Progress: 32.0%\n",
      "Progress: 32.3%\n",
      "Progress: 32.6%\n",
      "Progress: 32.9%\n",
      "Progress: 33.2%\n",
      "Progress: 33.5%\n",
      "Progress: 33.8%\n",
      "Progress: 34.1%\n",
      "Progress: 34.4%\n",
      "Progress: 34.7%\n",
      "Progress: 35.0%\n",
      "Progress: 35.3%\n",
      "Progress: 35.6%\n",
      "Progress: 35.9%\n",
      "Progress: 36.2%\n",
      "Progress: 36.6%\n",
      "Progress: 36.9%\n",
      "Progress: 37.2%\n",
      "Progress: 37.5%\n",
      "Progress: 37.8%\n",
      "Progress: 38.1%\n",
      "Progress: 38.4%\n",
      "Progress: 38.7%\n",
      "Progress: 39.0%\n",
      "Progress: 39.3%\n",
      "Progress: 39.6%\n",
      "Progress: 39.9%\n",
      "Progress: 40.2%\n",
      "Progress: 40.5%\n",
      "Progress: 40.8%\n",
      "Progress: 41.1%\n",
      "Progress: 41.4%\n",
      "Progress: 41.7%\n",
      "Progress: 42.0%\n",
      "Progress: 42.3%\n",
      "Progress: 42.6%\n",
      "Progress: 42.9%\n",
      "Progress: 43.3%\n",
      "Progress: 43.6%\n",
      "Progress: 43.9%\n",
      "Progress: 44.2%\n",
      "Progress: 44.5%\n",
      "Progress: 44.8%\n",
      "Progress: 45.1%\n",
      "Progress: 45.4%\n",
      "Progress: 45.7%\n",
      "Progress: 46.0%\n",
      "Progress: 46.3%\n",
      "Progress: 46.6%\n",
      "Progress: 46.9%\n",
      "Progress: 47.2%\n",
      "Progress: 47.5%\n",
      "Progress: 47.8%\n",
      "Progress: 48.1%\n",
      "Progress: 48.4%\n",
      "Progress: 48.7%\n",
      "Progress: 49.0%\n",
      "Progress: 49.3%\n",
      "Progress: 49.6%\n",
      "Progress: 50.0%\n",
      "Progress: 50.3%\n",
      "Progress: 50.6%\n",
      "Progress: 50.9%\n",
      "Progress: 51.2%\n",
      "Progress: 51.5%\n",
      "Progress: 51.8%\n",
      "Progress: 52.1%\n",
      "Progress: 52.4%\n",
      "Progress: 52.7%\n",
      "Progress: 53.0%\n",
      "Progress: 53.3%\n",
      "Progress: 53.6%\n",
      "Progress: 53.9%\n",
      "Progress: 54.2%\n",
      "Progress: 54.5%\n",
      "Progress: 54.8%\n",
      "Progress: 55.1%\n",
      "Progress: 55.4%\n",
      "Progress: 55.7%\n",
      "Progress: 56.0%\n",
      "Progress: 56.3%\n",
      "Progress: 56.7%\n",
      "Progress: 57.0%\n",
      "Progress: 57.3%\n",
      "Progress: 57.6%\n",
      "Progress: 57.9%\n",
      "Progress: 58.2%\n",
      "Progress: 58.5%\n",
      "Progress: 58.8%\n",
      "Progress: 59.1%\n",
      "Progress: 59.4%\n",
      "Progress: 59.7%\n",
      "Progress: 60.0%\n",
      "Progress: 60.3%\n",
      "Progress: 60.6%\n",
      "Progress: 60.9%\n",
      "Progress: 61.2%\n",
      "Progress: 61.5%\n",
      "Progress: 61.8%\n",
      "Progress: 62.1%\n",
      "Progress: 62.4%\n",
      "Progress: 62.7%\n",
      "Progress: 63.0%\n",
      "Progress: 63.4%\n",
      "Progress: 63.7%\n",
      "Progress: 64.0%\n",
      "Progress: 64.3%\n",
      "Progress: 64.6%\n",
      "Progress: 64.9%\n",
      "Progress: 65.2%\n",
      "Progress: 65.5%\n",
      "Progress: 65.8%\n",
      "Progress: 66.1%\n",
      "Progress: 66.4%\n",
      "Progress: 66.7%\n",
      "Progress: 67.0%\n",
      "Progress: 67.3%\n",
      "Progress: 67.6%\n",
      "Progress: 67.9%\n",
      "Progress: 68.2%\n",
      "Progress: 68.5%\n",
      "Progress: 68.8%\n",
      "Progress: 69.1%\n",
      "Progress: 69.4%\n",
      "Progress: 69.8%\n",
      "Progress: 70.1%\n",
      "Progress: 70.4%\n",
      "Progress: 70.7%\n",
      "Progress: 71.0%\n",
      "Progress: 71.3%\n",
      "Progress: 71.6%\n",
      "Progress: 71.9%\n",
      "Progress: 72.2%\n",
      "Progress: 72.5%\n",
      "Progress: 72.8%\n",
      "Progress: 73.1%\n",
      "Progress: 73.4%\n",
      "Progress: 73.7%\n",
      "Progress: 74.0%\n",
      "Progress: 74.3%\n",
      "Progress: 74.6%\n",
      "Progress: 74.9%\n",
      "Progress: 75.2%\n",
      "Progress: 75.5%\n",
      "Progress: 75.8%\n",
      "Progress: 76.1%\n",
      "Progress: 76.5%\n",
      "Progress: 76.8%\n",
      "Progress: 77.1%\n",
      "Progress: 77.4%\n",
      "Progress: 77.7%\n",
      "Progress: 78.0%\n",
      "Progress: 78.3%\n",
      "Progress: 78.6%\n",
      "Progress: 78.9%\n",
      "Progress: 79.2%\n",
      "Progress: 79.5%\n",
      "Progress: 79.8%\n",
      "Progress: 80.1%\n",
      "Progress: 80.4%\n",
      "Progress: 80.7%\n",
      "Progress: 81.0%\n",
      "Progress: 81.3%\n",
      "Progress: 81.6%\n",
      "Progress: 81.9%\n",
      "Progress: 82.2%\n",
      "Progress: 82.5%\n",
      "Progress: 82.8%\n",
      "Progress: 83.2%\n",
      "Progress: 83.5%\n",
      "Progress: 83.8%\n",
      "Progress: 84.1%\n",
      "Progress: 84.4%\n",
      "Progress: 84.7%\n",
      "Progress: 85.0%\n",
      "Progress: 85.3%\n",
      "Progress: 85.6%\n",
      "Progress: 85.9%\n",
      "Progress: 86.2%\n",
      "Progress: 86.5%\n",
      "Progress: 86.8%\n",
      "Progress: 87.1%\n",
      "Progress: 87.4%\n",
      "Progress: 87.7%\n",
      "Progress: 88.0%\n",
      "Progress: 88.3%\n",
      "Progress: 88.6%\n",
      "Progress: 88.9%\n",
      "Progress: 89.2%\n",
      "Progress: 89.5%\n",
      "Progress: 89.9%\n",
      "Progress: 90.2%\n",
      "Progress: 90.5%\n",
      "Progress: 90.8%\n",
      "Progress: 91.1%\n",
      "Progress: 91.4%\n",
      "Progress: 91.7%\n",
      "Progress: 92.0%\n",
      "Progress: 92.3%\n",
      "Progress: 92.6%\n",
      "Progress: 92.9%\n",
      "Progress: 93.2%\n",
      "Progress: 93.5%\n",
      "Progress: 93.8%\n",
      "Progress: 94.1%\n",
      "Progress: 94.4%\n",
      "Progress: 94.7%\n",
      "Progress: 95.0%\n",
      "Progress: 95.3%\n",
      "Progress: 95.6%\n",
      "Progress: 95.9%\n",
      "Progress: 96.2%\n",
      "Progress: 96.6%\n",
      "Progress: 96.9%\n",
      "Progress: 97.2%\n",
      "Progress: 97.5%\n",
      "Progress: 97.8%\n",
      "Progress: 98.1%\n",
      "Progress: 98.4%\n",
      "Progress: 98.7%\n",
      "Progress: 99.0%\n",
      "Progress: 99.3%\n",
      "Progress: 99.6%\n",
      "Progress: 99.9%\n",
      "Processing gas data...\n",
      "Progress: 0.0%\n",
      "Progress: 0.3%\n",
      "Progress: 0.6%\n",
      "Progress: 0.9%\n",
      "Progress: 1.2%\n",
      "Progress: 1.5%\n",
      "Progress: 1.8%\n",
      "Progress: 2.1%\n",
      "Progress: 2.4%\n",
      "Progress: 2.7%\n",
      "Progress: 3.0%\n",
      "Progress: 3.4%\n",
      "Progress: 3.7%\n",
      "Progress: 4.0%\n",
      "Progress: 4.3%\n",
      "Progress: 4.6%\n",
      "Progress: 4.9%\n",
      "Progress: 5.2%\n",
      "Progress: 5.5%\n",
      "Progress: 5.8%\n",
      "Progress: 6.1%\n",
      "Progress: 6.4%\n",
      "Progress: 6.7%\n",
      "Progress: 7.0%\n",
      "Progress: 7.3%\n",
      "Progress: 7.6%\n",
      "Progress: 7.9%\n",
      "Progress: 8.2%\n",
      "Progress: 8.5%\n",
      "Progress: 8.8%\n",
      "Progress: 9.1%\n",
      "Progress: 9.4%\n",
      "Progress: 9.7%\n",
      "Progress: 10.1%\n",
      "Progress: 10.4%\n",
      "Progress: 10.7%\n",
      "Progress: 11.0%\n",
      "Progress: 11.3%\n",
      "Progress: 11.6%\n",
      "Progress: 11.9%\n",
      "Progress: 12.2%\n",
      "Progress: 12.5%\n",
      "Progress: 12.8%\n",
      "Progress: 13.1%\n",
      "Progress: 13.4%\n",
      "Progress: 13.7%\n",
      "Progress: 14.0%\n",
      "Progress: 14.3%\n",
      "Progress: 14.6%\n",
      "Progress: 14.9%\n",
      "Progress: 15.2%\n",
      "Progress: 15.5%\n",
      "Progress: 15.8%\n",
      "Progress: 16.1%\n",
      "Progress: 16.4%\n",
      "Progress: 16.8%\n",
      "Progress: 17.1%\n",
      "Progress: 17.4%\n",
      "Progress: 17.7%\n",
      "Progress: 18.0%\n",
      "Progress: 18.3%\n",
      "Progress: 18.6%\n",
      "Progress: 18.9%\n",
      "Progress: 19.2%\n",
      "Progress: 19.5%\n",
      "Progress: 19.8%\n",
      "Progress: 20.1%\n",
      "Progress: 20.4%\n",
      "Progress: 20.7%\n",
      "Progress: 21.0%\n",
      "Progress: 21.3%\n",
      "Progress: 21.6%\n",
      "Progress: 21.9%\n",
      "Progress: 22.2%\n",
      "Progress: 22.5%\n",
      "Progress: 22.8%\n",
      "Progress: 23.1%\n",
      "Progress: 23.5%\n",
      "Progress: 23.8%\n",
      "Progress: 24.1%\n",
      "Progress: 24.4%\n",
      "Progress: 24.7%\n",
      "Progress: 25.0%\n",
      "Progress: 25.3%\n",
      "Progress: 25.6%\n",
      "Progress: 25.9%\n",
      "Progress: 26.2%\n",
      "Progress: 26.5%\n",
      "Progress: 26.8%\n",
      "Progress: 27.1%\n",
      "Progress: 27.4%\n",
      "Progress: 27.7%\n",
      "Progress: 28.0%\n",
      "Progress: 28.3%\n",
      "Progress: 28.6%\n",
      "Progress: 28.9%\n",
      "Progress: 29.2%\n",
      "Progress: 29.5%\n",
      "Progress: 29.8%\n",
      "Progress: 30.2%\n",
      "Progress: 30.5%\n",
      "Progress: 30.8%\n",
      "Progress: 31.1%\n",
      "Progress: 31.4%\n",
      "Progress: 31.7%\n",
      "Progress: 32.0%\n",
      "Progress: 32.3%\n",
      "Progress: 32.6%\n",
      "Progress: 32.9%\n",
      "Progress: 33.2%\n",
      "Progress: 33.5%\n",
      "Progress: 33.8%\n",
      "Progress: 34.1%\n",
      "Progress: 34.4%\n",
      "Progress: 34.7%\n",
      "Progress: 35.0%\n",
      "Progress: 35.3%\n",
      "Progress: 35.6%\n",
      "Progress: 35.9%\n",
      "Progress: 36.2%\n",
      "Progress: 36.6%\n",
      "Progress: 36.9%\n",
      "Progress: 37.2%\n",
      "Progress: 37.5%\n",
      "Progress: 37.8%\n",
      "Progress: 38.1%\n",
      "Progress: 38.4%\n",
      "Progress: 38.7%\n",
      "Progress: 39.0%\n",
      "Progress: 39.3%\n",
      "Progress: 39.6%\n",
      "Progress: 39.9%\n",
      "Progress: 40.2%\n",
      "Progress: 40.5%\n",
      "Progress: 40.8%\n",
      "Progress: 41.1%\n",
      "Progress: 41.4%\n",
      "Progress: 41.7%\n",
      "Progress: 42.0%\n",
      "Progress: 42.3%\n",
      "Progress: 42.6%\n",
      "Progress: 42.9%\n",
      "Progress: 43.3%\n",
      "Progress: 43.6%\n",
      "Progress: 43.9%\n",
      "Progress: 44.2%\n",
      "Progress: 44.5%\n",
      "Progress: 44.8%\n",
      "Progress: 45.1%\n",
      "Progress: 45.4%\n",
      "Progress: 45.7%\n",
      "Progress: 46.0%\n",
      "Progress: 46.3%\n",
      "Progress: 46.6%\n",
      "Progress: 46.9%\n",
      "Progress: 47.2%\n",
      "Progress: 47.5%\n",
      "Progress: 47.8%\n",
      "Progress: 48.1%\n",
      "Progress: 48.4%\n",
      "Progress: 48.7%\n",
      "Progress: 49.0%\n",
      "Progress: 49.3%\n",
      "Progress: 49.6%\n",
      "Progress: 50.0%\n",
      "Progress: 50.3%\n",
      "Progress: 50.6%\n",
      "Progress: 50.9%\n",
      "Progress: 51.2%\n",
      "Progress: 51.5%\n",
      "Progress: 51.8%\n",
      "Progress: 52.1%\n",
      "Progress: 52.4%\n",
      "Progress: 52.7%\n",
      "Progress: 53.0%\n",
      "Progress: 53.3%\n",
      "Progress: 53.6%\n",
      "Progress: 53.9%\n",
      "Progress: 54.2%\n",
      "Progress: 54.5%\n",
      "Progress: 54.8%\n",
      "Progress: 55.1%\n",
      "Progress: 55.4%\n",
      "Progress: 55.7%\n",
      "Progress: 56.0%\n",
      "Progress: 56.3%\n",
      "Progress: 56.7%\n",
      "Progress: 57.0%\n",
      "Progress: 57.3%\n",
      "Progress: 57.6%\n",
      "Progress: 57.9%\n",
      "Progress: 58.2%\n",
      "Progress: 58.5%\n",
      "Progress: 58.8%\n",
      "Progress: 59.1%\n",
      "Progress: 59.4%\n",
      "Progress: 59.7%\n",
      "Progress: 60.0%\n",
      "Progress: 60.3%\n",
      "Progress: 60.6%\n",
      "Progress: 60.9%\n",
      "Progress: 61.2%\n",
      "Progress: 61.5%\n",
      "Progress: 61.8%\n",
      "Progress: 62.1%\n",
      "Progress: 62.4%\n",
      "Progress: 62.7%\n",
      "Progress: 63.0%\n",
      "Progress: 63.4%\n",
      "Progress: 63.7%\n",
      "Progress: 64.0%\n",
      "Progress: 64.3%\n",
      "Progress: 64.6%\n",
      "Progress: 64.9%\n",
      "Progress: 65.2%\n",
      "Progress: 65.5%\n",
      "Progress: 65.8%\n",
      "Progress: 66.1%\n",
      "Progress: 66.4%\n",
      "Progress: 66.7%\n",
      "Progress: 67.0%\n",
      "Progress: 67.3%\n",
      "Progress: 67.6%\n",
      "Progress: 67.9%\n",
      "Progress: 68.2%\n",
      "Progress: 68.5%\n",
      "Progress: 68.8%\n",
      "Progress: 69.1%\n",
      "Progress: 69.4%\n",
      "Progress: 69.8%\n",
      "Progress: 70.1%\n",
      "Progress: 70.4%\n",
      "Progress: 70.7%\n",
      "Progress: 71.0%\n",
      "Progress: 71.3%\n",
      "Progress: 71.6%\n",
      "Progress: 71.9%\n",
      "Progress: 72.2%\n",
      "Progress: 72.5%\n",
      "Progress: 72.8%\n",
      "Progress: 73.1%\n",
      "Progress: 73.4%\n",
      "Progress: 73.7%\n",
      "Progress: 74.0%\n",
      "Progress: 74.3%\n",
      "Progress: 74.6%\n",
      "Progress: 74.9%\n",
      "Progress: 75.2%\n",
      "Progress: 75.5%\n",
      "Progress: 75.8%\n",
      "Progress: 76.1%\n",
      "Progress: 76.5%\n",
      "Progress: 76.8%\n",
      "Progress: 77.1%\n",
      "Progress: 77.4%\n",
      "Progress: 77.7%\n",
      "Progress: 78.0%\n",
      "Progress: 78.3%\n",
      "Progress: 78.6%\n",
      "Progress: 78.9%\n",
      "Progress: 79.2%\n",
      "Progress: 79.5%\n",
      "Progress: 79.8%\n",
      "Progress: 80.1%\n",
      "Progress: 80.4%\n",
      "Progress: 80.7%\n",
      "Progress: 81.0%\n",
      "Progress: 81.3%\n",
      "Progress: 81.6%\n",
      "Progress: 81.9%\n",
      "Progress: 82.2%\n",
      "Progress: 82.5%\n",
      "Progress: 82.8%\n",
      "Progress: 83.2%\n",
      "Progress: 83.5%\n",
      "Progress: 83.8%\n",
      "Progress: 84.1%\n",
      "Progress: 84.4%\n",
      "Progress: 84.7%\n",
      "Progress: 85.0%\n",
      "Progress: 85.3%\n",
      "Progress: 85.6%\n",
      "Progress: 85.9%\n",
      "Progress: 86.2%\n",
      "Progress: 86.5%\n",
      "Progress: 86.8%\n",
      "Progress: 87.1%\n",
      "Progress: 87.4%\n",
      "Progress: 87.7%\n",
      "Progress: 88.0%\n",
      "Progress: 88.3%\n",
      "Progress: 88.6%\n",
      "Progress: 88.9%\n",
      "Progress: 89.2%\n",
      "Progress: 89.5%\n",
      "Progress: 89.9%\n",
      "Progress: 90.2%\n",
      "Progress: 90.5%\n",
      "Progress: 90.8%\n",
      "Progress: 91.1%\n",
      "Progress: 91.4%\n",
      "Progress: 91.7%\n",
      "Progress: 92.0%\n",
      "Progress: 92.3%\n",
      "Progress: 92.6%\n",
      "Progress: 92.9%\n",
      "Progress: 93.2%\n",
      "Progress: 93.5%\n",
      "Progress: 93.8%\n",
      "Progress: 94.1%\n",
      "Progress: 94.4%\n",
      "Progress: 94.7%\n",
      "Progress: 95.0%\n",
      "Progress: 95.3%\n",
      "Progress: 95.6%\n",
      "Progress: 95.9%\n",
      "Progress: 96.2%\n",
      "Progress: 96.6%\n",
      "Progress: 96.9%\n",
      "Progress: 97.2%\n",
      "Progress: 97.5%\n",
      "Progress: 97.8%\n",
      "Progress: 98.1%\n",
      "Progress: 98.4%\n",
      "Progress: 98.7%\n",
      "Progress: 99.0%\n",
      "Progress: 99.3%\n",
      "Progress: 99.6%\n",
      "Progress: 99.9%\n"
     ]
    }
   ],
   "source": [
    "# Define parameters and columns\n",
    "time_steps = 90\n",
    "\n",
    "# Features and target definition (Strom und Gas)\n",
    "feature_columns = [col for col in df_clean.columns if col not in ['electric_total_consumption_Wh', 'gas_total_consumption_Wh', 'homeid', 'roomid']]\n",
    "target_column_electric = 'electric_total_consumption_Wh'\n",
    "target_column_gas = 'gas_total_consumption_Wh'\n",
    "\n",
    "def create_memmap_array(shape, filename, dtype='float32'):\n",
    "    \"\"\"Create memory-mapped array\"\"\"\n",
    "    path = Path('temp_arrays')\n",
    "    path.mkdir(exist_ok=True)\n",
    "    return np.memmap(path / filename, dtype=dtype, mode='w+', shape=shape)\n",
    "\n",
    "def process_data_efficiently(df_clean, target_column, feature_columns, time_steps, prefix):\n",
    "    \"\"\"Process data with disk-based storage\"\"\"\n",
    "    total_sequences = len(df_clean) - time_steps\n",
    "    n_features = len(feature_columns)\n",
    "    \n",
    "    # Create memory-mapped arrays\n",
    "    X = create_memmap_array((total_sequences, time_steps, n_features), f'{prefix}_X.mmap')\n",
    "    y = create_memmap_array((total_sequences,), f'{prefix}_y.mmap')\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    chunk_size = 500\n",
    "    feature_data = df_clean[feature_columns].values\n",
    "    target_data = df_clean[target_column].values\n",
    "    \n",
    "    print(f\"Processing {prefix} data...\")\n",
    "    for chunk_start in range(0, total_sequences, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_sequences)\n",
    "        \n",
    "        for i in range(chunk_start, chunk_end):\n",
    "            X[i] = feature_data[i:i + time_steps]\n",
    "            y[i] = target_data[i + time_steps]\n",
    "            \n",
    "        if chunk_start % (chunk_size * 10) == 0:\n",
    "            print(f\"Progress: {chunk_start/total_sequences*100:.1f}%\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Process both electric and gas data\n",
    "time_steps = 90\n",
    "feature_columns = [col for col in df_clean.columns if col not in \n",
    "                  ['electric_total_consumption_Wh', 'gas_total_consumption_Wh', 'homeid', 'roomid']]\n",
    "\n",
    "# Process electric data\n",
    "X_electric, y_electric = process_data_efficiently(\n",
    "    df_clean, \n",
    "    'electric_total_consumption_Wh',\n",
    "    feature_columns,\n",
    "    time_steps,\n",
    "    'electric'\n",
    ")\n",
    "\n",
    "# Process gas data\n",
    "X_gas, y_gas = process_data_efficiently(\n",
    "    df_clean,\n",
    "    'gas_total_consumption_Wh',\n",
    "    feature_columns,\n",
    "    time_steps,\n",
    "    'gas'\n",
    ")\n",
    "\n",
    "# Split datasets\n",
    "X_train_electric, X_test_electric, y_train_electric, y_test_electric = train_test_split(\n",
    "    X_electric, y_electric, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "X_train_gas, X_test_gas, y_train_gas, y_test_gas = train_test_split(\n",
    "    X_gas, y_gas, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Show shapes of the data\n",
    "train_test_summary = {\n",
    "    'X_train_electric': X_train_electric.shape,\n",
    "    'X_test_electric': X_test_electric.shape,\n",
    "    'X_train_gas': X_train_gas.shape,\n",
    "    'X_test_gas': X_test_gas.shape,\n",
    "}\n",
    "train_test_summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E5KwcVmgqyx"
   },
   "source": [
    "**4. LSTM Modell erstellen**\n",
    "1. Daten in das LSTM Format bringen (X_train, y_train)\n",
    "2. LSTM schichten definieren (Tensorflow)\n",
    "3. Modell kompilieren und trainieren\n",
    "4. Hyperparameter-Tuning (z.B Anzahl Neuronen, Learning Rate,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "G3H2XcgAgqyx",
    "outputId": "feb40253-cf67-42b8-8fd3-8c7561b0d285"
   },
   "outputs": [],
   "source": [
    "# # Configure device\n",
    "# def get_device():\n",
    "#     \"\"\"Get available device (GPU or CPU)\"\"\"\n",
    "#     if tf.config.list_physical_devices('GPU'):\n",
    "#         return '/GPU:0'\n",
    "#     return '/CPU:0'\n",
    "\n",
    "# # Configure training environment\n",
    "# def setup_training_env():\n",
    "#     \"\"\"Setup TensorFlow training environment\"\"\"\n",
    "#     tf.keras.backend.clear_session()\n",
    "\n",
    "#     # Configure GPU if available\n",
    "#     physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#     if physical_devices:\n",
    "#         try:\n",
    "#             for device in physical_devices:\n",
    "#                 tf.config.experimental.set_memory_growth(device, True)\n",
    "#         except:\n",
    "#             print(\"Memory growth not supported\")\n",
    "#     else:\n",
    "#         print(\"Using CPU for training\")\n",
    "\n",
    "# def train_with_batches(X, y, model_name, batch_size=8):\n",
    "#     \"\"\"Train LSTM model with device optimization\"\"\"\n",
    "#     device = get_device()\n",
    "\n",
    "#     with tf.device(device):\n",
    "#         # Build model with distribution strategy\n",
    "#         strategy = tf.distribute.MirroredStrategy() if device == '/GPU:0' else tf.distribute.OneDeviceStrategy(device)\n",
    "\n",
    "#         with strategy.scope():\n",
    "#             model = tf.keras.Sequential([\n",
    "#                 tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(90, 24)),\n",
    "#                 tf.keras.layers.Dropout(0.2),\n",
    "#                 tf.keras.layers.LSTM(16),\n",
    "#                 tf.keras.layers.Dense(1)\n",
    "#             ])\n",
    "#             model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#         # Create optimized dataset\n",
    "#         dataset = tf.data.Dataset.from_tensor_slices((X, y))\\\n",
    "#             .batch(batch_size)\\\n",
    "#             .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#         # Train with device optimization\n",
    "#         history = model.fit(\n",
    "#             dataset,\n",
    "#             epochs=20,\n",
    "#             verbose=1\n",
    "#         )\n",
    "\n",
    "#         return model, history\n",
    "\n",
    "# # Main training pipeline\n",
    "# def train_models():\n",
    "#     setup_training_env()\n",
    "\n",
    "#     try:\n",
    "#         print(f\"Training on: {get_device()}\")\n",
    "\n",
    "#         print(\"Training Electric Model...\")\n",
    "#         model_electric, history_electric = train_with_batches(\n",
    "#             X_train_electric.astype('float32'),\n",
    "#             y_train_electric.astype('float32'),\n",
    "#             \"electric\"\n",
    "#         )\n",
    "\n",
    "#         print(\"\\nTraining Gas Model...\")\n",
    "#         model_gas, history_gas = train_with_batches(\n",
    "#             X_train_gas.astype('float32'),\n",
    "#             y_train_gas.astype('float32'),\n",
    "#             \"gas\"\n",
    "#         )\n",
    "\n",
    "#         return model_electric, model_gas\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Training Error: {e}\")\n",
    "#         raise\n",
    "\n",
    "# # Execute training\n",
    "# model_electric, model_gas = train_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Purposes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check and configure MPS device for M2 Mac\n",
    "# has_mps = torch.backends.mps.is_available()\n",
    "# if has_mps:\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"Using MPS device\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"MPS device not found, using CPU\")\n",
    "\n",
    "# # Add after MPS device setup and before model initialization\n",
    "# def prepare_data_for_training(X, y, batch_size=32):\n",
    "#     \"\"\"Convert numpy arrays to PyTorch DataLoader\"\"\"\n",
    "#     X_tensor = torch.FloatTensor(X)\n",
    "#     y_tensor = torch.FloatTensor(y).reshape(-1, 1)\n",
    "#     dataset = TensorDataset(X_tensor, y_tensor)\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Create dataloaders\n",
    "# train_loader_electric = prepare_data_for_training(X_train_electric, y_train_electric)\n",
    "# train_loader_gas = prepare_data_for_training(X_train_gas, y_train_gas)\n",
    "\n",
    "\n",
    "# def train_model(model, train_loader, model_name, num_epochs=6):\n",
    "#     checkpoint_dir = 'checkpoints'\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#     # Create epoch progress bar\n",
    "#     epoch_pbar = tqdm(range(num_epochs), desc=f'Training {model_name}', position=0)\n",
    "    \n",
    "#     for epoch in epoch_pbar:\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "        \n",
    "#         # Create batch progress bar\n",
    "#         batch_pbar = tqdm(train_loader, \n",
    "#                          desc=f'Epoch {epoch+1}/{num_epochs}',\n",
    "#                          leave=False, \n",
    "#                          position=1)\n",
    "        \n",
    "#         for inputs, targets in batch_pbar:\n",
    "#             inputs = inputs.to(device)\n",
    "#             targets = targets.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item()\n",
    "#             batch_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "#         epoch_loss = running_loss / len(train_loader)\n",
    "#         epoch_pbar.set_postfix({'loss': f'{epoch_loss:.4f}'})\n",
    "\n",
    "#         # Save checkpoint\n",
    "#         checkpoint_path = os.path.join(checkpoint_dir, f'{model_name}_epoch{epoch+1}.pt')\n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': epoch_loss,\n",
    "#         }, checkpoint_path)\n",
    "\n",
    "#     print(f\"\\n{model_name} training complete!\")\n",
    "#     return model\n",
    "\n",
    "# # Initialize models\n",
    "# model_electric = LSTMModel(\n",
    "#     input_size=len(feature_columns), \n",
    "#     hidden_size=32, \n",
    "#     num_layers=2, \n",
    "#     output_size=1\n",
    "# ).to(device)\n",
    "\n",
    "# model_gas = LSTMModel(\n",
    "#     input_size=len(feature_columns), \n",
    "#     hidden_size=32, \n",
    "#     num_layers=2, \n",
    "#     output_size=1\n",
    "# ).to(device)\n",
    "\n",
    "# # Replace the training calls with:\n",
    "# print(\"Training Electric Model...\")\n",
    "# model_electric = train_model(model_electric, train_loader_electric, \"electric\")\n",
    "\n",
    "# print(\"\\nTraining Gas Model...\")\n",
    "# model_gas = train_model(model_gas, train_loader_gas, \"gas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_78qhv1gqyx"
   },
   "source": [
    "**5. Modell evaluieren & Vorhersagen interpretieren**\n",
    "1. Vorhersagen auf Testdaten durchführen\n",
    "2. Metriken berechnen (RMSE, MAE, R^2)\n",
    "3. XAI mit SHAP oder LIME anwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Configure device for M2 Mac\n",
    "has_mps = torch.backends.mps.is_available()\n",
    "if has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not found, using CPU\")\n",
    "\n",
    "class SHAPWrapper(nn.Module):\n",
    "    \"\"\"Enhanced LSTM wrapper for SHAP\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle input shape for LSTM\n",
    "        if len(x.shape) == 2:\n",
    "            # Reshape (batch_size, features) to (batch_size, sequence_length, features)\n",
    "            x = x.view(x.size(0), -1, 24)  # Assuming 24 features per timestep\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "def calculate_metrics(predictions, actuals):\n",
    "    \"\"\"Calculate RMSE, MAE, and R2 metrics\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def load_model_weights(model, model_name, epoch):\n",
    "    checkpoint_path = f'checkpoints/{model_name}_epoch{epoch}.pt'\n",
    "    print(f'Loading model weights from: {checkpoint_path}')\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device)['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def make_predictions(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Making predictions\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.to('cpu').numpy())\n",
    "            actuals.append(targets.to('cpu').numpy())\n",
    "    return np.concatenate(predictions), np.concatenate(actuals)\n",
    "\n",
    "def apply_shap_optimized(model, test_data, max_samples=50):\n",
    "    \"\"\"Optimized SHAP implementation with shape handling\"\"\"\n",
    "    print(\"Moving model to CPU for SHAP analysis...\")\n",
    "    model_cpu = model.to('cpu')\n",
    "    wrapped_model = SHAPWrapper(model_cpu)\n",
    "    wrapped_model.eval()\n",
    "    \n",
    "    # Prepare data with correct shape\n",
    "    if len(test_data.shape) == 2:\n",
    "        test_data = test_data.reshape(-1, 90, 24)  # Reshape to (samples, timesteps, features)\n",
    "    \n",
    "    # Sample and convert data\n",
    "    sample_indices = np.random.choice(len(test_data), min(max_samples, len(test_data)), replace=False)\n",
    "    test_data_subset = test_data[sample_indices]\n",
    "    \n",
    "    # Convert to tensor with correct shape\n",
    "    test_data_tensor = torch.FloatTensor(test_data_subset)\n",
    "    background = test_data_tensor[:min(10, len(test_data_subset))]\n",
    "    \n",
    "    try:\n",
    "        print(\"Computing SHAP values...\")\n",
    "        explainer = shap.GradientExplainer(wrapped_model, background)\n",
    "        shap_values = explainer.shap_values(test_data_tensor)\n",
    "        \n",
    "        # Convert to numpy and reshape if needed\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        shap_values = np.array(shap_values)\n",
    "        \n",
    "        # Reshape for visualization\n",
    "        final_shape = (len(sample_indices), -1)\n",
    "        shap_values = shap_values.reshape(final_shape)\n",
    "        \n",
    "        return shap_values, test_data_subset.reshape(final_shape)\n",
    "        \n",
    "    finally:\n",
    "        model.to(device)\n",
    "\n",
    "\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Use bar plot instead of violin plot\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            test_data,\n",
    "            feature_names=feature_names,\n",
    "            plot_type=\"bar\",\n",
    "            show=False\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Feature Importance for {model_name} Model')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'shap_{model_name.lower()}_importance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting SHAP values: {str(e)}\")\n",
    "\n",
    "def apply_lime_batched(model, test_data, feature_names, max_samples=50, batch_size=10):\n",
    "    \"\"\"Apply LIME with batching and memory management\"\"\"\n",
    "    \n",
    "    def predict_fn(x):\n",
    "        with torch.no_grad():\n",
    "            tensor_x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "            return model(tensor_x).to('cpu').numpy()\n",
    "    \n",
    "    # Sample subset of test data\n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(len(test_data), min(max_samples, len(test_data)), replace=False)\n",
    "    test_data_subset = test_data[sample_indices]\n",
    "    \n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=np.array(test_data_subset),\n",
    "        feature_names=feature_names,\n",
    "        mode='regression',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    lime_explanations = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(test_data_subset), batch_size), desc=\"Applying LIME\"):\n",
    "        batch = test_data_subset[i:i + batch_size]\n",
    "        batch_explanations = []\n",
    "        \n",
    "        for sample in batch:\n",
    "            try:\n",
    "                exp = explainer.explain_instance(\n",
    "                    sample, \n",
    "                    predict_fn,\n",
    "                    num_features=len(feature_names)\n",
    "                )\n",
    "                batch_explanations.append(exp)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        lime_explanations.extend(batch_explanations)\n",
    "        \n",
    "        # Clear memory\n",
    "        if has_mps:\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    return lime_explanations\n",
    "\n",
    "# Evaluation pipeline\n",
    "# Update evaluate_model function\n",
    "def evaluate_model(model, model_name, test_loader, test_data):\n",
    "    print(f\"\\nEvaluating {model_name} model...\")\n",
    "    \n",
    "    with tqdm(total=2, desc=f\"{model_name} Analysis\") as pbar:\n",
    "        # Predictions and metrics\n",
    "        predictions, actuals = make_predictions(model, test_loader)\n",
    "        rmse, mae, r2 = calculate_metrics(predictions, actuals)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        print(f\"Metrics for {model_name}:\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R2: {r2:.4f}\")\n",
    "        \n",
    "        # LIME analysis with batching\n",
    "        print(\"\\nGenerating LIME explanations...\")\n",
    "        lime_explanations = apply_lime_batched(\n",
    "            model, \n",
    "            test_data, \n",
    "            feature_columns,\n",
    "            max_samples=50,  # Reduce number of samples\n",
    "            batch_size=10    # Process in smaller batches\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        \n",
    "        return predictions, actuals, lime_explanations\n",
    "\n",
    "# Update data loading with progress bars\n",
    "print(\"Preparing data loaders...\")\n",
    "with tqdm(total=2, desc=\"Loading datasets\") as pbar:\n",
    "    # For Electric data\n",
    "    test_dataset_electric = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_test_electric).to(device),\n",
    "        torch.FloatTensor(y_test_electric).to(device)\n",
    "    )\n",
    "    test_loader_electric = torch.utils.data.DataLoader(\n",
    "        test_dataset_electric, \n",
    "        batch_size=16, \n",
    "        shuffle=False\n",
    "    )\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # For Gas data\n",
    "    test_dataset_gas = torch.utils.data.TensorDataset(\n",
    "        torch.FloatTensor(X_test_gas).to(device),\n",
    "        torch.FloatTensor(y_test_gas).to(device)\n",
    "    )\n",
    "    test_loader_gas = torch.utils.data.DataLoader(\n",
    "        test_dataset_gas, \n",
    "        batch_size=16, \n",
    "        shuffle=False\n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "model_electric = LSTMModel(input_size=len(feature_columns), hidden_size=32, num_layers=2, output_size=1).to(device)\n",
    "model_gas = LSTMModel(input_size=len(feature_columns), hidden_size=32, num_layers=2, output_size=1).to(device)\n",
    "\n",
    "# Define models dictionary\n",
    "models = {\n",
    "    'Electric': (\n",
    "        load_model_weights(model_electric, 'electric', epoch=6),\n",
    "        test_loader_electric,\n",
    "        X_test_electric\n",
    "    ),\n",
    "    'Gas': (\n",
    "        load_model_weights(model_gas, 'gas', epoch=6),\n",
    "        test_loader_gas,\n",
    "        X_test_gas\n",
    "    )\n",
    "}\n",
    "\n",
    "# Run evaluation with progress tracking\n",
    "for model_name, (model, loader, test_data) in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "    predictions, actuals, lime_exps = evaluate_model(model, model_name, loader, test_data)  # Updated unpacking\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Predictive_Analytics_Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
